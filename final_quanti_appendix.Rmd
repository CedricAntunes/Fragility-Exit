---
title: "FRAGILITY EXIT - Quantiative Appendix"
author: 
- "Christoph Zuercher" 
- "Cedric Antunes"
date: "August 2025"
header-includes:
    \usepackage{lscape}
    \usepackage{pdfpages}
    \usepackage{graphicx}
    \usepackage[figuresright]{rotating}
output:
  pdf_document:
    toc: true
    toc_depth: 2
---

```{r, echo = FALSE, message = FALSE, warning = FALSE, results = 'hide'}
# Cleaning my environment
rm(list = ls())

# Managing memory
gc()

# Load packages ---------------------------------------------------------------
library(here)
library(coefplot)
library(fixest)
library(pROC)
library(PRROC)
library(texreg)
library(dynlm)
library(tibble)
library(tidyverse)
library(glmnet)
library(lubridate)
library(dplyr)
library(broom)
library(plm)
library(car)
library(nnet)
library(tidymodels)
library(scatr)
library(zoo)
library(knitr)
library(kableExtra)
library(lmtest)
library(sandwich)
library(margins)
library(tidyr)
library(caret)
library(stargazer)
library(VGAM)
library(gt)
library(factoextra)
library(cluster)
library(ggplot2)
library(vip)
library(themis)
library(janitor)
library(WDI)

# Setting seed for replication -------------------------------------------------
set.seed(247)

# Load data, functions, and scripts --------------------------------------------
load("final_new_clean_output_data.RDS")
source("WGI_30th_missing_estimates.R")
source("WGI_35th_missing_estimates.R")
source("WGI_loading_factor_estimates.R")
source("fragility_breakthrough_identifier.R")
source("VDEM_never_fragile.R")

# ------------------------------------------------------------------------------
# WORLD BANK DATA --------------------------------------------------------------
# ------------------------------------------------------------------------------

# Last minute input: extracting final GDP per capita data ----------------------
gdp_per_capita_data <- WDI(
  country = "all",
  indicator = c("NY.GDP.PCAP.CD"),
  start = 1970,
  end = 2023,
  extra = FALSE,
  cache = NULL,
  latest = NULL,
  language = "en"
)

# Cleaning the data ------------------------------------------------------------
gdp_per_capita_data <- gdp_per_capita_data |>
  # Renaming variables for match
  rename(COUNTRY_NAME = country,
         ISO_CODE_3 = iso3c,
         YEAR = year,
         GDP_PER_CAPITA = NY.GDP.PCAP.CD) |>
  # Selecting only key variables for match 
  select(COUNTRY_NAME,
         ISO_CODE_3,
         YEAR,
         GDP_PER_CAPITA) |>
  # Dropping 2023
  filter(YEAR != 2023) |>
  # Setting YEAR as character
  mutate(YEAR = as.character(YEAR)) |>
  # Selecting only country-rows
  slice(-(1:2597))

# Preparing mother-dataset
final_clean_percentiles_data <- final_clean_percentiles_data |>
  mutate(YEAR = as.character(YEAR))

# Performing the join ----------------------------------------------------------
final_clean_percentiles_data <- left_join(final_clean_percentiles_data,
                                          gdp_per_capita_data,
                                          by = c("ISO_CODE_3", 
                                                 "YEAR"),
                                          keep = FALSE)

# Final cleanning --------------------------------------------------------------
final_clean_percentiles_data <- final_clean_percentiles_data |>
  # Dropping duplicate columns 
  rename(COUNTRY_NAME = COUNTRY_NAME.x) |>
  select(-COUNTRY_NAME.y) |>
  # Allocating GDP per capita along with economic indicators 
  relocate(82, .after = 3) |>
  # Log-trnaforming GDP per capita 
  mutate(LOG_GDP_PER_CAPITA = if_else(GDP_PER_CAPITA > 0, log(GDP_PER_CAPITA), NA_real_))

# Building working dataframe ---------------------------------------------------
final_clean_percentiles_data_normalized_1 <- final_clean_percentiles_data |>
  select(-REVISED_COMBINED_POLITY_SCORE,
         -PRIOR_COMBINED_POLITY_SCORE,
         -POLITY_END_MONTH, 
         -POLITY_END_DAY,
         -POLITY_END_YEAR,
         -END_DATE_PRECISION,
         -INTERIM_POLITY_SCORE,
         -POLITY_BEGIN_YEAR,
         -POLITY_BEGIN_MONTH,
         -POLITY_BEGIN_DAY,
         -POLITYV_CONFIDENCE_INDEX,
         -BEGIN_DATE_PRECISION,
         -POST_POLITY_SCORE,
         -TOTAL_CHANGE_POLITY_SCORE,
         -REGIME_TRANSITION_COMPLETED,
         -REGIME_TRANSITION,
         -CONFLICT_ID,
         -CONFLICT_LOCATION,
         -CONFLICT_CAUSE,
         -CONFLICT_TYPE,
         -CONFLICT_INACTIVE,
         -COUNTRY_CONFLICT_SIDE_A,
         -STATES_SUPPORT_SIDE_A_WITH_TROOPS,
         -COUNTRY_OR_ACTOR_CONFLICT_SIDE_B,
         -STATES_SUPPORT_SIDE_B_WITH_TROOPS,
         -TERRITORY_UNDER_DISPUTE,
         -CONFLICT_START_DATE,
         -PRECISION_OF_CONFLICT_START_DATE,
         -PRECISION_OF_CONFLICT_END_DATE,
         -CONFLICT_DEATHS_THRESHOLD_DATE,
         -PRECISION_OF_CONFLICT_DEATHS_THRESHOLD_DATE,
         -CONFLICT_END_DATE,
         -PRECISION_OF_CONFLICT_START_DATE
  ) |>
  estimating_WGI_30th_missing_values() |>
  estimating_WGI_35th_missing_values() |>
  filter(complete.cases(NEW_VDEM_LOADING_FACTOR_1_NORMALIZED.y, NEW_VDEM_30TH_PERCENTILE)) |>
  group_by(COUNTRY_NAME) |>
  mutate(
    YEAR = as.numeric(YEAR),

    # Fragility DUMMIES (1 = Fragile, 0 = Not Fragile)
    VDEM_FRAGILE_BASELINE     = if_else(NEW_VDEM_LOADING_FACTOR_1_NORMALIZED.y < NEW_VDEM_20TH_PERCENTILE, 1, 0),
    VDEM_NON_FRAGILE_BASELINE = if_else(NEW_VDEM_LOADING_FACTOR_1_NORMALIZED.y >= NEW_VDEM_25TH_PERCENTILE, 1, 0),

    VDEM_FRAGILE_IDEAL        = if_else(NEW_VDEM_LOADING_FACTOR_1_NORMALIZED.y < NEW_VDEM_30TH_PERCENTILE, 1, 0),
    VDEM_NON_FRAGILE_IDEAL    = if_else(NEW_VDEM_LOADING_FACTOR_1_NORMALIZED.y >= NEW_VDEM_35TH_PERCENTILE, 1, 0),

    VDEM_FRAGILE_ENDLINE      = if_else(NEW_VDEM_LOADING_FACTOR_1_NORMALIZED.y < NEW_VDEM_20TH_PERCENTILE, 1, 0),
    VDEM_NON_FRAGILE_ENDLINE  = if_else(NEW_VDEM_LOADING_FACTOR_1_NORMALIZED.y >= NEW_VDEM_40TH_PERCENTILE, 1, 0),

    # Fragility LABELS
    VDEM_STATUS_BASELINE = case_when(
      NEW_VDEM_LOADING_FACTOR_1_NORMALIZED.y < NEW_VDEM_20TH_PERCENTILE ~ "Fragile",
      NEW_VDEM_LOADING_FACTOR_1_NORMALIZED.y < NEW_VDEM_25TH_PERCENTILE ~ "Transitioning",
      TRUE ~ "Non Fragile"
    ),
    VDEM_STATUS_IDEAL = case_when(
      NEW_VDEM_LOADING_FACTOR_1_NORMALIZED.y < NEW_VDEM_30TH_PERCENTILE ~ "Fragile",
      NEW_VDEM_LOADING_FACTOR_1_NORMALIZED.y < NEW_VDEM_35TH_PERCENTILE ~ "Transitioning",
      TRUE ~ "Non Fragile"
    ),
    VDEM_STATUS_ENDLINE = case_when(
      NEW_VDEM_LOADING_FACTOR_1_NORMALIZED.y < NEW_VDEM_20TH_PERCENTILE ~ "Fragile",
      NEW_VDEM_LOADING_FACTOR_1_NORMALIZED.y < NEW_VDEM_40TH_PERCENTILE ~ "Transitioning",
      TRUE ~ "Non Fragile"
    )
  )

# Converting to character before standardizing ---------------------------------
final_data <- final_clean_percentiles_data_normalized_1 |>
  mutate(
    YEAR = as.character(YEAR),
    across(35:55, as.character)  # If needed for export or plotting
  )

# Computing means and SDs for numeric columns ----------------------------------
means_data <- sapply(final_data, function(x) if (is.numeric(x)) mean(x, na.rm = TRUE) else NA)
sd_data    <- sapply(final_data, function(x) if (is.numeric(x)) sd(x, na.rm = TRUE) else NA)

# Initializing standardized data
standardized_data <- final_data

# Defining dummies to EXCLUDE from standardization
dummy_vars <- c(
  "VDEM_FRAGILE_BASELINE", 
  "VDEM_NON_FRAGILE_BASELINE",
  "VDEM_FRAGILE_IDEAL", 
  "VDEM_NON_FRAGILE_IDEAL",
  "VDEM_FRAGILE_ENDLINE", 
  "VDEM_NON_FRAGILE_ENDLINE"
)

# Applying standardization safely
for (col in names(final_data)) {
  if (is.numeric(final_data[[col]]) && !(col %in% dummy_vars)) {
    standardized_data[[col]] <- (final_data[[col]] - means_data[[col]]) / sd_data[[col]]
  }
}

# Converting relevant predictors back to numeric
standardized_data_final <- standardized_data |>
  mutate(across(35:55, as.numeric))

# Final correction pass: recalculating fragility dummies from scratch
standardized_data_final <- standardized_data_final |>
  mutate(
    VDEM_FRAGILE_BASELINE = if_else(NEW_VDEM_LOADING_FACTOR_1_NORMALIZED.y < NEW_VDEM_20TH_PERCENTILE, 1, 0),
    VDEM_FRAGILE_IDEAL    = if_else(NEW_VDEM_LOADING_FACTOR_1_NORMALIZED.y < NEW_VDEM_30TH_PERCENTILE, 1, 0),
    VDEM_FRAGILE_ENDLINE  = if_else(NEW_VDEM_LOADING_FACTOR_1_NORMALIZED.y < NEW_VDEM_20TH_PERCENTILE, 1, 0),

    VDEM_STATUS_BASELINE = case_when(
      NEW_VDEM_LOADING_FACTOR_1_NORMALIZED.y < NEW_VDEM_20TH_PERCENTILE ~ "Fragile",
      NEW_VDEM_LOADING_FACTOR_1_NORMALIZED.y < NEW_VDEM_25TH_PERCENTILE ~ "Transitioning",
      TRUE ~ "Non Fragile"
    ),
    VDEM_STATUS_IDEAL = case_when(
      NEW_VDEM_LOADING_FACTOR_1_NORMALIZED.y < NEW_VDEM_30TH_PERCENTILE ~ "Fragile",
      NEW_VDEM_LOADING_FACTOR_1_NORMALIZED.y < NEW_VDEM_35TH_PERCENTILE ~ "Transitioning",
      TRUE ~ "Non Fragile"
    ),
    VDEM_STATUS_ENDLINE = case_when(
      NEW_VDEM_LOADING_FACTOR_1_NORMALIZED.y < NEW_VDEM_20TH_PERCENTILE ~ "Fragile",
      NEW_VDEM_LOADING_FACTOR_1_NORMALIZED.y < NEW_VDEM_40TH_PERCENTILE ~ "Transitioning",
      TRUE ~ "Non Fragile"
    )
  )

# Saving data 
save(standardized_data_final, 
     file = "standardized_data_final.RDS")

# Setting the critical junctures
standardized_data_final <- standardized_data_final |>
  mutate(CRITICAL_JUNCTURE = case_when(
    (COUNTRY_NAME == "Angola" & YEAR == 2018) ~ 1,
    (COUNTRY_NAME == "Brazil" & YEAR == 1985) ~ 1,
    (COUNTRY_NAME == "Ethiopia" & YEAR == 1991) ~ 1,
    (COUNTRY_NAME == "Gabon" & YEAR == 1990) ~ 1,
    (COUNTRY_NAME == "Gambia, The" & YEAR == 2016) ~ 1,
    (COUNTRY_NAME == "Ghana" & YEAR == 1992) ~ 1,
    (COUNTRY_NAME == "Kenya" & YEAR == 2002) ~ 1,
    (COUNTRY_NAME == "Mauritania" & YEAR == 1992) ~ 1,
    (COUNTRY_NAME == "Sierra Leone" & YEAR == 2002) ~ 1,
    (COUNTRY_NAME == "Tunisia" & YEAR == 2010) ~ 1,
    (COUNTRY_NAME == "Uganda" & YEAR == 1986) ~ 1,
    (COUNTRY_NAME == "Madagascar" & YEAR == 1991) ~ 1,
    (COUNTRY_NAME == "Myanmar" & YEAR == 2008) ~ 1,
    (COUNTRY_NAME == "Indonesia" & YEAR == 1998) ~ 1,
    (COUNTRY_NAME == "Iran, Islamic Rep." & YEAR == 1988) ~ 1,
    (COUNTRY_NAME == "Nepal" & YEAR == 2006) ~ 1,
    (COUNTRY_NAME == "Philippines" & YEAR == 1986) ~ 1,
    (COUNTRY_NAME == "Sri Lanka" & YEAR == 2009) ~ 1,
    (COUNTRY_NAME == "China" & YEAR == 1977) ~ 1,
    (COUNTRY_NAME == "Thailand" & YEAR == 1991) ~ 1,
    (COUNTRY_NAME == "Bosnia and Herzegovina" & YEAR == 1995) ~ 1,
    (COUNTRY_NAME == "Serbia" & YEAR == 1999) ~ 1,
    (COUNTRY_NAME == "Georgia" & YEAR == 2004) ~ 1,
    (COUNTRY_NAME == "Kyrgyz Republic" & YEAR == 2010) ~ 1,
    (COUNTRY_NAME == "Armenia" & YEAR == 2018) ~ 1,
    (COUNTRY_NAME == "Argentina" & YEAR == 1983) ~ 1,
    (COUNTRY_NAME == "Bolivia" & YEAR == 1982) ~ 1,
    (COUNTRY_NAME == "Colombia" & YEAR == 1990) ~ 1,
    (COUNTRY_NAME == "Ecuador" & YEAR == 1976) ~ 1,
    (COUNTRY_NAME == "Dominican Republic" & YEAR == 1994) ~ 1,
    (COUNTRY_NAME == "El Salvador" & YEAR == 1990) ~ 1,
    (COUNTRY_NAME == "Mexico" & YEAR == 1990) ~ 1,
    (COUNTRY_NAME == "Nicaragua" & YEAR == 1979) ~ 1,
    (COUNTRY_NAME == "Panama" & YEAR == 1989) ~ 1,
    (COUNTRY_NAME == "Paraguay" & YEAR == 1989) ~ 1,
    (COUNTRY_NAME == "Peru" & YEAR == 2000) ~ 1,
    TRUE ~ 0
  )) |>
  mutate(CRITICAL_JUNCTURE = as.numeric(CRITICAL_JUNCTURE))

# Checking classification ------------------------------------------------------
sample <- standardized_data_final |>
  select(YEAR,
         COUNTRY_NAME,
         VDEM_STATUS_BASELINE,
         VDEM_STATUS_IDEAL,
         VDEM_STATUS_ENDLINE) |>
  rename(VDEM_STATUS_20_25_PERCENTILES = VDEM_STATUS_BASELINE,
         VDEM_STATUS_30_35_PERCENTILES = VDEM_STATUS_IDEAL,
         VDEM_STATUS_20_40_PERCENTILES = VDEM_STATUS_ENDLINE)

# Saving classified country-year observations ----------------------------------
write.csv(sample,
          "countries_classified.csv")
```

\newpage

# 1. Executive Summary

This brief summary estimates:

* Country-year linear and logistic coefficients of 20 numeric predictors on a dichotomous outcome: whether a country is `Fragile` or `Non-Fragile` in a given year. The strategy is repeated with two-year lags of the same 20 numeric predictors. 

* The performance of the linear model (which returns the marginal effect of of one unit increase in the predictor over the target variable) and of the logistic model (which estimates the log-likelihood that the dichotomous outcome is true for a unit increase in the predictor). Predictors are normalized, so coefficients are directly comparable across the two models: the coefficients capture the change in the target variable for one standard deviation (positive or negative) in the predictor variable. The strategy is repeated with two-year lags of the same 20 numeric predictors.

* The LASSO regression parameters, tackling overfitting and multicollinaerity in identifying less relevant predictors. 

* The confusion matrix, which evaluates the accuracy of trained data in correctly identifying the class a country falls in for a given year (whether `Fragile` or `Non-Fragile`)

\newpage

# 2. Descriptive Statistics 

```{r, echo = FALSE, message = FALSE, Wwarning = FALSE}
descriptive_data <- final_clean_percentiles_data |>
  # Selecting only relevant predictors 
  select(GDP_PER_CAPITA,
         LOG_GDP_PER_CAPITA,
         ODA_RECEIVED_PER_CAPITA, 
         GDP_GROWTH, 
         GDP_PER_CAPITA_GROWTH, 
         GDP_DEFLATOR, 
         POLITICAL_REGIME, 
         ELECTORAL_DEMOCRACY_SCORE, 
         LIBERAL_DEMOCRACY_SCORE, 
         TERRITORIAL_FRAGMENTATION, 
         INSTITUTIONAL_DEMOCRACY_SOCRE, 
         INSTITUTIONAL_AUTOCRACY_SCORE,
         COMBINED_POLITY_SCORE, 
         REGIME_DURABILITY_YEARS, 
         INSTITUTIONAL_EXECUTIVE_RECRUTIMENT, 
         POLITICAL_COMPETITION_SCORE, 
         PARTIAL_DEMOCRACY_WITH_FACTIONALISM, 
         CONFLICT_INTENSITY_YEAR, 
         CONFLICT_CUMULATIVE_INTENSITY_ACROSS_YEARS, 
         N_WAR_FRONTS, 
         MAX_CONFLICT_INTENSITY, 
         AVG_CONFLICT_INTENSITY, 
         N_TOTAL_TROOPS)
```

```{r, descriptive-stats, echo = FALSE, message = FALSE, warning = FALSE, results = 'asis'}
stargazer(descriptive_data,
          title = "Descriptive Statistics Country Panel (1970-2022)",
          out.header = FALSE,
          header = FALSE,
          digits = 2,
          covariate.labels = c("GDP Per Capita",
                               "GDP Per Capita (logged)",
                               "ODA Received Per Capita",
                               "GDP Growth",
                               "GDP Per Capita Growth",
                               "GDP Deflator",
                               "Political Regime",
                               "Electoral Democracy Score",
                               "Liberal Democracy Score",
                               "Territorial Fragmentation",
                               "Institutional Democracy Score",
                               "Institutional Autocracy Score",
                               "Combined Polity Score",
                               "Regime Durability (Years)",
                               "Executive Recruitment",
                               "Political Competition Score",
                               "Partial Democracy With Factionalism",
                               "Conflict Intensity (Year)",
                               "Conflict Cumul. Intensity Across Years",
                               "N War Fronts",
                               "Max Conflict Intensity",
                               "Avg. Conflict Intensity",
                               "N Total Troops"))
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# ------------------------------------------------------------------------------
# GDP Data ---------------------------------------------------------------------
# ------------------------------------------------------------------------------

# Raw GDP histogram
p1 <- ggplot(standardized_data_final, aes(x = GDP_PER_CAPITA)) +
  geom_histogram(bins = 30, fill = "#6cbf84", color = "grey30", alpha = 0.9) +
  labs(
    title = "GDP per Capita (Raw)",
    x = "GDP per Capita (USD)",
    y = "Number of Countries"
  ) +
  theme_bw(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16),
    axis.text.x = element_text(vjust = 0.5),
    legend.position = "none"
  )

# Log GDP histogram
p2 <- ggplot(standardized_data_final, aes(x = log(GDP_PER_CAPITA))) +
  geom_histogram(bins = 30, fill = "#f26968", color = "grey30", alpha = 0.9) +
  labs(
    title = "GDP per Capita (Log)",
    x = "log(GDP per Capita)",
    y = "Number of Countries"
  ) +
  theme_bw(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16),
    axis.text.x = element_text(vjust = 0.5),
    legend.position = "none"
  )

# gridExtra
gridExtra::grid.arrange(p1, p2, ncol = 2)

# ------------------------------------------------------------------------------
# Time trends ------------------------------------------------------------------
# ------------------------------------------------------------------------------

# Count number of countries per category per year
trend_data <- standardized_data_final |>
  group_by(YEAR, 
           VDEM_STATUS_IDEAL) |>
  summarise(count = n(), .groups = "drop") |>
  mutate(YEAR = as.numeric(YEAR)) |>
  arrange(VDEM_STATUS_IDEAL,
          YEAR)

# Plotting the time trends -----------------------------------------------------
ggplot(trend_data, aes(x = YEAR, y = count, color = VDEM_STATUS_IDEAL, group = VDEM_STATUS_IDEAL)) +
  geom_line(size = 1) +
  geom_point(size = 1.5) +
  scale_color_manual(values = c("Fragile" = "#f26968",
                                "Non Fragile" = "#6cbf84",
                                "Transitioning" = "#323339")) +
  scale_x_continuous(breaks = seq(min(trend_data$YEAR), max(trend_data$YEAR), 5)) +
  labs(title = "Yearly Trends of VDEM Fragility Stauts (1970-2022)",
       x = "Year",
       y = "Number of Countries",
       color = "Status") +
  theme_light(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16),
    axis.text.x = element_text(vjust = 0.5),
    legend.position = "top"
  )

# ------------------------------------------------------------------------------
# INCOME DISTRIBUTION ----------------------------------------------------------
# ------------------------------------------------------------------------------

# Preparing the data 
df_gdp_quartiles <- standardized_data_final |>
  mutate(
    YEAR = as.numeric(YEAR)
  ) |>
  # Quartiles WITHIN year (1 = lowest income, 4 = highest)
  group_by(YEAR) |>
  mutate(gdp_pc_quartile = ntile(GDP_PER_CAPITA, 4)) |>
  ungroup() |>
  filter(!is.na(gdp_pc_quartile), !is.na(VDEM_STATUS_IDEAL), !is.na(YEAR))

# Aggregatting counts by year, status, and quartile
trend_by_q <- df_gdp_quartiles |>
  group_by(YEAR, 
           gdp_pc_quartile, 
           VDEM_STATUS_IDEAL) |>
  summarise(count = n(), .groups = "drop") |>
  mutate(
    gdp_pc_quartile = factor(
      gdp_pc_quartile,
      levels = 1:4,
      labels = c("Q1: Lowest income", "Q2", "Q3", "Q4: Highest income")
    )
  )

# Plotting time trends by income distribution ----------------------------------
ggplot(
  trend_by_q,
  aes(x = YEAR, y = count, color = VDEM_STATUS_IDEAL, group = VDEM_STATUS_IDEAL)
) +
  geom_line(size = 0.5) +
  geom_point(size = 1) +
  scale_color_manual(values = c(
    "Fragile" = "#f26968",
    "Non Fragile" = "#6cbf84",
    "Transitioning" = "#323339"
  )) +
  scale_x_continuous(breaks = seq(min(trend_by_q$YEAR), max(trend_by_q$YEAR), 10)) +
  labs(
    title = "VDEM Fragility Stauts by GDP per capita Quartile (1970-2022)",
    x = "Year",
    y = "Number of Countries",
    color = "Status"
  ) +
  facet_wrap(~ gdp_pc_quartile, ncol = 2) +  # rectangle headers you liked
  theme_bw(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16),
    axis.text.x = element_text(vjust = 0.5),
    legend.position = "top"
  )

# ------------------------------------------------------------------------------
# POLITY DISTRIBUTION ----------------------------------------------------------
# ------------------------------------------------------------------------------

# Preparing the data 
df_polity_quartiles <- standardized_data_final |>
  mutate(
    YEAR = as.numeric(YEAR)
  ) |>
  # Quartiles WITHIN year (1 = lowest income, 4 = highest)
  group_by(YEAR) |>
  mutate(polity_quartile = ntile(COMBINED_POLITY_SCORE, 4)) |>
  ungroup() |>
  filter(!is.na(polity_quartile), !is.na(VDEM_STATUS_IDEAL), !is.na(YEAR))

# Aggregatting counts by year, status, and quartile
trend_by_q_polity <- df_polity_quartiles |>
  group_by(YEAR, 
           polity_quartile, 
           VDEM_STATUS_IDEAL) |>
  summarise(count = n(), .groups = "drop") |>
  mutate(
    polity_quartile = factor(
      polity_quartile,
      levels = 1:4,
      labels = c("Q1: Strongly Autocratic", "Q2", "Q3", "Q4: Strongly Democratic")
    )
  )

# Plotting the data by polity distribution -------------------------------------
ggplot(
  trend_by_q_polity,
  aes(x = YEAR, y = count, color = VDEM_STATUS_IDEAL, group = VDEM_STATUS_IDEAL)
) +
  geom_line(size = 0.5) +
  geom_point(size = 1) +
  scale_color_manual(values = c(
    "Fragile" = "#f26968",
    "Non Fragile" = "#6cbf84",
    "Transitioning" = "#323339"
  )) +
  scale_x_continuous(breaks = seq(min(trend_by_q$YEAR), max(trend_by_q$YEAR), 10)) +
  labs(
    title = "VDEM Fragility Stauts by Polity Score (1970-2022)",
    x = "Year",
    y = "Number of Countries",
    color = "Status"
  ) +
  facet_wrap(~ polity_quartile, ncol = 2) +  
  theme_bw(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16),
    axis.text.x = element_text(vjust = 0.5),
    legend.position = "top"
  )
```

\newpage 

# 3. Success Episodes 

## 3.1. ODA: Pre vs. Post-Escape Pooled Estimates

The scatterplot below examines how ODA per capita evolves around a country’s successful breakthrough from fragility. For each escape case, we compute the average ODA per capita in the pre-escape period (x-axis, log-transformed) and the corresponding change in ODA per capita between the pre- and post-escape periods. Each point represents a single country. The solid black line shows the ordinary least squares (OLS) fit with 95% confidence intervals based on heteroskedasticity-consistent (HC) standard errors.

To minimize the influence of extreme ODA spikes and unequal exposure periods, we restrict attention to a 3-year window around each escape year and winsorize (i.e., replace outliers with less extreme observations) ODA values at the 1st and 99th percentiles. Within this symmetric window, ODA levels are averaged for each country before and after the escape event, and differences are computed (`Post – Pre`).

```{r, echo = FALSE, message = FALSE, warning = FALSE}

# Setting the critical junctures
standardized_data_final <- standardized_data_final |>
  mutate(YEAR = as.numeric(YEAR)) |>
  mutate(VDEM_ESCAPE_YEAR = as.numeric(case_when(
    COUNTRY_NAME == "Brazil"                & YEAR == 1988 ~ 1,
    COUNTRY_NAME == "Gabon"                 & YEAR == 1990 ~ 1,
    COUNTRY_NAME == "Gambia, The"           & YEAR == 2017 ~ 1,
    COUNTRY_NAME == "Kenya"                 & YEAR == 2010 ~ 1,
    COUNTRY_NAME == "Mauritania"            & YEAR == 2006 ~ 1,
    COUNTRY_NAME == "Sierra Leone"          & YEAR == 2018 ~ 1,
    COUNTRY_NAME == "Tunisia"               & YEAR == 2010 ~ 1,
    COUNTRY_NAME == "Uganda"                & YEAR == 1987 ~ 1,
    COUNTRY_NAME == "Iran, Islamic Rep."    & YEAR == 1997 ~ 1,
    COUNTRY_NAME == "Nepal"                 & YEAR == 2007 ~ 1,
    COUNTRY_NAME == "Philippines"           & YEAR == 1987 ~ 1,
    COUNTRY_NAME == "Sri Lanka"             & YEAR == 2009 ~ 1,
    COUNTRY_NAME == "Thailand"              & YEAR == 1991 ~ 1,
    COUNTRY_NAME == "Bosnia and Herzegovina"& YEAR == 1995 ~ 1,
    COUNTRY_NAME == "Serbia"                & YEAR == 2001 ~ 1,
    COUNTRY_NAME == "Georgia"               & YEAR == 2004 ~ 1,
    COUNTRY_NAME == "Kyrgyz Republic"       & YEAR == 2011 ~ 1,
    COUNTRY_NAME == "Armenia"               & YEAR == 2018 ~ 1,
    COUNTRY_NAME == "Argentina"             & YEAR == 1983 ~ 1,
    COUNTRY_NAME == "Colombia"              & YEAR == 2002 ~ 1,
    COUNTRY_NAME == "Ecuador"               & YEAR == 1976 ~ 1,
    COUNTRY_NAME == "Dominican Republic"    & YEAR == 1997 ~ 1,
    COUNTRY_NAME == "El Salvador"           & YEAR == 2009 ~ 1,
    COUNTRY_NAME == "Mexico"                & YEAR == 1994 ~ 1,
    COUNTRY_NAME == "Nicaragua"             & YEAR == 1990 ~ 1,
    COUNTRY_NAME == "Panama"                & YEAR == 1990 ~ 1,
    COUNTRY_NAME == "Peru"                  & YEAR == 2001 ~ 1,
    TRUE ~ 0
  )))

# ------------------------------------------------------------------------------
# POOLED PLOT ------------------------------------------------------------------
# ------------------------------------------------------------------------------

# Parameters -------------------------------------------------------------------
ODA_VAR <- "ODA_RECEIVED_PER_CAPITA"  

# Minimal Pre and Post-Escape years 
MIN_N_PER_SIDE <- 2                 

# Identifying escape years per country
escape_years <- standardized_data_final |>
  filter(VDEM_ESCAPE_YEAR == 1) |>
  select(COUNTRY_NAME, 
         ESCAPE_YEAR = YEAR)

# Keeping escape countries and tagging Pre/Post 
# (We exclude the escape year itself)
df_ep <- standardized_data_final |>
  semi_join(escape_years, by = "COUNTRY_NAME") |>
  left_join(escape_years, by = "COUNTRY_NAME") |>
  mutate(
    YEAR = as.numeric(YEAR),
    period = case_when(
      YEAR < ESCAPE_YEAR ~ "Pre",
      YEAR > ESCAPE_YEAR ~ "Post",
      TRUE ~ NA_character_
    )
  ) |>
  filter(!is.na(period))

# Summarising mean ODA per country and period
sum_by_period <- df_ep |>
  group_by(COUNTRY_NAME, 
           period) |>
  summarise(
    mean_oda = mean(.data[[ODA_VAR]], na.rm = TRUE),
    n_years  = sum(!is.na(.data[[ODA_VAR]])),
    .groups = "drop"
  )

# Widening the table Pre/Post means, keeping countries with data on both sides
pre_post <- sum_by_period |>
  pivot_wider(
    names_from = period, values_from = c(mean_oda, n_years)
  ) |>
  # Keeping countries with enough observations on both sides
  filter(!is.na(mean_oda_Pre), !is.na(mean_oda_Post),
         n_years_Pre >= MIN_N_PER_SIDE, n_years_Post >= MIN_N_PER_SIDE) |>
  transmute(
    COUNTRY_NAME,
    pre_mean  = mean_oda_Pre,
    post_mean = mean_oda_Post,
    delta     = post_mean - pre_mean
  )

# Scatterplot with 95% confidence interval 
p <- ggplot(pre_post, aes(x = pre_mean, y = delta)) +
  geom_point(size = 2.5, alpha = 0.85) +
  geom_smooth(method = "lm", se = TRUE, size = 1, color = "black", fill = "grey80") +
  scale_x_continuous(trans = "log1p") +
  labs(
    title = "Change in ODA per capita: Escape vs. Pre-escape Level",
    subtitle = "Each dot is a country; line = OLS fit with 95% CI",
    x = "Pre-escape mean ODA per capita (log1p)",
    y = "Post – Pre change in ODA per capita"
  ) +
  theme_classic(base_size = 13)

p

# ------------------------------------------------------------------------------
# Symetric windows around pre- and post-escape ---------------------------------
# ------------------------------------------------------------------------------
WINDOW <- 3  # years on each side

df_ep <- standardized_data_final |>
  semi_join(escape_years, by = "COUNTRY_NAME") |>
  left_join(escape_years, by = "COUNTRY_NAME") |>
  mutate(period = case_when(
    YEAR >= ESCAPE_YEAR - WINDOW & YEAR <  ESCAPE_YEAR ~ "Pre",
    YEAR >  ESCAPE_YEAR          & YEAR <= ESCAPE_YEAR + WINDOW ~ "Post",
    TRUE ~ NA_character_
  )) |>
  filter(!is.na(period))

# ------------------------------------------------------------------------------
# Winsorizing extreme ODA before averaging -------------------------------------
# ------------------------------------------------------------------------------
winsor01 <- function(x, p = 0.01) {
  q <- quantile(x, c(p, 1-p), na.rm = TRUE)
  pmin(pmax(x, q[[1]]), q[[2]])
}

df_ep <- df_ep |>
  mutate(ODA_W = winsor01(.data[[ODA_VAR]], p = 0.01))

# ------------------------------------------------------------------------------
# Paired tests with robust SEs -------------------------------------------------
# ------------------------------------------------------------------------------
sum_by_period <- df_ep |>
  group_by(COUNTRY_NAME, period) |>
  summarise(
    mean_oda = mean(ODA_W, na.rm = TRUE),
    n_years  = sum(!is.na(ODA_W)),
    .groups = "drop"
  )

pre_post <- sum_by_period |>
  pivot_wider(names_from = period, values_from = c(mean_oda, n_years)) |>
  filter(!is.na(mean_oda_Pre), !is.na(mean_oda_Post),
         n_years_Pre >= MIN_N_PER_SIDE, n_years_Post >= MIN_N_PER_SIDE) |>
  transmute(COUNTRY_NAME,
            pre_mean  = mean_oda_Pre,
            post_mean = mean_oda_Post,
            delta     = post_mean - pre_mean)

# Paired test
t_out <- with(pre_post, t.test(post_mean, pre_mean, paired = TRUE))

# Robust slope with sandwich SEs (country-level points)
mod <- lm(delta ~ log1p(pre_mean), data = pre_post)
rob_se <- sqrt(diag(sandwich::vcovHC(mod, type = "HC1")))
est <- coef(summary(mod))
est[,2] <- rob_se  # replace SE

# ------------------------------------------------------------------------------
# Final Robust Plot ------------------------------------------------------------
# ------------------------------------------------------------------------------
p <- ggplot(pre_post, aes(x = pre_mean, y = delta)) +
  geom_point(size = 2.5, alpha = 0.85) +
  geom_smooth(method = "lm", se = TRUE, size = 1, color = "black", fill = "grey80") +
  scale_x_continuous(trans = "log1p") +
  labs(
    title = "Change in ODA per capita around escape (3 years)",
    subtitle = "Each dot = country; line = OLS with HC-robust 95% CI",
    x = "Pre-escape mean ODA per capita (log1p, winsorized 1%)",
    y = "Post – Pre change in ODA per capita (winsorized 1%)"
  ) +
  theme_classic(base_size = 13)

p

# ------------------------------------------------------------------------------
# Descriptive table ------------------------------------------------------------
# ------------------------------------------------------------------------------
summary_tbl <- tibble(
  n_countries   = nrow(pre_post),
  mean_pre_ODA  = mean(pre_post$pre_mean,  na.rm = TRUE),
  mean_post_ODA = mean(pre_post$post_mean, na.rm = TRUE),
  mean_delta    = mean(pre_post$delta,     na.rm = TRUE),
  t_p_value     = signif(t_out$p.value, 3),
  slope         = round(coef(mod)[2], 3),
  robust_se     = round(rob_se[2], 3))

knitr::kable(summary_tbl,
             caption = "ODA dynamics around escape from fragility (3 years window)",
             digits = 3,
             align = "c")
```

A paired t-test across 27 countries indicates that ODA per capita rises significantly following escape (mean increase around $0.31$ SD, $p = 0.008$). The fitted slope suggests that countries already receiving higher levels of aid tend to experience somewhat larger post-escape increases. We recognize that this association is imprecisely estimated due to the small sample size and high cross-country variance in aid levels.

Taken together, these results imply that aid inflows tend to follow rather than precede successful exits from fragility. Donors appear to increase assistance once credible political and institutional progress is underway—consistent with ODA functioning more as an *ex-post* reward or reinforcement mechanism than as a fundamental pre-condition or catalyst for state recovery. 

## 3.2. ODA: Pre vs. Post-Escape Country-Estimates

```{r, echo = FALSE, message = FALSE, warning = FALSE}

# Parameters -------------------------------------------------------------------

# log1p y-scale (zeros/ skew)
USE_LOG <- TRUE      

# Tagging Pre/Post (dropping the escape year itself)
df_ep <- standardized_data_final |>
  semi_join(escape_years, by = "COUNTRY_NAME") |>
  left_join(escape_years, by = "COUNTRY_NAME") |>
  mutate(
    YEAR = as.numeric(YEAR),
    period = case_when(
      YEAR < ESCAPE_YEAR ~ "Pre",
      YEAR > ESCAPE_YEAR ~ "Post",
      TRUE               ~ NA_character_
    )
  ) |>
  filter(!is.na(period), !is.na(.data[[ODA_VAR]]))

# Keeping only countries with data on BOTH sides
eligible <- df_ep |>
  count(COUNTRY_NAME, 
        period) |>
  tidyr::pivot_wider(names_from = period, 
                     values_from = n, 
                     values_fill = 0) |>
  filter(Pre >= MIN_N_PER_SIDE, Post >= MIN_N_PER_SIDE) |>
  pull(COUNTRY_NAME)

df_ep <- df_ep |> 
  filter(COUNTRY_NAME %in% eligible)

# Building the single Pre/Post plot for each country
plot_one_country <- function(cname, use_log = TRUE) {
  dat <- df_ep |>
    filter(COUNTRY_NAME == cname)

  # Welch t-test (Post vs Pre)
  tt <- t.test(dat[[ODA_VAR]] ~ dat$period, var.equal = FALSE)
  p_label <- ifelse(tt$p.value < 0.001, "p<0.001", sprintf("p=%.3f", tt$p.value))

  # Summary for CI bars
  summ <- dat |>
    group_by(period) |>
    summarise(
      mean_oda = mean(.data[[ODA_VAR]], na.rm = TRUE),
      se_oda   = sd(.data[[ODA_VAR]], na.rm = TRUE) / sqrt(n()),
      n = n(),
      .groups = "drop"
    ) |>
    mutate(
      ci_low  = mean_oda - 1.96 * se_oda,
      ci_high = mean_oda + 1.96 * se_oda,
      period  = factor(period, levels = c("Pre", "Post"))
    )

  p <- ggplot(summ, aes(x = period, y = mean_oda, color = period)) +
    geom_point(size = 2.8) +
    geom_errorbar(aes(ymin = ci_low, ymax = ci_high), width = 0.18, size = 0.7) +
    geom_line(aes(group = 1), color = "#f26968", linewidth = 0.6) +
    scale_color_manual(values = c("Pre" = "#f26968", "Post" = "#6cbf84")) +
    { if (use_log) scale_y_continuous(trans = "log1p") else scale_y_continuous() } +
    labs(
      title = paste0(cname, " ODA per capita: Pre vs Post (", p_label, ")"),
      subtitle = "Points = mean, bars = 95% CI. Welch t-test for Pre vs Post.",
      x = NULL,
      y = if (use_log) "Mean ODA per capita (log1p)" else "Mean ODA per capita",
      color = "Period"
    ) +
    theme_classic(base_size = 13) +
    theme(legend.position = "none",
          plot.title = element_text(face = "bold"))
  p
}

# Looping plots for all countries
countries <- unique(df_ep$COUNTRY_NAME)
plots <- setNames(map(countries, ~ plot_one_country(.x, use_log = USE_LOG)), countries)
walk(plots, print)

# ------------------------------------------------------------------------------
# Robustness -------------------------------------------------------------------
# ------------------------------------------------------------------------------
USE_LOG    <- TRUE       
WINDOW     <- 3          
MIN_N_PER_SIDE <- 2
WINSOR_P   <- 0.01       

# Function for winsorization (1%)
winsor01 <- function(x, p = 0.01) {
  if (all(is.na(x))) return(x)
  qs <- quantile(x, c(p, 1 - p), na.rm = TRUE)
  pmin(pmax(x, qs[1]), qs[2])
}

perm_pvalue <- function(x, g, B = 5000L, seed = 247) {
  # two-sample difference in means, exact/randomization p via label permutations
  set.seed(seed)
  x <- x[!is.na(x) & !is.na(g)]
  g <- g[!is.na(x) & !is.na(g)]
  if (length(unique(g)) != 2) return(NA_real_)
  obs <- mean(x[g == "Post"]) - mean(x[g == "Pre"])
  B <- as.integer(B)
  stats <- replicate(B, {
    g_star <- sample(g, length(g), replace = FALSE)
    mean(x[g_star == "Post"]) - mean(x[g_star == "Pre"])
  })
  (sum(abs(stats) >= abs(obs)) + 1) / (B + 1)
}

# Pre/Post with symmetric window + winsorization -------------------------------
escape_years <- standardized_data_final |>
  filter(VDEM_ESCAPE_YEAR == 1) |>
  select(COUNTRY_NAME, 
         ESCAPE_YEAR = YEAR)

df_ep <- standardized_data_final |>
  semi_join(escape_years, 
            by = "COUNTRY_NAME") |>
  left_join(escape_years, 
            by = "COUNTRY_NAME") |>
  mutate(
    YEAR = as.numeric(YEAR),
    period = case_when(
      YEAR >= ESCAPE_YEAR - WINDOW & YEAR <  ESCAPE_YEAR ~ "Pre",
      YEAR >  ESCAPE_YEAR          & YEAR <= ESCAPE_YEAR + WINDOW ~ "Post",
      TRUE ~ NA_character_
    )
  ) |>
  filter(!is.na(period), !is.na(.data[[ODA_VAR]])) |>
  # winsorizing within country
  group_by(COUNTRY_NAME) |>
  mutate(ODA_W = winsor01(.data[[ODA_VAR]], p = WINSOR_P)) |>
  ungroup()

# keeping countries with data on both sides ------------------------------------
eligible <- df_ep |>
  count(COUNTRY_NAME, period) |>
  pivot_wider(names_from = period, values_from = n, values_fill = 0) |>
  filter(Pre >= MIN_N_PER_SIDE, Post >= MIN_N_PER_SIDE) |>
  pull(COUNTRY_NAME)

df_ep <- df_ep |>
  filter(COUNTRY_NAME %in% eligible) |>
  mutate(
    # stats on log scale
    ODA_log = log1p(ODA_W)  
  )

# Looping plots ----------------------------------------------------------------
plot_one_country <- function(cname, use_log = TRUE) {
  dat <- df_ep |>
    filter(COUNTRY_NAME == cname)

  yvar <- if (use_log) "ODA_log" else "ODA_W"

  # Welch t-test
  f <- stats::as.formula(paste(yvar, "~ period"))
  tt <- stats::t.test(f, data = dat, var.equal = FALSE)

  # Permutation p-value (robust small-N check)
  p_perm <- perm_pvalue(dat[[yvar]], dat$period, B = 5000L)

  p_label <- if (!is.na(p_perm)) sprintf("p_perm=%.3f", p_perm) else sprintf("p=%.3f", tt$p.value)

  # Summary means/SE/CI on the same scale used for the test
  summ <- dat |>
    group_by(period) |>
    summarise(
      mean_y = mean(.data[[yvar]], na.rm = TRUE),
      se_y   = stats::sd(.data[[yvar]], na.rm = TRUE) / sqrt(dplyr::n()),
      n      = n(),
      .groups = "drop"
    ) |>
    mutate(
      ci_low  = mean_y - 1.96 * se_y,
      ci_high = mean_y + 1.96 * se_y,
      period  = factor(period, levels = c("Pre", "Post"))
    )

  ggplot(summ, ggplot2::aes(x = period, y = mean_y, color = period)) +
    geom_point(size = 2.8) +
    geom_errorbar(aes(ymin = ci_low, ymax = ci_high), width = 0.18, linewidth = 0.7) +
    geom_line(aes(group = 1), color = "#f26968", linewidth = 0.6) +
    scale_color_manual(values = c("Pre" = "#f26968", "Post" = "#6cbf84")) +
    { if (use_log) scale_y_continuous() else scale_y_continuous() } +
    labs(
      title = paste0(cname, " ODA per capita: Pre vs Post (", p_label, ")"),
      subtitle = if (use_log) "Means and 95% CI on log1p-winsorized ODA; Welch t + permutation p"
                 else         "Means and 95% CI on winsorized ODA; Welch t + permutation p",
      x = NULL,
      y = if (use_log) "Mean log1p(ODA per capita)" else "Mean ODA per capita (winsorized)",
      color = "Period"
    ) +
    theme_classic(base_size = 13) +
    theme(legend.position = "none",
                   plot.title = element_text(face = "bold"))
}

# Plotting ---------------------------------------------------------------------
countries <- unique(df_ep$COUNTRY_NAME)

country_stats <- lapply(countries, function(cn) {
  dat <- filter(df_ep, COUNTRY_NAME == cn)
  yvar <- if (USE_LOG) "ODA_log" else "ODA_W"
  f <- stats::as.formula(paste(yvar, "~ period"))
  tt <- stats::t.test(f, data = dat, var.equal = FALSE)
  p_perm <- perm_pvalue(dat[[yvar]], dat$period, B = 5000L)
  data.frame(country = cn,
             p_welch = tt$p.value,
             p_perm  = p_perm,
             n_pre   = sum(dat$period == "Pre"),
             n_post  = sum(dat$period == "Post"),
             stringsAsFactors = FALSE)
})

country_stats <- bind_rows(country_stats) |>
  mutate(
    p_perm_adj = p.adjust(p_perm, method = "BH"),
    p_welch_adj= p.adjust(p_welch, method = "BH")
  )

# plots named list
plots <- setNames(purrr::map(countries, ~ plot_one_country(.x, use_log = USE_LOG)), countries)
purrr::walk(plots, print)
```

\newpage

# 4. A Classification Problem 

To validate the internal consistency and empirical grounding of our fragility classification (`Fragile`, `Transitioning` or `Non-Fragile`), we implement a supervised machine learning exercise using two distinct algorithms: multinomial logistic regression (logit) and random forest. We train both models on a set of 20 theoretically motivated structural predictors, including regime type, democracy indicators, institutional durability, economic performance, and conflict intensity (i.e., all predictors we employ in our empirical specifications). The outcome variable is our three-tier classification (\texttt{VDEM\_STATUS\_IDEAL}), derived independently of the predictors via percentile cutoffs on a normalized V-Dem latent factor. 

We label our country cases according to the subsequent taxonomy:

| Classification Label     | Fragile Threshold            | Transitioning Range              | Non-Fragile Threshold           |
|--------------------------|------------------------------|----------------------------------|---------------------------------|
| `RELAXED`                | Below 20th percentile        | 20th to 25th percentile          | Above 25th percentile           |
| `IDEAL`                  | Below 30th percentile        | 30th to 35th percentile          | Above 35th percentile           |
| `STRICT`                 | Below 20th percentile        | 20th to 40th percentile          | Above 40th percentile           |


The two supervised learning algorithms serve complementary purposes to validate our classification:

- **Multinomial Logistic Regression (Logit)** estimates the probability that a given observation belongs to one of the three mutually exclusive classes (`Fragile`, `Transitioning` or `Non-Fragile`), based on a linear combination of structural predictors. It is fully interpretable, assumes additive relationships, and provides insight into the marginal effects of each variable. Because it is transparent and theory-aligned, it serves as an ideal benchmark for testing the recoverability of our classification from exogenous structural features.

- **Random Forest** is a non-parametric ensemble method that builds hundreds of decision trees on random subsets of predictors and observations. It excels at modeling complex, nonlinear interactions and is robust to multicollinearity and overfitting. In our context, it offers a high-performing, assumption-light alternative that tests whether the classification can still be recovered without relying on linearity or additivity assumptions.

\newpage 

## 4.1. Ideal Classification

We evaluate models with repeated 10-fold cross-validation (3 repeats; 30 folds). The multinomial logit attains 80–80.5% accuracy (SE = 0.005) and multiclass log-loss = 0.488 (SE = 0.011). The random forest performs substantially better: 89.4% accuracy (SE = 0.004) and log-loss = 0.328–0.329 (SE = 0.007). Confusion matrices show the logit almost never predicts `Transitioning` (recall = 0%), while the RF recovers some of that class (recall = 21%, 9/42) and improves recall for `Fragile` (92.6% vs. 81.9%) and `Non-Fragile` (95.2% vs. 88.5%). Standard errors are small, indicating stable performance across folds.

These results support the validity of our labeling scheme: observed institutional, economic, and conflict covariates reliably recover the three-tier classification, with RF clearly dominating logit in both accuracy and probability quality (lower loss). The main weakness is the `Transitioning` class — consistent with its conceptual “in-between” status and class imbalance. Overall, the evidence indicates our tiers are empirically grounded rather than artifacts of arbitrary thresholds.

```{r, echo = FALSE, message = FALSE, warning = FALSE}

# ------------------------------------------------------------------------------
# IDEAL CLASSIFICATION ---------------------------------------------------------
# ------------------------------------------------------------------------------

# Defining our predictors 
predictors_20 <- c(
  "ODA_RECEIVED_PER_CAPITA", 
  "GDP_GROWTH", 
  "GDP_PER_CAPITA_GROWTH", 
  "GDP_DEFLATOR",
  "POLITICAL_REGIME", 
  "ELECTORAL_DEMOCRACY_SCORE", 
  "LIBERAL_DEMOCRACY_SCORE", 
  "TERRITORIAL_FRAGMENTATION", 
  "INSTITUTIONAL_DEMOCRACY_SOCRE",  
  "INSTITUTIONAL_AUTOCRACY_SCORE",
  "COMBINED_POLITY_SCORE",
  "REGIME_DURABILITY_YEARS",
  "INSTITUTIONAL_EXECUTIVE_RECRUTIMENT",
  "POLITICAL_COMPETITION_SCORE",
  "PARTIAL_DEMOCRACY_WITH_FACTIONALISM",
  "CONFLICT_INTENSITY_YEAR",
  "CONFLICT_CUMULATIVE_INTENSITY_ACROSS_YEARS",
  "N_WAR_FRONTS",
  "MAX_CONFLICT_INTENSITY",
  "AVG_CONFLICT_INTENSITY",
  "N_TOTAL_TROOPS"
)

# Preparing the data 
df <- standardized_data_final |>
  # Dropping COUNTRY_NAME identifier
  ungroup() |>
  # Selecting the relevant predictors
  select(all_of(predictors_20), 
         # Selecting the target variable 
         VDEM_STATUS_IDEAL) |>
  filter(!is.na(VDEM_STATUS_IDEAL)) |>
  # Dropping NAs
  drop_na() |>
  # Setting target variable as factor
  mutate(VDEM_STATUS_IDEAL = as.factor(VDEM_STATUS_IDEAL)) 

# Train/test split
data_split <- initial_split(df, strata = VDEM_STATUS_IDEAL)
train_data <- training(data_split)
test_data  <- testing(data_split)

# Recipe
rec <- recipe(VDEM_STATUS_IDEAL ~ ., data = train_data) |>
  step_zv(all_predictors()) |>
  step_normalize(all_numeric_predictors())

# ------------------------------------------------------------------------------
# Model specifications ---------------------------------------------------------
# ------------------------------------------------------------------------------

# Logit Model ------------------------------------------------------------------
logit_spec <- multinom_reg(mode = "classification") |> 
  set_engine("nnet")

# Random Forest Model ----------------------------------------------------------
rf_spec <- rand_forest(mode = "classification", 
                       trees = 500) |>
  set_engine("ranger", 
             importance = "permutation")

# Workflows --------------------------------------------------------------------

# Logit model workflow
logit_wf <- workflow() |> 
  add_model(logit_spec) |> 
  add_recipe(rec)

# Random Forest workflow
rf_wf    <- workflow() |> 
  add_model(rf_spec) |> 
  add_recipe(rec)

# Cross-Validation Setup -------------------------------------------------------
folds <- vfold_cv(train_data, 
                  v = 10, 
                  repeats = 3, 
                  strata = VDEM_STATUS_IDEAL)

# Cross-Validation model evaluation --------------------------------------------

# Logit model
logit_res <- fit_resamples(logit_wf, 
                           folds, 
                           metrics = metric_set(accuracy, 
                                                mn_log_loss))

# Random Forest 
rf_res    <- fit_resamples(rf_wf, 
                           folds, 
                           metrics = metric_set(accuracy, 
                                                mn_log_loss))

# Final model fits -------------------------------------------------------------

# Logit model 
logit_fit <- fit(logit_wf, 
                 data = train_data)

# Random Forest model 
rf_fit    <- fit(rf_wf, 
                 data = train_data)

#-------------------------------------------------------------------------------
# PREDICTION -------------------------------------------------------------------
# ------------------------------------------------------------------------------

# Logit
logit_preds <- predict(logit_fit, test_data, type = "class") |>
  bind_cols(truth = test_data$VDEM_STATUS_IDEAL)

# Random Forest 
rf_preds <- predict(rf_fit, test_data, type = "class") |>
  bind_cols(truth = test_data$VDEM_STATUS_IDEAL)

# ------------------------------------------------------------------------------
# CONFUSION MATRICES + ACCURACY ------------------------------------------------
# ------------------------------------------------------------------------------

# computing confusion matrices
conf_mat_logit <- conf_mat(logit_preds, truth = truth, estimate = .pred_class)
conf_mat_rf    <- conf_mat(rf_preds, truth = truth, estimate = .pred_class)

# Computing model accuracy 
accuracy_logit <- accuracy(logit_preds, truth = truth, estimate = .pred_class)$.estimate
accuracy_rf    <- accuracy(rf_preds, truth = truth, estimate = .pred_class)$.estimate

# Plotting the matrices
autoplot(conf_mat_logit, type = "heatmap") +
  scale_fill_gradient(low = "#deebf7", high = "#3182bd") +
  labs(
    title = paste0("Logit Confusion Matrix (Accuracy = ", round(accuracy_logit * 100, 1), "%)"),
    fill = "Count"
  ) +
  theme_minimal(base_size = 14)

autoplot(conf_mat_rf, type = "heatmap") +
  scale_fill_gradient(low = "#fee5d9", high = "#cb181d") +
  labs(
    title = paste0("RF Confusion Matrix (Accuracy = ", round(accuracy_rf * 100, 1), "%)"),
    fill = "Count"
  ) +
  theme_minimal(base_size = 14)

# Variable importance ----------------------------------------------------------
vip(extract_fit_parsnip(rf_fit), num_features = 20, geom = "col") +
  labs(title = "RF Variable Importance") +
  theme_classic(base_size = 14)

# Metrics ----------------------------------------------------------------------
collect_metrics(logit_res)
collect_metrics(rf_res) 

# ------------------------------------------------------------------------------
# METRICS DISPLAY --------------------------------------------------------------
# ------------------------------------------------------------------------------
metrics_tbl <- bind_rows(
  collect_metrics(logit_res) |> mutate(Model = "Logit"),
  collect_metrics(rf_res)    |> mutate(Model = "Random Forest")
) |>
  mutate(.metric = as.character(.metric)) |>
  mutate(.metric = dplyr::recode(
    .metric,
    "accuracy"    = "Accuracy",
    "mn_log_loss" = "Multinomial Log Loss",
    .default      = .metric
  )) |>
  select(.metric, Model, mean, std_err) |>
  pivot_wider(
    names_from  = Model,
    values_from = c(mean, std_err),
    names_glue  = "{Model} {.value}"
  )

metrics_tbl_print <- metrics_tbl |>
  mutate(
    `Logit mean`            = ifelse(.metric == "Accuracy",
                                     scales::percent(`Logit mean`, accuracy = 0.1),
                                     sprintf("%.3f", `Logit mean`)),
    `Random Forest mean`    = ifelse(.metric == "Accuracy",
                                     scales::percent(`Random Forest mean`, accuracy = 0.1),
                                     sprintf("%.3f", `Random Forest mean`)),
    `Logit std_err`         = sprintf("%.3f", `Logit std_err`),
    `Random Forest std_err` = sprintf("%.3f", `Random Forest std_err`)
  )

knitr::kable(metrics_tbl_print, caption = "Cross-Validated Performance (10-fold CV, 3 repeats)")
```

\newpage

## 4.2. Relaxed Classification

As a robustness check, we re-estimate the supervised validation using an alternative target variable, which narrows the Transitioning window (`Fragile` = bottom 20% of the V-Dem fragility index; `Non-Fragile` = top 25%; observations between 20–25% coded as `Transitioning`). This increases separation between adjacent classes and provides a more conservative test of internal validity.

Under this scheme, the multinomial logit attains 84.8% accuracy (SE = 0.005) with multiclass log-loss = 0.408 (SE = 0.019). The random forest clearly dominates, reaching 92.3% accuracy (SE = 0.004) and log-loss = 0.262 (SE = 0.006). Confusion matrices show the logit almost never predicts Transitioning (= 2/24, ~8% recall), while the RF recovers a meaningful share of that class (= 10/24, ~42% recall) and improves recall for Fragile (= 91% vs. 74%) and Non-Fragile (= 97% vs. 92%). Misclassifications remain concentrated along adjacent categories, with very little confusion between `Fragile` and `Non-Fragile`.

The baseline results reinforce the empirical coherence of our labels: using only structural covariates, models — especially the Random Forest — recover the hand-coded tiers with high accuracy and well-calibrated probabilities (lower loss). Variable-importance diagnostics indicate democracy/institutional indicators (e.g., *Liberal Democracy Score*, *Electoral Democracy Score*, *Political Regime*, *Institutional Executive Recruitment*, *Combined Polity Score*) as dominant predictors, with conflict intensity measures playing a smaller role. The main weakness is (again) the `Transitioning` class, consistent with its narrow band and class imbalance.

```{r, echo = FALSE, message = FALSE, warning = FALSE}

# ------------------------------------------------------------------------------
# RELAXED CLASSIFICATION -------------------------------------------------------
# ------------------------------------------------------------------------------

# Defining our predictors 
predictors_20 <- c(
  "ODA_RECEIVED_PER_CAPITA", 
  "GDP_GROWTH", 
  "GDP_PER_CAPITA_GROWTH", 
  "GDP_DEFLATOR",
  "POLITICAL_REGIME", 
  "ELECTORAL_DEMOCRACY_SCORE", 
  "LIBERAL_DEMOCRACY_SCORE", 
  "TERRITORIAL_FRAGMENTATION", 
  "INSTITUTIONAL_DEMOCRACY_SOCRE",  
  "INSTITUTIONAL_AUTOCRACY_SCORE",
  "COMBINED_POLITY_SCORE",
  "REGIME_DURABILITY_YEARS",
  "INSTITUTIONAL_EXECUTIVE_RECRUTIMENT",
  "POLITICAL_COMPETITION_SCORE",
  "PARTIAL_DEMOCRACY_WITH_FACTIONALISM",
  "CONFLICT_INTENSITY_YEAR",
  "CONFLICT_CUMULATIVE_INTENSITY_ACROSS_YEARS",
  "N_WAR_FRONTS",
  "MAX_CONFLICT_INTENSITY",
  "AVG_CONFLICT_INTENSITY",
  "N_TOTAL_TROOPS"
)

# Preparing the data 
df <- standardized_data_final |>
  # Dropping COUNTRY_NAME identifier
  ungroup() |>
  # Selecting the relevant predictors
  select(all_of(predictors_20), 
         # Selecting the target variable 
         VDEM_STATUS_BASELINE) |>
  filter(!is.na(VDEM_STATUS_BASELINE)) |>
  # Dropping NAs
  drop_na() |>
  # Setting target variable as factor
  mutate(VDEM_STATUS_BASELINE = as.factor(VDEM_STATUS_BASELINE)) 

# Train/test split
data_split <- initial_split(df, strata = VDEM_STATUS_BASELINE)
train_data <- training(data_split)
test_data  <- testing(data_split)

# Recipe
rec <- recipe(VDEM_STATUS_BASELINE ~ ., data = train_data) |>
  step_zv(all_predictors()) |>
  step_normalize(all_numeric_predictors())

# ------------------------------------------------------------------------------
# Model specifications ---------------------------------------------------------
# ------------------------------------------------------------------------------

# Logit Model ------------------------------------------------------------------
logit_spec <- multinom_reg(mode = "classification") |> 
  set_engine("nnet")

# Random Forest Model ----------------------------------------------------------
rf_spec <- rand_forest(mode = "classification", 
                       trees = 500) |>
  set_engine("ranger", 
             importance = "permutation")

# Workflows --------------------------------------------------------------------

# Logit model workflow
logit_wf <- workflow() |> 
  add_model(logit_spec) |> 
  add_recipe(rec)

# Random Forest workflow
rf_wf    <- workflow() |> 
  add_model(rf_spec) |> 
  add_recipe(rec)

# Cross-Validation Setup -------------------------------------------------------
folds <- vfold_cv(train_data, 
                  v = 10, 
                  repeats = 3, 
                  strata = VDEM_STATUS_BASELINE)

# Cross-Validation model evaluation --------------------------------------------

# Logit model
logit_res <- fit_resamples(logit_wf, 
                           folds, 
                           metrics = metric_set(accuracy, 
                                                mn_log_loss))

# Random Forest 
rf_res    <- fit_resamples(rf_wf, 
                           folds, 
                           metrics = metric_set(accuracy, 
                                                mn_log_loss))

# Final model fits -------------------------------------------------------------

# Logit model 
logit_fit <- fit(logit_wf, 
                 data = train_data)

# Random Forest model 
rf_fit    <- fit(rf_wf, 
                 data = train_data)

#-------------------------------------------------------------------------------
# PREDICTION -------------------------------------------------------------------
# ------------------------------------------------------------------------------

# Logit
logit_preds <- predict(logit_fit, test_data, type = "class") |>
  bind_cols(truth = test_data$VDEM_STATUS_BASELINE)

# Random Forest 
rf_preds <- predict(rf_fit, test_data, type = "class") |>
  bind_cols(truth = test_data$VDEM_STATUS_BASELINE)

# ------------------------------------------------------------------------------
# CONFUSION MATRICES + ACCURACY ------------------------------------------------
# ------------------------------------------------------------------------------

# computing confusion matrices
conf_mat_logit <- conf_mat(logit_preds, truth = truth, estimate = .pred_class)
conf_mat_rf    <- conf_mat(rf_preds, truth = truth, estimate = .pred_class)

# Computing model accuracy 
accuracy_logit <- accuracy(logit_preds, truth = truth, estimate = .pred_class)$.estimate
accuracy_rf    <- accuracy(rf_preds, truth = truth, estimate = .pred_class)$.estimate

# Plotting the matrices
autoplot(conf_mat_logit, type = "heatmap") +
  scale_fill_gradient(low = "#deebf7", high = "#3182bd") +
  labs(
    title = paste0("Logit Confusion Matrix (Accuracy = ", round(accuracy_logit * 100, 1), "%)"),
    fill = "Count"
  ) +
  theme_minimal(base_size = 14)

autoplot(conf_mat_rf, type = "heatmap") +
  scale_fill_gradient(low = "#fee5d9", high = "#cb181d") +
  labs(
    title = paste0("RF Confusion Matrix (Accuracy = ", round(accuracy_rf * 100, 1), "%)"),
    fill = "Count"
  ) +
  theme_minimal(base_size = 14)

# Variable importance ----------------------------------------------------------
vip(extract_fit_parsnip(rf_fit), num_features = 20, geom = "col") +
  labs(title = "RF Variable Importance") +
  theme_classic(base_size = 14)

# Metrics ----------------------------------------------------------------------
collect_metrics(logit_res)
collect_metrics(rf_res) 

# ------------------------------------------------------------------------------
# METRICS DISPLAY --------------------------------------------------------------
# ------------------------------------------------------------------------------
metrics_tbl <- bind_rows(
  collect_metrics(logit_res) |> mutate(Model = "Logit"),
  collect_metrics(rf_res)    |> mutate(Model = "Random Forest")
) |>
  mutate(.metric = as.character(.metric)) |>
  mutate(.metric = dplyr::recode(
    .metric,
    "accuracy"    = "Accuracy",
    "mn_log_loss" = "Multinomial Log Loss",
    .default      = .metric
  )) |>
  select(.metric, Model, mean, std_err) |>
  pivot_wider(
    names_from  = Model,
    values_from = c(mean, std_err),
    names_glue  = "{Model} {.value}"
  )

metrics_tbl_print <- metrics_tbl |>
  mutate(
    `Logit mean`            = ifelse(.metric == "Accuracy",
                                     scales::percent(`Logit mean`, accuracy = 0.1),
                                     sprintf("%.3f", `Logit mean`)),
    `Random Forest mean`    = ifelse(.metric == "Accuracy",
                                     scales::percent(`Random Forest mean`, accuracy = 0.1),
                                     sprintf("%.3f", `Random Forest mean`)),
    `Logit std_err`         = sprintf("%.3f", `Logit std_err`),
    `Random Forest std_err` = sprintf("%.3f", `Random Forest std_err`)
  )

knitr::kable(metrics_tbl_print, caption = "Cross-Validated Performance (10-fold CV, 3 repeats)")
```

\newpage

## 4.3. Strict Classification

As a stricter robustness check, we re-ran the validation with a more stringent target variable: country–years below the 20th percentile of the normalized V-Dem factor are coded `Fragile`, those above the 40th percentile `Non-Fragile`, and `Transitioning` spans 20–40. We fit the same two learners (multinomial logit and random forest) on the 20 structural predictors and evaluated them with repeated 10-fold CV (3 repeats; 30 folds).

Under this scheme, the multinomial logit attains 74.6% accuracy (SE = 0.005) and multiclass log-loss = 0.564 (SE = 0.006). The random forest is (per usual) markedly stronger: 90.1% accuracy (SE = 0.004) and log-loss = 0.343 (SE = 0.004). Standard errors are small, indicating stable performance across folds.

Out-of-sample confusion matrices show the RF improves recall in every class — particularly `Transitioning` (= 76% vs. 56% for logit), with `Fragile` recall = 93% (RF) vs. 78% (logit) and `Non-Fragile` = 94% (RF) vs. 88% (logit). Misclassifications are largely along adjacent categories; cross-extreme mistakes are rare (e.g., `Fragile-Non-Fragile`: logit ~8–10 cases; RF ~1–3).

Even with the tighter definition, the labels are recoverable from structural covariates, and RF delivers both higher accuracy and better probability quality (lower loss). Variable-importance remains led by democratic-institutional indicators (*Liberal Democracy Score*, *Electoral Democracy Score*, *Political Regime*, *Institutional Executive Recruitment*, *Combined Polity*), with conflict measures contributing less. The chief challenge is the inherently fuzzy `Transitioning` tier; if needed, class weights or threshold tuning can further sharpen performance. At the end of the day, results reinforce that the three-tier scheme captures empirically discernible differences rather than arbitrary cutoffs.

```{r, echo = FALSE, message = FALSE, warning = FALSE}

# ------------------------------------------------------------------------------
# STRICT CLASSIFICATION --------------------------------------------------------
# ------------------------------------------------------------------------------

# Defining our predictors 
predictors_20 <- c(
  "ODA_RECEIVED_PER_CAPITA", 
  "GDP_GROWTH", 
  "GDP_PER_CAPITA_GROWTH", 
  "GDP_DEFLATOR",
  "POLITICAL_REGIME", 
  "ELECTORAL_DEMOCRACY_SCORE", 
  "LIBERAL_DEMOCRACY_SCORE", 
  "TERRITORIAL_FRAGMENTATION", 
  "INSTITUTIONAL_DEMOCRACY_SOCRE",  
  "INSTITUTIONAL_AUTOCRACY_SCORE",
  "COMBINED_POLITY_SCORE",
  "REGIME_DURABILITY_YEARS",
  "INSTITUTIONAL_EXECUTIVE_RECRUTIMENT",
  "POLITICAL_COMPETITION_SCORE",
  "PARTIAL_DEMOCRACY_WITH_FACTIONALISM",
  "CONFLICT_INTENSITY_YEAR",
  "CONFLICT_CUMULATIVE_INTENSITY_ACROSS_YEARS",
  "N_WAR_FRONTS",
  "MAX_CONFLICT_INTENSITY",
  "AVG_CONFLICT_INTENSITY",
  "N_TOTAL_TROOPS"
)

# Preparing the data 
df <- standardized_data_final |>
  # Dropping COUNTRY_NAME identifier
  ungroup() |>
  # Selecting the relevant predictors
  select(all_of(predictors_20), 
         # Selecting the target variable 
         VDEM_STATUS_ENDLINE) |>
  filter(!is.na(VDEM_STATUS_ENDLINE)) |>
  # Dropping NAs
  drop_na() |>
  # Setting target variable as factor
  mutate(VDEM_STATUS_ENDLINE = as.factor(VDEM_STATUS_ENDLINE)) 

# Train/test split
data_split <- initial_split(df, strata = VDEM_STATUS_ENDLINE)
train_data <- training(data_split)
test_data  <- testing(data_split)

# Recipe
rec <- recipe(VDEM_STATUS_ENDLINE ~ ., data = train_data) |>
  step_zv(all_predictors()) |>
  step_normalize(all_numeric_predictors())

# ------------------------------------------------------------------------------
# Model specifications ---------------------------------------------------------
# ------------------------------------------------------------------------------

# Logit Model ------------------------------------------------------------------
logit_spec <- multinom_reg(mode = "classification") |> 
  set_engine("nnet")

# Random Forest Model ----------------------------------------------------------
rf_spec <- rand_forest(mode = "classification", 
                       trees = 500) |>
  set_engine("ranger", 
             importance = "permutation")

# Workflows --------------------------------------------------------------------

# Logit model workflow
logit_wf <- workflow() |> 
  add_model(logit_spec) |> 
  add_recipe(rec)

# Random Forest workflow
rf_wf    <- workflow() |> 
  add_model(rf_spec) |> 
  add_recipe(rec)

# Cross-Validation Setup -------------------------------------------------------
folds <- vfold_cv(train_data, 
                  v = 10, 
                  repeats = 3, 
                  strata = VDEM_STATUS_ENDLINE)

# Cross-Validation model evaluation --------------------------------------------

# Logit model
logit_res <- fit_resamples(logit_wf, 
                           folds, 
                           metrics = metric_set(accuracy, 
                                                mn_log_loss))

# Random Forest 
rf_res    <- fit_resamples(rf_wf, 
                           folds, 
                           metrics = metric_set(accuracy, 
                                                mn_log_loss))

# Final model fits -------------------------------------------------------------

# Logit model 
logit_fit <- fit(logit_wf, 
                 data = train_data)

# Random Forest model 
rf_fit    <- fit(rf_wf, 
                 data = train_data)

#-------------------------------------------------------------------------------
# PREDICTION -------------------------------------------------------------------
# ------------------------------------------------------------------------------

# Logit
logit_preds <- predict(logit_fit, test_data, type = "class") |>
  bind_cols(truth = test_data$VDEM_STATUS_ENDLINE)

# Random Forest 
rf_preds <- predict(rf_fit, test_data, type = "class") |>
  bind_cols(truth = test_data$VDEM_STATUS_ENDLINE)

# ------------------------------------------------------------------------------
# CONFUSION MATRICES + ACCURACY ------------------------------------------------
# ------------------------------------------------------------------------------

# computing confusion matrices
conf_mat_logit <- conf_mat(logit_preds, truth = truth, estimate = .pred_class)
conf_mat_rf    <- conf_mat(rf_preds, truth = truth, estimate = .pred_class)

# Computing model accuracy 
accuracy_logit <- accuracy(logit_preds, truth = truth, estimate = .pred_class)$.estimate
accuracy_rf    <- accuracy(rf_preds, truth = truth, estimate = .pred_class)$.estimate

# Plotting the matrices
autoplot(conf_mat_logit, type = "heatmap") +
  scale_fill_gradient(low = "#deebf7", high = "#3182bd") +
  labs(
    title = paste0("Logit Confusion Matrix (Accuracy = ", round(accuracy_logit * 100, 1), "%)"),
    fill = "Count"
  ) +
  theme_minimal(base_size = 14)

autoplot(conf_mat_rf, type = "heatmap") +
  scale_fill_gradient(low = "#fee5d9", high = "#cb181d") +
  labs(
    title = paste0("RF Confusion Matrix (Accuracy = ", round(accuracy_rf * 100, 1), "%)"),
    fill = "Count"
  ) +
  theme_minimal(base_size = 14)

# Variable importance ----------------------------------------------------------
vip(extract_fit_parsnip(rf_fit), num_features = 20, geom = "col") +
  labs(title = "RF Variable Importance") +
  theme_classic(base_size = 14)

# Metrics ----------------------------------------------------------------------
collect_metrics(logit_res)
collect_metrics(rf_res) 

# ------------------------------------------------------------------------------
# METRICS DISPLAY --------------------------------------------------------------
# ------------------------------------------------------------------------------
metrics_tbl <- bind_rows(
  collect_metrics(logit_res) |> mutate(Model = "Logit"),
  collect_metrics(rf_res)    |> mutate(Model = "Random Forest")
) |>
  mutate(.metric = as.character(.metric)) |>
  mutate(.metric = dplyr::recode(
    .metric,
    "accuracy"    = "Accuracy",
    "mn_log_loss" = "Multinomial Log Loss",
    .default      = .metric
  )) |>
  select(.metric, Model, mean, std_err) |>
  pivot_wider(
    names_from  = Model,
    values_from = c(mean, std_err),
    names_glue  = "{Model} {.value}"
  )

metrics_tbl_print <- metrics_tbl |>
  mutate(
    `Logit mean`            = ifelse(.metric == "Accuracy",
                                     scales::percent(`Logit mean`, accuracy = 0.1),
                                     sprintf("%.3f", `Logit mean`)),
    `Random Forest mean`    = ifelse(.metric == "Accuracy",
                                     scales::percent(`Random Forest mean`, accuracy = 0.1),
                                     sprintf("%.3f", `Random Forest mean`)),
    `Logit std_err`         = sprintf("%.3f", `Logit std_err`),
    `Random Forest std_err` = sprintf("%.3f", `Random Forest std_err`)
  )

knitr::kable(metrics_tbl_print, caption = "Cross-Validated Performance (10-fold CV, 3 repeats)") 
```

## 4.4. Conclusion: Validating Fragility Classifications

Across three target definitions — **Ideal**, **Relaxed**, and **Strict** — supervised learning reliably recovers our three-tier labels from 20 structural predictors using repeated 10-fold cross-validation (3 repeats, 30 folds). Random Forest consistently outperforms multinomial logit in both overall accuracy and probability quality, with small standard errors throughout, indicating stable out-of-sample performance.

Under the **Ideal** target, multinomial logit attains about 80.0–80.5% accuracy with multiclass log-loss = 0.488, while Random Forest reaches =89.4% accuracy with log-loss = 0.328–0.329. With the **Relaxed** 20–25 scheme, performance rises further: logit achieves 84.8% accuracy and 0.408 log-loss, whereas Random Forest delivers 92.3% accuracy and 0.262 log-loss. Using the stricter Baseline 20–40 specification, logit drops to 74.6% accuracy and 0.564 log-loss, but Random Forest remains strong at 90.1% accuracy and 0.343 log-loss. In all cases, standard errors are small (roughly 0.004–0.019), underscoring the stability of these estimates.

The error structure is substantively coherent. Misclassifications concentrate along adjacent categories (i.e., `Fragile` vs. `Transitioning` and `Transitioning` vs. `Non-Fragile` — with few cross-extreme mistakes between `Fragile` and `Non-Fragile`. Multinomial logit systematically under-predicts the `Transitioning` class, while Random Forest recovers a meaningful share of those observations and also improves recall in the `Fragile` and `Non-Fragile` tiers. Variable-importance patterns are led by democratic-institutional indicators (e.g., *Liberal* and *Electoral Democracy scores*, *Political Regime*, *Institutional Executive Recruitment*, *Combined Polity*), with conflict measures contributing less, which aligns with the theoretical underpinnings of fragility.

Taken together, these results validate the internal logic of our classification. Models trained only on exogenous institutional, economic, and conflict covariates reproduce the labels derived from V-Dem, indicating that our tiers capture real, observable structure rather than arbitrary thresholds.

\newpage

## 4.5. Leakage and Class Imbalance

We deliberately adopt a **conservative evaluation strategy** to convince readers that our three-tier fragility labels reflect genuine structure in the data rather than artefacts of the modelling pipeline.  All code and a cleaned replication file are archived in the project repository; the key design choices are summarised below.

**Data partitioning**

* *Country-blocked split*: an 80 / 20 initial split created with `group_initial_split()` keeps every country’s observations together, so the test set contains **entirely unseen countries**.  
* *Grouped cross-validation*: within the training block we use 10-fold, 3-repeat grouped CV (`group_vfold_cv()`), again holding each country in a single fold.  This prevents spatial leakage while providing stable performance estimates.

**Pre-processing inside each resample**

* Remove zero-variance predictors (`step_zv`).  
* **Address class imbalance** with `step_upsample(VDEM_STATUS_IDEAL, over_ratio = 1)`, duplicating minority-tier rows only within the analysis portion of each fold.  
* Standardise numeric predictors (`step_normalize`) after up-sampling, ensuring the scaling parameters are learned solely from training data.

**Algorithms**

* *Multinomial logit* for full interpretability (`nnet::multinom`).  
* *Random forest* with 500 trees (`ranger`) and **inverse-frequency class weights** to penalise mis-classifying rare *Transitioning* cases.

**Evaluation metrics**

* Overall accuracy and multiclass log-loss (calibration).  
* **Macro-averaged recall and F-measure**, which give equal weight to each tier regardless of prevalence.

**Robustness checks**

* Re-estimate all models **without democracy-derived predictors** to rule out construct overlap.  
* Shift percentile cut-offs by ± 5 points to show thresholds are not hand-tuned.  
* Re-run with one- and two-year lagged covariates to verify that results are not driven by simultaneity.

**Headline results**

* Random forest: accuracy = 0.75, macro-F1 = 0.61, log-loss = 0.74.  
* *Transitioning* recall rises from 0 % (naïve split) to **0.38** after up-sampling and class weighting, demonstrating that the middle tier is identifiable.  
* Democracy-free specification retains 0.72 accuracy—evidence the model is not tautological.

Taken together, these diagnostics meet current best-practice standards for machine-learning validation in top political-science outlets and underpin the empirical claims in Section 2.

```{r, echo = FALSE, message = FALSE, warning = FALSE}

# Pretty table toggle: only use gt if xfun is new enough
use_gt <- rlang::is_installed("gt") && utils::packageVersion("xfun") >= "0.52"

# Avoid metric masking from caret if loaded
if ("package:caret" %in% search()) detach("package:caret", unload = TRUE)

set.seed(42)

# ---- Toggles ----
DO_IMPUTE       <- FALSE   # TRUE to add median imputation inside resamples
DO_CORR_PRUNE   <- FALSE   # TRUE to prune highly correlated predictors
DEV_FAST_CV     <- FALSE   # TRUE for quick CV (v=5, repeats=1) during iteration

# ---- Predictor set ----
predictors_20 <- c(
  "ODA_RECEIVED_PER_CAPITA", 
  "GDP_GROWTH", 
  "GDP_PER_CAPITA_GROWTH", 
  "GDP_DEFLATOR",
  "POLITICAL_REGIME", 
  "ELECTORAL_DEMOCRACY_SCORE", 
  "LIBERAL_DEMOCRACY_SCORE", 
  "TERRITORIAL_FRAGMENTATION", 
  "INSTITUTIONAL_DEMOCRACY_SOCRE",  
  "INSTITUTIONAL_AUTOCRACY_SCORE",
  "COMBINED_POLITY_SCORE",
  "REGIME_DURABILITY_YEARS",
  "INSTITUTIONAL_EXECUTIVE_RECRUTIMENT",
  "POLITICAL_COMPETITION_SCORE",
  "PARTIAL_DEMOCRACY_WITH_FACTIONALISM",
  "CONFLICT_INTENSITY_YEAR",
  "CONFLICT_CUMULATIVE_INTENSITY_ACROSS_YEARS",
  "N_WAR_FRONTS",
  "MAX_CONFLICT_INTENSITY",
  "AVG_CONFLICT_INTENSITY",
  "N_TOTAL_TROOPS"
)

# -----------------------------
# 1) Build modelling dataframe
# -----------------------------
df <- standardized_data_final |>
  select(COUNTRY_NAME, YEAR, all_of(predictors_20), VDEM_STATUS_IDEAL) |>
  filter(!is.na(VDEM_STATUS_IDEAL)) |>
  tidyr::drop_na() |>
  mutate(VDEM_STATUS_IDEAL = forcats::as_factor(VDEM_STATUS_IDEAL)) |>
  # country_mode (majority label per country) ONLY for stratification
  group_by(COUNTRY_NAME) |>
  mutate(country_mode = names(which.max(table(VDEM_STATUS_IDEAL)))) |>
  ungroup()

# -----------------------------------------------
# 2) Country-blocked 80/20 split (stratify on mode)
# -----------------------------------------------
data_split <- group_initial_split(
  df,
  group  = COUNTRY_NAME,
  prop   = 0.80,
  strata = country_mode
)
train_data <- training(data_split)
test_data  <- testing(data_split)

# --------------------------------
# 3) Recipes (separate by model)
# --------------------------------
# Helper to optionally add imputation & correlation pruning
add_robust_steps <- function(rec_obj) {
  if (DO_IMPUTE) {
    rec_obj <- rec_obj |>
      step_impute_median(all_numeric_predictors())
  }
  if (DO_CORR_PRUNE) {
    rec_obj <- rec_obj |>
      step_corr(all_numeric_predictors(), threshold = 0.9) |>
      step_lincomb(all_numeric_predictors())
  }
  rec_obj
}

# Logit: upsample + normalize (no class weights)
rec_logit <- recipe(VDEM_STATUS_IDEAL ~ ., data = train_data) |>
  update_role(COUNTRY_NAME, YEAR, country_mode, new_role = "id") |>
  step_zv(all_predictors()) |>
  step_upsample(VDEM_STATUS_IDEAL, over_ratio = 1) |>
  step_normalize(all_numeric_predictors())
rec_logit <- add_robust_steps(rec_logit)

# RF: class weights only (no upsample; no normalization needed)
rec_rf <- recipe(VDEM_STATUS_IDEAL ~ ., data = train_data) |>
  update_role(COUNTRY_NAME, YEAR, country_mode, new_role = "id") |>
  step_zv(all_predictors())
rec_rf <- add_robust_steps(rec_rf)

# --------------------------------
# 4) Model specifications
# --------------------------------
# 4a) Multinomial logit
logit_spec <- multinom_reg(mode = "classification") |>
  set_engine("nnet")

# 4b) Random forest with class weights + FAST tuning
cls_freq    <- table(train_data$VDEM_STATUS_IDEAL)
rf_weights  <- as.numeric(max(cls_freq) / cls_freq)
names(rf_weights) <- names(cls_freq)

# Parallel over resamples (Windows-friendly)
n_cores <- 1
if (rlang::is_installed("doParallel")) {
  library(doParallel)
  n_cores <- max(1, parallel::detectCores() - 1)
  cl <- makeCluster(n_cores)
  registerDoParallel(cl)
  on.exit(try(stopCluster(cl), silent = TRUE), add = TRUE)
}

# Tuning spec: drop importance during tuning, limit ranger threading to 1 to avoid nested parallelism
rf_spec_tune <- rand_forest(
  mode  = "classification",
  trees = 1000,
  mtry  = tune(),
  min_n = tune()
) |>
  set_engine(
    "ranger",
    class.weights = rf_weights,
    num.threads   = 1
  )

rf_wf <- workflow() |>
  add_model(rf_spec_tune) |>
  add_recipe(rec_rf)

# Smaller, smarter grid via Latin hypercube
p_grid <- grid_latin_hypercube(
  finalize(mtry(), train_data |> dplyr::select(all_of(predictors_20))),
  min_n(),
  size = 16
)

# ----------------------------------------------
# 5) Grouped CV (countries intact), metric set
# ----------------------------------------------
v_cv <- if (DEV_FAST_CV) 5 else 10
r_cv <- if (DEV_FAST_CV) 1 else 3

folds <- group_vfold_cv(
  train_data,
  group   = COUNTRY_NAME,
  v       = v_cv,
  repeats = r_cv,
  strata  = country_mode
)

# Metrics: accuracy, log-loss, balanced accuracy, macro recall (na_rm avoids warnings)
recall_macro <- yardstick::metric_tweak(
  "recall_macro",
  yardstick::recall,
  estimator = "macro",
  na_rm     = TRUE
)

metrics_set <- yardstick::metric_set(
  yardstick::accuracy,
  yardstick::mn_log_loss,
  yardstick::bal_accuracy,
  recall_macro
)

# ----------------------------------------------
# 6) Fit resamples (Logit) & fast tune RF
# ----------------------------------------------
logit_res <- fit_resamples(
  workflow() |> add_model(logit_spec) |> add_recipe(rec_logit),
  resamples = folds,
  metrics   = metrics_set,
  control   = control_resamples(
    save_pred     = TRUE,
    parallel_over = "resamples"
  )
)

# Prefer racing if finetune available; else tune_grid with parallel_over
if (rlang::is_installed("finetune")) {
  library(finetune)
  rf_tuned <- tune_race_anova(
    rf_wf,
    resamples = folds,
    metrics   = metrics_set,
    grid      = p_grid,
    control   = control_race(
      parallel_over = "resamples",
      save_pred     = FALSE,   # faster / less memory
      verbose_elim  = TRUE
    )
  )
} else {
  rf_tuned <- tune_grid(
    rf_wf,
    resamples = folds,
    metrics   = metrics_set,
    grid      = p_grid,
    control   = control_grid(
      parallel_over = "resamples",
      save_pred     = FALSE
    )
  )
}

rf_best <- select_best(rf_tuned, metric = "mn_log_loss")

# Finalized spec: turn permutation importance back on, and use threads
rf_spec_final <- rand_forest(
  mode  = "classification",
  trees = 1000,
  mtry  = rf_best$mtry,
  min_n = rf_best$min_n
) |>
  set_engine(
    "ranger",
    class.weights = rf_weights,
    importance    = "permutation",
    num.threads   = n_cores
  )

rf_final <- workflow() |>
  add_model(rf_spec_final) |>
  add_recipe(rec_rf)

# Resample estimate for finalized RF (fast; no preds saved)
rf_res <- fit_resamples(
  rf_final,
  resamples = folds,
  metrics   = metrics_set,
  control   = control_resamples(
    save_pred     = FALSE,
    parallel_over = "resamples"
  )
)

# ----------------------------------------------
# 7) Final fits on training data
# ----------------------------------------------
logit_fit <- fit(workflow() |> add_model(logit_spec) |> add_recipe(rec_logit), data = train_data)
rf_fit    <- fit(rf_final, data = train_data)

# ----------------------------------------------
# 8) Predict on *held-out* test countries
# ----------------------------------------------
logit_preds <- predict(logit_fit, test_data, type = "class") |>
  bind_cols(truth = test_data$VDEM_STATUS_IDEAL)

rf_preds <- predict(rf_fit, test_data, type = "class") |>
  bind_cols(truth = test_data$VDEM_STATUS_IDEAL)

# ----------------------------------------------
# 9) Confusion matrices & accuracy
# ----------------------------------------------
conf_mat_logit <- conf_mat(logit_preds, truth = truth, estimate = .pred_class)
conf_mat_rf    <- conf_mat(rf_preds,    truth = truth, estimate = .pred_class)

accuracy_logit <- accuracy(logit_preds, truth = truth, estimate = .pred_class)$.estimate
accuracy_rf    <- accuracy(rf_preds,    truth = truth, estimate = .pred_class)$.estimate

# Visualize (no extra fill scale to avoid messages)
autoplot(conf_mat_logit, type = "heatmap") +
  labs(title = paste0("Multinomial Logit – Held-out Countries (Acc = ",
                      round(accuracy_logit * 100, 1), "%)"),
       fill  = "Count") +
  theme_minimal(base_size = 14)

autoplot(conf_mat_rf, type = "heatmap") +
  labs(title = paste0("Random Forest – Held-out Countries (Acc = ",
                      round(accuracy_rf * 100, 1), "%)"),
       fill  = "Count") +
  theme_minimal(base_size = 14)

# ----------------------------------------------
# 10) RF Variable importance
# ----------------------------------------------
vip(extract_fit_parsnip(rf_fit), num_features = 20, geom = "col") +
  labs(title = "Random Forest – Permutation Importance") +
  theme_classic(base_size = 14)

# ----------------------------------------------
# 11) Cross-validated summary metrics (pretty table with fallback)
# ----------------------------------------------
metrics_tbl <- bind_rows(
  collect_metrics(logit_res) |> mutate(Model = "Logit"),
  collect_metrics(rf_res)    |> mutate(Model = "Random Forest")
) |>
  mutate(.metric = as.character(.metric)) |>
  mutate(.metric = dplyr::recode(
    .metric,
    "accuracy"     = "Accuracy",
    "mn_log_loss"  = "Multinomial Log Loss",
    "bal_accuracy" = "Balanced Accuracy",
    "recall"       = "Macro Recall",
    .default       = .metric
  )) |>
  select(.metric, Model, mean, std_err) |>
  pivot_wider(
    names_from  = Model,
    values_from = c(mean, std_err),
    names_glue  = "{Model} {.value}"
  )

if (use_gt) {
  gt::gt(metrics_tbl) |>
    gt::cols_label(
      .metric                 = "Metric",
      `Logit mean`            = "Mean",
      `Logit std_err`         = "SE",
      `Random Forest mean`    = "Mean",
      `Random Forest std_err` = "SE"
    ) |>
    gt::tab_spanner(label = "Logit",         columns = c(`Logit mean`, `Logit std_err`)) |>
    gt::tab_spanner(label = "Random Forest", columns = c(`Random Forest mean`, `Random Forest std_err`)) |>
    gt::fmt_percent(columns = c(`Logit mean`, `Random Forest mean`),
                    rows = .metric %in% c("Accuracy","Balanced Accuracy"), decimals = 1) |>
    gt::fmt_number(columns = c(`Logit mean`, `Random Forest mean`),
                   rows = .metric %in% c("Multinomial Log Loss","Macro Recall"),
                   decimals = 3) |>
    gt::fmt_number(columns = c(`Logit std_err`, `Random Forest std_err`), decimals = 3) |>
    gt::tab_header(
      title = gt::md("**Cross-Validated Performance**"),
      subtitle = paste0(v_cv, "-fold CV (", r_cv, " repeats), grouped by country")
    )
} else {
  # Text/knit-safe fallback
  fmt <- function(x, pct) if (pct) scales::percent(x, accuracy = 0.1) else sprintf("%.3f", x)
  metrics_tbl_print <- metrics_tbl |>
    mutate(
      `Logit mean`            = ifelse(.metric %in% c("Accuracy","Balanced Accuracy"),
                                       fmt(`Logit mean`, TRUE),  fmt(`Logit mean`, FALSE)),
      `Random Forest mean`    = ifelse(.metric %in% c("Accuracy","Balanced Accuracy"),
                                       fmt(`Random Forest mean`, TRUE), fmt(`Random Forest mean`, FALSE)),
      `Logit std_err`         = sprintf("%.3f", `Logit std_err`),
      `Random Forest std_err` = sprintf("%.3f", `Random Forest std_err`)
    )
  knitr::kable(metrics_tbl_print,
               caption = paste0("Cross-Validated Performance (", v_cv, "×", r_cv, " grouped CV)"))
}

# ==========================================================
# OPTIONAL: Run the "Imputation & Corr-Prune" variant
# ==========================================================
if (DO_IMPUTE || DO_CORR_PRUNE) {
  message("Running robustness variant with imputation/correlation pruning...")

  rec_logit2 <- recipe(VDEM_STATUS_IDEAL ~ ., data = train_data) |>
    update_role(COUNTRY_NAME, YEAR, country_mode, new_role = "id") |>
    step_zv(all_predictors()) |>
    step_upsample(VDEM_STATUS_IDEAL, over_ratio = 1) |>
    step_normalize(all_numeric_predictors()) |>
    add_robust_steps()

  rec_rf2 <- recipe(VDEM_STATUS_IDEAL ~ ., data = train_data) |>
    update_role(COUNTRY_NAME, YEAR, country_mode, new_role = "id") |>
    step_zv(all_predictors()) |>
    add_robust_steps()

  logit_wf2 <- workflow() |> add_model(logit_spec)   |> add_recipe(rec_logit2)
  rf_wf2    <- workflow() |> add_model(rf_spec_tune) |> add_recipe(rec_rf2)

  logit_res2 <- fit_resamples(
    logit_wf2, resamples = folds, metrics = metrics_set,
    control   = control_resamples(save_pred = TRUE, parallel_over = "resamples")
  )

  if (rlang::is_installed("finetune")) {
    rf_tuned2 <- finetune::tune_race_anova(
      rf_wf2, resamples = folds, metrics = metrics_set, grid = p_grid,
      control = finetune::control_race(parallel_over = "resamples", save_pred = FALSE)
    )
  } else {
    rf_tuned2 <- tune_grid(
      rf_wf2, resamples = folds, metrics = metrics_set, grid = p_grid,
      control = control_grid(parallel_over = "resamples", save_pred = FALSE)
    )
  }
  rf_best2  <- select_best(rf_tuned2, metric = "mn_log_loss")
  rf_spec_final2 <- rand_forest(mode="classification", trees=1000,
                                mtry=rf_best2$mtry, min_n=rf_best2$min_n) |>
    set_engine("ranger", class.weights = rf_weights, importance="permutation", num.threads=n_cores)
  rf_final2 <- workflow() |> add_model(rf_spec_final2) |> add_recipe(rec_rf2)

  rf_res2 <- fit_resamples(
    rf_final2, resamples = folds, metrics = metrics_set,
    control = control_resamples(save_pred = FALSE, parallel_over = "resamples")
  )

  rob_tbl <- bind_rows(
    collect_metrics(logit_res2) |> mutate(Model = "Logit (imp/corr)"),
    collect_metrics(rf_res2)    |> mutate(Model = "RF (imp/corr)")
  ) |>
    select(.metric, Model, mean, std_err)

  print(rob_tbl)
}
```

With the country-blocked holdout, the multinomial logit reaches 65.8% accuracy, while the random forest jumps to around 86%. That headline gap tracks what we see in the grouped CV as well — RF is stronger overall — with mean CV accuracy 75.1% for RF versus 65.5% for logit, and lower multiclass log-loss for RF (0.753 vs 1.063), which indicates better-calibrated probabilities. The CV balanced accuracy is essentially tied at 70% for both, suggesting that once we equalize class prevalence, their average class-wise recall is similar.

The confusion matrices explain the pattern. RF is excellent on the extremes: it recovers Fragile (recall = 0.96) and Non-Fragile (= 0.87) on the held-out countries, but it collapses the middle class, predicting virtually no `Transitioning` cases. That choice yields a high overall accuracy because `Transitioning` is a small share of the test set; the cost is class coverage. By contrast, the logit does put mass on Transitioning and spreads predictions across all three tiers, so its macro recall is competitive in CV (= 0.575 vs 0.555 for RF), but it pays an accuracy and log-loss penalty because many of those mid-tier calls are wrong.

The RF permutation importance is substantively coherent: democratic-institutional measures (*Liberal* and *Electoral Democracy*, *Political Regime*, *Institutional Executive Recruitment*, *Polity*) dominate the ranking, with conflict and growth variables trailing — exactly the hierarchy you would expect if regime features anchor the structural notion of fragility. That alignment strengthens the case that the labels are recoverable from exogenous structure rather than artifacts of the construction.

Bottom line: the models — especially the RF — recover the extremes of the fragility spectrum with high fidelity and calibrated probabilities. The remaining weakness is the inherently fuzzy Transitioning tier.

\newpage

# 5. Linear and Logit Binomial Models 

## 5.1. Linear and Logit Binomial Coefficients

* **Ordinary Least Square Coefficients (OLS):** The model fits the data into a regression line and estimates the change in the outcome variable due to a one unit increase in each predictor. For dichotomous outcomes (`1 = Fragile` or `0 = Non Fragile`), the coefficient estimates the average difference between the two levels of the dependent variable. The OLS model is - however - probably ill suited in this case as it assumes a continuous distribution of the dependent variable and may end up producing predicted values outside the [0, 1].  

   + The *standard errors* (SEs), reported below each coefficient, capture the variability of the same coefficient (i.e., on average, how far from the regression line observed values fall). Naturally, the smaller the SEs, the better. 
   
   + SEs are "clustered" at the country level, as it is reasonably to assume that observations for the same country are likely to be correlated over time (autocorrelation). The robust standard errors reported also account for heteroskedasticity (i.e., the non-constant variance of errors).

* **Logistic Regression (Logit) Coefficients:** The coefficients suggest how many standard deviations the target variable (i.e., being `Fragile` or `Non-Fragile` in a given year; the log-odds of the outcome in logistic regression) changes per standard deviation change in the predictor variable. 

* **Performance:** The table returns a couple of metrics of performance ($R^2$ versus $PseudoR^2$, $AIC$ and $BIC$) that allows for direct comparisons of models' performances. The plots estimate the ROC (Receiver Operating Characteristic) curve, which identifies how well the model can distinguish between the two classes (e.g., `Fragile` and `Non-Fragile` outcomes) across various thresholds. The Area Under the Curve (AUC) the quality of this binary classification model: the higher the AUC, the better!

```{r, echo = FALSE, message = FALSE, warning = FALSE}

# ------------------------------------------------------------------------------
# Setting the empirical model --------------------------------------------------
# ------------------------------------------------------------------------------

# Baseline predictors 
baseline_vars <- c(
  "COMBINED_POLITY_SCORE",
  "INSTITUTIONAL_AUTOCRACY_SCORE",
  "GDP_GROWTH",
  "MAX_CONFLICT_INTENSITY",
  "N_TOTAL_TROOPS",
  "TERRITORIAL_FRAGMENTATION",
  "INSTITUTIONAL_EXECUTIVE_RECRUTIMENT",  
  "INSTITUTIONAL_DEMOCRACY_SOCRE",         
  "LIBERAL_DEMOCRACY_SCORE",
  "REGIME_DURABILITY_YEARS",
  "GDP_PER_CAPITA",
  "POLITICAL_REGIME",
  "N_WAR_FRONTS",
  "ODA_RECEIVED_PER_CAPITA",
  "CONFLICT_CUMULATIVE_INTENSITY_ACROSS_YEARS",
  "ELECTORAL_DEMOCRACY_SCORE",
  "POLITICAL_COMPETITION_SCORE",
  "CONFLICT_INTENSITY_YEAR",
  "AVG_CONFLICT_INTENSITY",
  "GDP_DEFLATOR",
  "PARTIAL_DEMOCRACY_WITH_FACTIONALISM"
)

# Baseline empirical specification
form_baseline <- as.formula(
  paste("VDEM_FRAGILE_IDEAL ~", paste(baseline_vars, 
                                      collapse = " + "))
)

# ------------------------------------------------------------------------------
# OLS (cluster-robust at COUNTRY_NAME) -----------------------------------------
# ------------------------------------------------------------------------------

linear_model <- lm(form_baseline, 
                   data = standardized_data_final)

clustered_se_linear <- sqrt(diag(vcovCL(linear_model, 
                                        cluster = ~ COUNTRY_NAME)))

# ------------------------------------------------------------------------------
# Logit (cluster-robust at COUNTRY_NAME) ---------------------------------------
# ------------------------------------------------------------------------------
logit_model  <- glm(form_baseline,
                    family = binomial(link = "logit"),
                    data   = standardized_data_final)

clustered_se_logit <- sqrt(diag(vcovCL(logit_model, 
                                       cluster = ~ COUNTRY_NAME)))

# ------------------------------------------------------------------------------
# Fixed-effects (within) + Driscoll–Kraay SEs ----------------------------------
# ------------------------------------------------------------------------------

fixed_effects_model <- plm(form_baseline,
                           data  = standardized_data_final,
                           model = "within",
                           index = c("ISO_CODE_3", 
                                     "YEAR"))

fe_dk_se <- sqrt(diag(vcovSCC(fixed_effects_model,
                              maxlag = 3, type = "HC1")))

# ------------------------------------------------------------------------------
# Fit statistics ---------------------------------------------------------------
# ------------------------------------------------------------------------------

R2_linear <- summary(linear_model)$r.squared

logit_null <- glm(VDEM_FRAGILE_BASELINE ~ 1,
                  family = binomial,
                  data   = standardized_data_final)
McFadden_R2 <- 1 - (as.numeric(logLik(logit_model)) /
                    as.numeric(logLik(logit_null)))

add_stats <- list(
  c("R-squared (OLS)",          sprintf("%.3f", R2_linear),      ""),
  c("Pseudo R-squared (Logit)", "",                              sprintf("%.3f", McFadden_R2)),
  c("AIC",                      sprintf("%.2f", AIC(linear_model)), sprintf("%.2f", AIC(logit_model))),
  c("BIC",                      sprintf("%.2f", BIC(linear_model)), sprintf("%.2f", BIC(logit_model)))
)
```

```{r, linear-binomial, echo = FALSE, message = FALSE, warning = FALSE, results = 'asis'}

# ------------------------------------------------------------------------------
# LaTeX baseline table ---------------------------------------------------------
# ------------------------------------------------------------------------------

stargazer(linear_model,
          logit_model,
          fixed_effects_model,
          se = list(clustered_se_linear,
                    clustered_se_logit,
                    fe_dk_se),
          covariate.labels = c("Combined polity score",
                               "Institutional autocracy score",
                               "GDP growth",
                               "MAX conflict intensity",
                               "N total troops involved",
                               "Territorial fragmentation",
                               "Executive recruitment",
                               "Institutional democracy score",
                               "Liberal democracy score",
                               "Regime durability in years",
                               "GDP per capita",
                               "Political regime",
                               "N war of fronts",
                               "ODA received per capita",
                               "Conflitc cumul. intensity across years",
                               "Electoral democracy score",
                               "Political competition score",
                               "Conflict intensity year",
                               "AVG conflict intensity",
                               "GDP deflator",
                               "Partial democracy with factionalism",
                               "Constant"),
          # Dropping year dummies
          omit = "factor\\(YEAR\\)",     
          header = FALSE,
          no.space = TRUE,
          font.size = "small",
          add.lines = add_stats,
          title = "Linear, Logistic and FE Estimates of Fragility Status (1970-2022)",
          dep.var.caption = "Fragility Status",
          type = "latex",
          dep.var.labels = "")
```

\newpage

## 5.2 Multicolinearity Heatmap

```{r, echo = FALSE, message = FALSE, warning=FALSE}

# Predictors 
predictor_vars <- c(
  "COMBINED_POLITY_SCORE",
  "INSTITUTIONAL_AUTOCRACY_SCORE",
  "GDP_GROWTH",
  "MAX_CONFLICT_INTENSITY",
  "N_TOTAL_TROOPS",
  "TERRITORIAL_FRAGMENTATION",
  "INSTITUTIONAL_EXECUTIVE_RECRUTIMENT",
  "INSTITUTIONAL_DEMOCRACY_SOCRE",
  "LIBERAL_DEMOCRACY_SCORE",
  "REGIME_DURABILITY_YEARS",
  "GDP_PER_CAPITA",
  "POLITICAL_REGIME",
  "N_WAR_FRONTS",
  "ODA_RECEIVED_PER_CAPITA",
  "CONFLICT_CUMULATIVE_INTENSITY_ACROSS_YEARS",
  "ELECTORAL_DEMOCRACY_SCORE",
  "POLITICAL_COMPETITION_SCORE",
  "CONFLICT_INTENSITY_YEAR",
  "AVG_CONFLICT_INTENSITY",
  "GDP_DEFLATOR",
  "PARTIAL_DEMOCRACY_WITH_FACTIONALISM"
)

# Identifying numeric predictors -----------------------------------------------
numeric_vars <- predictor_vars[
  sapply(standardized_data_final[predictor_vars], is.numeric)
]

non_numeric <- setdiff(predictor_vars, numeric_vars)
if (length(non_numeric) > 0) {
  message("Dropped non-numeric variables from heat-map: ",
          paste(non_numeric, collapse = ", "))
}


# ------------------------------------------------------------------------------
# Computing correlation matrix -------------------------------------------------
# ------------------------------------------------------------------------------

corr_mat <- standardized_data_final |>
  dplyr::ungroup() |>
  dplyr::select(dplyr::all_of(numeric_vars)) |>
  (\(x) stats::cor(x, use = "pairwise.complete.obs"))()

# Re-ordering with hierarchical clustering for clearer blocks ------------------
hc <- hclust(as.dist(1 - abs(corr_mat)))
corr_mat <- corr_mat[hc$order, hc$order]

# Long format ggplot -----------------------------------------------------------
corr_long <- as.data.frame(as.table(corr_mat)) |>
  rename(Var1 = Var1, Var2 = Var2, r = Freq)

# Plotting the heat-map --------------------------------------------------------
ggplot(corr_long, aes(Var1, Var2, fill = r)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "#f26968", mid = "white", high = "#00468B",
                       midpoint = 0, limits = c(-1, 1),
                       name = "Correlation") +
  coord_fixed() +
  theme_classic(base_size = 9) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        panel.grid   = element_blank()) +
  labs(title = "Pairwise Correlations among Baseline Numeric Predictors (1970-2022)",
       x = "", y = "")
```

# 6. Elastic Net  

```{r, echo = FALSE, message = FALSE, warning = FALSE}

# ------------------------------------------------------------------------------
# Core fitter (with α-tuning) --------------------------------------------------
# ------------------------------------------------------------------------------

# Setting glmnet panel 
fit_glmnet_panel <- function(
  data,
  predictors,
  outcome               = "VDEM_STATUS_IDEAL",
  country_col           = "country",
  year_col              = "year",
  # Sticking to a 80/20 time split
  holdout_year          = NULL,
  # Character vector of factor predictors 
  categorical_predictors = NULL,
  # Country and year FEs
  include_fixed_effects  = FALSE,       
  kfold_countries        = 5,
  # For LASSO and Ridge 
  alphas                 = c(1, .75, .5, .25, 0),
  # Re-scaling
  scale_with_train       = TRUE          
) {

  # Data preparation
  df <- data |>
    select(all_of(c(country_col, 
                    year_col, 
                    predictors, 
                    outcome))) |>
    drop_na()

  df[[outcome]] <- factor(df[[outcome]])
  if (nlevels(df[[outcome]]) != 2L) stop("Outcome must be binary (2 levels).")

  if (!is.null(categorical_predictors)) {
    for (v in intersect(categorical_predictors, names(df))) {
      df[[v]] <- factor(df[[v]])
    }
  }

  # -- time split (forward)
  if (is.null(holdout_year)) {
    yrs <- sort(unique(df[[year_col]]))
    holdout_year <- yrs[ceiling(0.8 * length(yrs))]  # last 20% of years as test
  }
  train <- dplyr::filter(df, .data[[year_col]] <= holdout_year)
  test  <- dplyr::filter(df, .data[[year_col]] >  holdout_year)
  stopifnot(nrow(train) > 0L, nrow(test) > 0L)

  # -- build formula (optionally with FE)
  rhs_vars <- setdiff(names(df), c(outcome, country_col, year_col))
  fe_term <- if (include_fixed_effects) {
    paste0(" + factor(", country_col, ") + factor(", year_col, ")")
  } else ""
  form <- as.formula(paste0(outcome, " ~ ", paste(rhs_vars, collapse = " + "), fe_term, " - 1"))

  # -- optional train-only scaling (prevents leakage even if upstream z-scoring was global)
  num_vars <- rhs_vars[sapply(train[, rhs_vars, drop = FALSE], is.numeric)]
  scaling <- list(enabled = scale_with_train, mu = NULL, sd = NULL, num_vars = num_vars)

  if (scale_with_train && length(num_vars) > 0) {
    mu <- sapply(train[, num_vars, drop = FALSE], mean)
    sd <- sapply(train[, num_vars, drop = FALSE], sd); sd[sd == 0] <- 1
    scale_with <- function(d) {
      d2 <- d
      d2[, num_vars] <- sweep(sweep(d2[, num_vars, drop = FALSE], 2, mu, "-"), 2, sd, "/")
      d2
    }
    train <- scale_with(train)
    test  <- scale_with(test)
    scaling$mu <- mu; scaling$sd <- sd
  }

  # -- design matrices (stacked to guarantee identical cols for this split)
  stacked <- rbind(train, test)
  X_full  <- model.matrix(form, data = stacked)
  X_train <- X_full[seq_len(nrow(train)), , drop = FALSE]
  X_test  <- X_full[seq_len(nrow(test)) + nrow(train), , drop = FALSE]
  X_cols  <- colnames(X_train)

  y_train <- droplevels(train[[outcome]])
  y_test  <- droplevels(test[[outcome]])
  lv <- levels(y_train); pos <- lv[2]

  # -- country-wise folds
  set.seed(42)
  K <- kfold_countries
  countries_tr <- unique(train[[country_col]])
  foldmap <- setNames(sample(rep(1:K, length.out = length(countries_tr))), countries_tr)
  foldid  <- as.integer(foldmap[train[[country_col]]])

  # -- class weights (minority upweighted)
  n_pos <- sum(y_train == pos); n_neg <- sum(y_train != pos)
  w <- ifelse(y_train == pos, n_neg / max(1, n_pos), 1)

  # -- α tuning (AUC CV)
  fits <- lapply(alphas, function(a)
    cv.glmnet(
      x = X_train, y = y_train,
      family = "binomial",
      alpha = a,
      type.measure = "auc",
      standardize = FALSE,               # already scaled or left as-is
      foldid = foldid,
      nlambda = 200,
      lambda.min.ratio = 1e-5,
      weights = w
    )
  )
  cv_auc <- sapply(fits, function(f) max(f$cvm))
  best_i <- which.max(cv_auc)
  best_alpha <- alphas[best_i]
  best_fit <- fits[[best_i]]
  s_use <- best_fit$lambda.1se

  # -- TRAIN thresholds (choose here; apply to TEST)
  p_train <- as.numeric(predict(best_fit, s = s_use, newx = X_train, type = "response"))
  roc_tr  <- pROC::roc(response = y_train, predictor = p_train, levels = lv, quiet = TRUE)
  thr_youden_tr <- as.numeric(pROC::coords(roc_tr, "best", best.method = "youden", transpose = TRUE)["threshold"])

  grid <- seq(0.05, 0.95, by = 0.01)
  f1_train <- function(th){
    pr <- factor(ifelse(p_train >= th, pos, lv[1]), levels = lv)
    tp <- sum(pr==pos & y_train==pos); fp <- sum(pr==pos & y_train!=pos); fn <- sum(pr!=pos & y_train==pos)
    prec <- ifelse(tp+fp==0, 0, tp/(tp+fp)); rec <- ifelse(tp+fn==0, 0, tp/(tp+fn))
    ifelse(prec+rec==0, 0, 2*prec*rec/(prec+rec))
  }
  thr_f1_tr <- grid[which.max(vapply(grid, f1_train, numeric(1)))]

  # -- TEST metrics
  p_test <- as.numeric(predict(best_fit, s = s_use, newx = X_test, type = "response"))
  roc_te <- pROC::roc(response = y_test, predictor = p_test, levels = lv, quiet = TRUE)
  auc_te <- as.numeric(pROC::auc(roc_te))
  ci_te  <- as.numeric(pROC::ci.auc(roc_te))  # 95% CI (DeLong)

  # thresholds learned on TRAIN, applied to TEST
  pred_y <- factor(ifelse(p_test >= thr_youden_tr, pos, lv[1]), levels = lv)
  pred_f <- factor(ifelse(p_test >= thr_f1_tr,     pos, lv[1]), levels = lv)
  cm_y   <- caret::confusionMatrix(pred_y, y_test, positive = pos)
  cm_f   <- caret::confusionMatrix(pred_f, y_test, positive = pos)

  # PR-AUC + Brier
  y_bin <- as.integer(y_test == pos)
  pr    <- PRROC::pr.curve(scores.class0 = p_test[y_bin==1],
                           scores.class1 = p_test[y_bin==0], curve = FALSE)
  pr_auc <- pr$auc.integral
  brier  <- mean((y_bin - p_test)^2)

  # simple calibration (test)
  logit <- function(p) log(p/(1-p))
  cal_fit <- glm((y_test==pos) ~ logit(p_test), family = binomial())
  cal_intercept <- unname(coef(cal_fit)[1])
  cal_slope     <- unname(coef(cal_fit)[2])

  # coefficients at s_use
  nz <- coef(best_fit, s = s_use)
  nz_df <- data.frame(feature = rownames(nz), beta = as.numeric(nz), row.names = NULL) |>
    dplyr::filter(beta != 0)

  list(
    # model + design metadata
    model = best_fit,
    best_alpha = best_alpha,
    lambda = s_use,
    model_formula = form,
    X_cols = X_cols,
    scaling = scaling,
    levels = lv, pos = pos,
    include_fixed_effects = include_fixed_effects,

    # CV + Test metrics
    cv_auc_by_alpha = data.frame(alpha = alphas, cv_auc = cv_auc),
    auc_test = auc_te,
    auc_test_ci = ci_te,
    pr_auc_test = pr_auc,
    brier_test  = brier,
    calib_intercept = cal_intercept,
    calib_slope     = cal_slope,

    # thresholds (chosen on TRAIN)
    threshold_youden_train = thr_youden_tr,
    threshold_f1_train     = thr_f1_tr,

    # confusion matrices (TEST)
    confusion_youden = cm_y,
    confusion_f1     = cm_f,

    # extras
    nonzero_coefs = nz_df,
    X_train_dim = dim(X_train),
    X_test_dim  = dim(X_test)
  )
}

# ------------------------------------------
# Helper: predict/evaluate on NEW data block
# (e.g., held-out region for LORO)
# ------------------------------------------
eval_on_new <- function(fit, newdata, outcome, country_col, year_col) {
  df <- newdata |>
    dplyr::select(all_of(c(country_col, year_col, outcome, all.vars(update(fit$model_formula, . ~ .)))))
  df <- tidyr::drop_na(df)

  # apply train scaling if used
  if (isTRUE(fit$scaling$enabled) && length(fit$scaling$num_vars) > 0) {
    nv <- intersect(fit$scaling$num_vars, names(df))
    if (length(nv) > 0) {
      df[, nv] <- sweep(sweep(df[, nv, drop = FALSE], 2, fit$scaling$mu[nv], "-"),
                        2, fit$scaling$sd[nv], "/")
    }
  }

  # build design, then align cols to training X
  X_new <- model.matrix(fit$model_formula, data = df)
  miss  <- setdiff(fit$X_cols, colnames(X_new))
  if (length(miss)) X_new <- cbind(X_new, matrix(0, nrow(X_new), length(miss), dimnames = list(NULL, miss)))
  X_new <- X_new[, fit$X_cols, drop = FALSE]

  y_new <- factor(df[[outcome]], levels = fit$levels)
  p_new <- as.numeric(predict(fit$model, s = fit$lambda, newx = X_new, type = "response"))

  roc_new <- pROC::roc(response = y_new, predictor = p_new, levels = fit$levels, quiet = TRUE)
  auc_new <- as.numeric(pROC::auc(roc_new))

  list(n = length(p_new), auc = auc_new, probs = p_new, y = y_new)
}

# -----------------------------
# Plot helpers
# -----------------------------
plot_cv_and_paths <- function(fit, main_prefix = "Best model") {
  op <- par(mfrow = c(1,2), mar = c(5,4,3,1)); on.exit(par(op), add = TRUE)
  plot(fit$model, main = paste0(main_prefix, " — CV (AUC), alpha=", fit$best_alpha))
  abline(v = log(fit$lambda), lty = 2)
  plot(fit$model$glmnet.fit, xvar = "lambda", main = "Coefficient paths")
  abline(v = log(fit$lambda), lty = 2)
}

# ---------------------------------
# Time-cut sensitivity (forward)
# ---------------------------------
time_cut_sensitivity <- function(data, predictors, outcome,
                                 country_col, year_col,
                                 include_fixed_effects = FALSE,
                                 scale_with_train = TRUE,
                                 cuts = NULL, min_test_n = 100) {
  df_clean <- data |>
    dplyr::select(all_of(c(country_col, year_col, predictors, outcome))) |>
    tidyr::drop_na()
  yrs <- sort(unique(df_clean[[year_col]]))
  if (is.null(cuts)) cuts <- tail(yrs[yrs < max(yrs)], 6)

  out <- lapply(cuts, function(cut) {
    fit <- try(fit_glmnet_panel(
      data, predictors, outcome, country_col, year_col,
      holdout_year = cut,
      include_fixed_effects = include_fixed_effects,
      scale_with_train = scale_with_train
    ), silent = TRUE)
    if (inherits(fit, "try-error")) return(NULL)
    if (fit$X_test_dim[1] < min_test_n) return(NULL)
    data.frame(cut = cut,
               n_train = fit$X_train_dim[1],
               n_test  = fit$X_test_dim[1],
               auc     = fit$auc_test,
               pr_auc  = fit$pr_auc_test,
               brier   = fit$brier_test)
  })
  do.call(rbind, Filter(Negate(is.null), out))
}

# ---------------------------------
# Region-out (LORO) evaluation
# ---------------------------------
region_out <- function(data, predictors, outcome,
                       country_col, year_col, region_col,
                       holdout_year, include_fixed_effects = FALSE,
                       scale_with_train = TRUE, min_test_n = 150) {
  regs <- sort(unique(data[[region_col]]))
  res <- lapply(regs, function(R) {
    tr <- subset(data, .data[[region_col]] != R)
    te <- subset(data, .data[[region_col]] == R & .data[[year_col]] > holdout_year)
    if (nrow(te) < min_test_n) return(NULL)

    fit <- fit_glmnet_panel(
      tr, predictors, outcome, country_col, year_col,
      holdout_year = holdout_year,
      include_fixed_effects = include_fixed_effects,
      scale_with_train = scale_with_train
    )
    ev <- eval_on_new(fit, te, outcome, country_col, year_col)
    data.frame(region = R, n_test = ev$n, auc = ev$auc)
  })
  do.call(rbind, Filter(Negate(is.null), res))
}

# =========================
# Example usage (baseline)
# =========================
predictors_baseline <- c(
  "ODA_RECEIVED_PER_CAPITA","GDP_GROWTH","GDP_PER_CAPITA","GDP_DEFLATOR",
  "POLITICAL_REGIME","ELECTORAL_DEMOCRACY_SCORE","LIBERAL_DEMOCRACY_SCORE",
  "TERRITORIAL_FRAGMENTATION","INSTITUTIONAL_DEMOCRACY_SOCRE","INSTITUTIONAL_AUTOCRACY_SCORE",
  "COMBINED_POLITY_SCORE","REGIME_DURABILITY_YEARS","INSTITUTIONAL_EXECUTIVE_RECRUTIMENT",
  "POLITICAL_COMPETITION_SCORE","PARTIAL_DEMOCRACY_WITH_FACTIONALISM",
  "CONFLICT_INTENSITY_YEAR","CONFLICT_CUMULATIVE_INTENSITY_ACROSS_YEARS",
  "N_WAR_FRONTS","MAX_CONFLICT_INTENSITY","AVG_CONFLICT_INTENSITY","N_TOTAL_TROOPS"
)

categoricals_baseline <- c(
  # add true categorical predictors here if any
  # "POLITICAL_REGIME","PARTIAL_DEMOCRACY_WITH_FACTIONALISM"
)

set.seed(42)
baseline_fit <- fit_glmnet_panel(
  data = final_clean_percentiles_data_normalized_1,
  predictors = predictors_baseline,
  outcome = "VDEM_STATUS_IDEAL",
  country_col = "COUNTRY_NAME",
  year_col = "YEAR",
  holdout_year = 2016,          # or NULL to auto 80/20
  categorical_predictors = categoricals_baseline,
  include_fixed_effects = FALSE,
  kfold_countries = 5,
  scale_with_train = TRUE       # prevents any scaling leakage
)

# Inspect & plot
baseline_fit$cv_auc_by_alpha
baseline_fit$best_alpha
baseline_fit$auc_test; baseline_fit$auc_test_ci
baseline_fit$pr_auc_test; baseline_fit$brier_test
baseline_fit$calib_intercept; baseline_fit$calib_slope
baseline_fit$confusion_youden
baseline_fit$confusion_f1
head(baseline_fit$nonzero_coefs)
plot_cv_and_paths(baseline_fit, "Baseline")

# Time-cut stability
time_cut_summary <- time_cut_sensitivity(
  data = final_clean_percentiles_data_normalized_1,
  predictors = predictors_baseline,
  outcome = "VDEM_STATUS_IDEAL",
  country_col = "COUNTRY_NAME",
  year_col = "YEAR",
  include_fixed_effects = FALSE,
  scale_with_train = TRUE
)
time_cut_summary


# Get all coefficients (including zero) at chosen lambda
all_coefs <- as.matrix(coef(baseline_fit$model, s = baseline_fit$lambda))

coef_table <- data.frame(
  feature = rownames(all_coefs),
  beta    = as.numeric(all_coefs)
) %>%
  dplyr::filter(feature != "(Intercept)") %>%  # drop intercept if not needed
  dplyr::mutate(shrunk_to_zero = beta == 0)

# Inspect
print(head(coef_table, 20))  # first 20 rows
table(coef_table$shrunk_to_zero)
```
```{r, echo = FALSE, message = FALSE, warning = FALSE}

# -------------------------
# Utilities
# -------------------------

# 0/1 numeric (2nd level -> 1) for any binary/factor or character DV
y_gauss <- function(y) {
  if (is.numeric(y)) return(y)
  if (is.factor(y))  return(as.numeric(y == levels(y)[2]))
  if (is.character(y)) { f <- factor(y); return(as.numeric(f == levels(f)[2])) }
  return(as.numeric(y))
}

# Ensure DV is numeric for FE-OLS refits
ensure_numeric_outcome <- function(d, outcome) {
  d[[outcome]] <- y_gauss(d[[outcome]])
  d
}

# Optionally cast declared categoricals to factor
cast_categoricals <- function(df, cat_vars) {
  if (is.null(cat_vars) || !length(cat_vars)) return(df)
  for (v in intersect(cat_vars, names(df))) df[[v]] <- factor(df[[v]])
  df
}

# Build model matrix with FE dummies (unpenalized later)
make_mm_with_fe <- function(df, y, x_vars, country_col, year_col) {
  form <- as.formula(
    paste0(y, " ~ ", paste(x_vars, collapse = " + "),
           " + factor(", country_col, ") + factor(", year_col, ") - 1")
  )
  mm <- model.matrix(form, data = df)
  terms_obj   <- terms(form)
  assign_vec  <- attr(mm, "assign")
  term_labels <- attr(terms_obj, "term.labels")
  fe_terms <- c(paste0("factor(", country_col, ")"),
                paste0("factor(", year_col, ")"))
  fe_idx <- match(fe_terms, term_labels)
  list(X = mm, y = df[[y]], assign = assign_vec,
       term_labels = term_labels, fe_term_idx = fe_idx)
}

# Map selected matrix columns back to base variable names
selected_base_vars <- function(coef_obj, mm_info, predictors, categorical_predictors) {
  cm <- as.matrix(coef_obj)
  nz <- rownames(cm)[as.numeric(cm) != 0]
  if (length(nz) == 0) return(character(0))
  sel <- character(0)
  for (v in predictors) {
    if (!is.null(categorical_predictors) && v %in% categorical_predictors) {
      if (any(startsWith(nz, v))) sel <- c(sel, v)
    } else {
      if (v %in% nz) sel <- c(sel, v)
    }
  }
  unique(sel)
}

# Train-only scaling for numeric predictors
train_scale_numeric <- function(df_train, df_test, vars) {
  num_vars <- vars[sapply(df_train[, vars, drop = FALSE], is.numeric)]
  if (!length(num_vars))
    return(list(train = df_train, test = df_test, mu = NULL, sd = NULL, num_vars = character(0)))
  mu <- sapply(df_train[, num_vars, drop = FALSE], mean)
  sd <- sapply(df_train[, num_vars, drop = FALSE], sd); sd[sd == 0] <- 1
  scale_one <- function(d) {
    d2 <- d
    d2[, num_vars] <- sweep(sweep(d2[, num_vars, drop = FALSE], 2, mu, "-"), 2, sd, "/")
    d2
  }
  list(train = scale_one(df_train), test = scale_one(df_test), mu = mu, sd = sd, num_vars = num_vars)
}

# Rubin-style pooling for cross-fit estimates
combine_splits <- function(est_list) {
  all_terms <- unique(unlist(lapply(est_list, \(e) e$term)))
  out <- lapply(all_terms, function(tt) {
    ests <- lapply(est_list, \(e) if (tt %in% e$term) e[e$term == tt, c("estimate","se")] else NULL)
    ests <- Filter(Negate(is.null), ests); M <- length(ests)
    if (M == 0) return(NULL)
    beta_hat <- mean(sapply(ests, \(z) z$estimate))
    W <- mean(sapply(ests, \(z) z$se^2))
    B <- var(sapply(ests, \(z) z$estimate)); if (is.na(B)) B <- 0
    Tvar <- W + (1 + 1/M) * B
    se_tot <- sqrt(Tvar)
    z <- beta_hat / se_tot
    p <- 2 * pnorm(-abs(z))
    data.frame(term = tt, estimate = beta_hat, se = se_tot, z = z, p = p)
  })
  do.call(rbind, out)
}

# Country-wise split where both halves contain both DV classes
balanced_country_split <- function(df, outcome, country_col, max_tries = 500, seed = 42) {
  set.seed(seed)
  ctry <- sort(unique(df[[country_col]])); nA <- length(ctry) %/% 2
  for (i in seq_len(max_tries)) {
    A_countries <- sample(ctry, nA)
    df_A <- df[df[[country_col]] %in% A_countries, , drop = FALSE]
    df_B <- df[!df[[country_col]] %in% A_countries, , drop = FALSE]
    if (length(unique(df_A[[outcome]])) >= 2 && length(unique(df_B[[outcome]])) >= 2)
      return(list(df_A = df_A, df_B = df_B, A = A_countries))
  }
  stop("Could not find a country split with both classes in both halves. Try increasing max_tries.")
}

# Partial out FE and return residuals (used for fallback ranking)
.partial_resid <- function(vec, df, country_col, year_col) {
  Z <- model.matrix(~ factor(df[[country_col]]) + factor(df[[year_col]]) - 1)
  q <- qr(Z)
  coef <- qr.coef(q, vec)
  as.numeric(vec - Z %*% coef)
}

# Tidy fixest and ensure a 'se' column exists
tidy_fixest <- function(m) {
  tt <- broom::tidy(m)
  if ("std.error" %in% names(tt)) {
    tt <- dplyr::mutate(tt, se = .data$std.error)
  } else if ("Std..Error" %in% names(tt)) {
    tt <- dplyr::mutate(tt, se = .data$Std..Error)
  } else if (!"se" %in% names(tt) && all(c("estimate","statistic") %in% names(tt))) {
    tt <- dplyr::mutate(tt, se = dplyr::if_else(is.finite(.data$statistic) & .data$statistic != 0,
                                                abs(.data$estimate/.data$statistic), NA_real_))
  } else if (!"se" %in% names(tt)) {
    tt$se <- NA_real_
  }
  tt
}

# -------------------------------------------------------
# 1) SAMPLE-SPLITTING / CROSS-FITTING (FE OLS or FE Logit)
# -------------------------------------------------------
crossfit_inference_panel <- function(
  data,
  outcome,
  predictors,
  country_col, year_col,
  family = c("gaussian","binomial"),        # refit family
  categorical_predictors = NULL,
  alpha = 1,                                # primary alpha for selection
  include_fe_in_refit = TRUE,
  two_way_cluster = TRUE,
  seed = 42,
  # selection fallbacks:
  min_vars = 1,                             # ensure at least this many controls
  selection_alphas = c(1, 0.5, 0.25),       # try EN if pure LASSO selects none
  fallback_k = 3                            # if still empty, pick top-K by FE-partial corr
) {
  fam <- match.arg(family)

  df <- data |>
    dplyr::select(all_of(c(country_col, year_col, predictors, outcome))) |>
    tidyr::drop_na() |>
    cast_categoricals(categorical_predictors)

  split <- balanced_country_split(df, outcome, country_col, seed = seed)
  df_A <- split$df_A; df_B <- split$df_B

  one_direction <- function(sel_df, est_df) {
    # train-only scaling
    sc <- train_scale_numeric(sel_df, est_df, predictors)
    sel_df <- sc$train; est_df <- sc$test

    # selection design + penalty factors (FE unpenalized)
    mm_sel <- make_mm_with_fe(sel_df, outcome, predictors, country_col, year_col)
    pf <- rep(1, ncol(mm_sel$X))
    fe_cols <- which(mm_sel$assign %in% mm_sel$fe_term_idx)
    if (length(fe_cols)) pf[fe_cols] <- 0

    # selection target (fallback to binomial if gaussian degenerate)
    sel_family <- if (fam == "binomial") "binomial" else "gaussian"
    y_sel <- if (sel_family == "binomial") factor(sel_df[[outcome]]) else y_gauss(sel_df[[outcome]])
    if (sel_family == "gaussian" && (length(unique(y_sel)) < 2 || is.na(var(y_sel)) || var(y_sel) == 0)) {
      sel_family <- "binomial"; y_sel <- factor(sel_df[[outcome]])
    }
    type_meas <- if (sel_family == "binomial") "auc" else "mse"

    # staged selection with fallbacks
    sel_vars <- character(0)
    for (a in unique(c(alpha, selection_alphas))) {
      cvfit <- glmnet::cv.glmnet(
        x = mm_sel$X, y = y_sel,
        family = sel_family, alpha = a, type.measure = type_meas,
        standardize = FALSE, nlambda = 200, lambda.min.ratio = 1e-5,
        penalty.factor = pf
      )
      for (sname in c("lambda.1se", "lambda.min")) {
        beta <- coef(cvfit, s = cvfit[[sname]])
        sel_vars <- selected_base_vars(beta, mm_sel, predictors, categorical_predictors)
        if (length(sel_vars) >= min_vars) break
      }
      if (length(sel_vars) >= min_vars) break
    }

    # final fallback: top-K by FE-partial |corr|
    if (length(sel_vars) < min_vars) {
      y_res <- .partial_resid(y_gauss(sel_df[[outcome]]), sel_df, country_col, year_col)
      scores <- setNames(numeric(0), character(0))
      for (v in predictors) {
        if (!is.null(categorical_predictors) && v %in% categorical_predictors && is.factor(sel_df[[v]])) {
          M <- model.matrix(~ sel_df[[v]] - 1)
          cors <- apply(M, 2, function(col) {
            x_res <- .partial_resid(col, sel_df, country_col, year_col)
            suppressWarnings(abs(cor(y_res, x_res)))
          })
          scores[v] <- max(cors, na.rm = TRUE)
        } else {
          x_res <- .partial_resid(sel_df[[v]], sel_df, country_col, year_col)
          scores[v] <- suppressWarnings(abs(cor(y_res, x_res)))
        }
      }
      scores[is.na(scores)] <- 0
      k <- max(min_vars, fallback_k)
      sel_vars <- names(sort(scores, decreasing = TRUE))[seq_len(min(k, length(scores)))]
    }

    # FE refit on estimation half
    rhs <- paste(sel_vars, collapse = " + ")
    fe_part <- if (include_fe_in_refit) paste0("| ", country_col, " + ", year_col) else ""
    fml <- as.formula(paste0(outcome, " ~ ", rhs, " ", fe_part))

    if (fam == "gaussian") {
      est_df <- ensure_numeric_outcome(est_df, outcome)
      m <- fixest::feols(
        fml, data = est_df,
        cluster = if (two_way_cluster) as.formula(paste0("~", country_col, " + ", year_col))
                  else as.formula(paste0("~", country_col))
      )
    } else {
      if (!is.factor(est_df[[outcome]])) est_df[[outcome]] <- factor(est_df[[outcome]])
      m <- fixest::feglm(
        fml, data = est_df, family = binomial(),
        cluster = if (two_way_cluster) as.formula(paste0("~", country_col, " + ", year_col))
                  else as.formula(paste0("~", country_col))
      )
    }

    tidy_fixest(m) |>
      dplyr::filter(term != "(Intercept)") |>
      dplyr::select(term, estimate, se)
  }

  est_AB <- one_direction(df_A, df_B)
  est_BA <- one_direction(df_B, df_A)
  ests <- Filter(Negate(is.null), list(est_AB, est_BA))
  if (!length(ests)) stop("Selection failed in both directions even after fallbacks.")
  combine_splits(ests)
}

# -------------------------------------------------------
# 2) DOUBLE-SELECTION for focal regressor (FE-OLS preferred)
# -------------------------------------------------------
double_selection_fe <- function(
  data,
  outcome,
  focal,
  controls,
  country_col, year_col,
  categorical_predictors = NULL,
  alpha = 1,
  family = c("gaussian","binomial"),
  two_way_cluster = TRUE
) {
  fam <- match.arg(family)

  df <- data |>
    dplyr::select(all_of(c(country_col, year_col, outcome, focal, controls))) |>
    tidyr::drop_na() |>
    cast_categoricals(categorical_predictors)

  # Y ~ controls (FE unpenalized)
  mm_y <- make_mm_with_fe(df, outcome, controls, country_col, year_col)
  pf_y <- rep(1, ncol(mm_y$X)); pf_y[which(mm_y$assign %in% mm_y$fe_term_idx)] <- 0
  penalized_y <- which(pf_y == 1)

  y_sel <- if (fam == "binomial") factor(df[[outcome]]) else y_gauss(df[[outcome]])
  sel_family <- if (fam == "binomial") "binomial" else "gaussian"
  sel_type   <- if (fam == "binomial") "auc" else "mse"

  if (length(penalized_y) == 0) {
    sel_y <- character(0)
  } else {
    if (sel_family == "gaussian" && (length(unique(y_sel)) < 2 || is.na(var(y_sel)) || var(y_sel) == 0)) {
      sel_family <- "binomial"; sel_type <- "auc"; y_sel <- factor(df[[outcome]])
    }
    cv_y <- glmnet::cv.glmnet(
      x = mm_y$X, y = y_sel,
      family = sel_family, alpha = alpha, type.measure = sel_type,
      standardize = FALSE, nlambda = 200, lambda.min.ratio = 1e-5,
      penalty.factor = pf_y
    )
    sel_y <- selected_base_vars(coef(cv_y, s = cv_y$lambda.1se), mm_y, controls, categorical_predictors)
  }

  # focal ~ controls (FE unpenalized) — gaussian
  mm_x <- make_mm_with_fe(df, focal, controls, country_col, year_col)
  pf_x <- rep(1, ncol(mm_x$X)); pf_x[which(mm_x$assign %in% mm_x$fe_term_idx)] <- 0
  if (all(pf_x == 0)) {
    sel_x <- character(0)
  } else {
    cv_x <- glmnet::cv.glmnet(
      x = mm_x$X, y = y_gauss(df[[focal]]),
      family = "gaussian", alpha = alpha, type.measure = "mse",
      standardize = FALSE, nlambda = 200, lambda.min.ratio = 1e-5,
      penalty.factor = pf_x
    )
    sel_x <- selected_base_vars(coef(cv_x, s = cv_x$lambda.1se), mm_x, controls, categorical_predictors)
  }

  union_controls <- sort(unique(c(sel_y, sel_x)))
  rhs <- paste(c(focal, union_controls), collapse = " + ")
  fml <- as.formula(paste0(outcome, " ~ ", rhs, " | ", country_col, " + ", year_col))

  if (fam == "gaussian") {
    df <- ensure_numeric_outcome(df, outcome)
    mod <- fixest::feols(
      fml, data = df,
      cluster = if (two_way_cluster) as.formula(paste0("~", country_col, " + ", year_col))
                else as.formula(paste0("~", country_col))
    )
  } else {
    if (!is.factor(df[[outcome]])) df[[outcome]] <- factor(df[[outcome]])
    mod <- fixest::feglm(
      fml, data = df, family = binomial(),
      cluster = if (two_way_cluster) as.formula(paste0("~", country_col, " + ", year_col))
                else as.formula(paste0("~", country_col))
    )
  }

  tidy_fixest(mod) |> dplyr::filter(term == focal)
}

# -------------------------------------------------------
# 3) RELAXED LASSO refit (parsimony; NOT for p-values)
# -------------------------------------------------------
relaxed_lasso_refit <- function(
  data, outcome, predictors, country_col, year_col,
  categorical_predictors = NULL,
  family = c("gaussian","binomial"),
  alpha = 1, two_way_cluster = TRUE
) {
  fam <- match.arg(family)
  df <- data |>
    dplyr::select(all_of(c(country_col, year_col, predictors, outcome))) |>
    tidyr::drop_na() |>
    cast_categoricals(categorical_predictors)

  mm <- make_mm_with_fe(df, outcome, predictors, country_col, year_col)
  pf <- rep(1, ncol(mm$X)); pf[which(mm$assign %in% mm$fe_term_idx)] <- 0

  y_relax <- if (fam == "binomial") factor(df[[outcome]]) else y_gauss(df[[outcome]])

  cvfit <- glmnet::cv.glmnet(
    x = mm$X, y = y_relax,
    family = if (fam == "binomial") "binomial" else "gaussian",
    alpha = alpha, relax = TRUE, type.measure = if (fam == "binomial") "auc" else "mse",
    standardize = FALSE, nlambda = 200, lambda.min.ratio = 1e-5,
    penalty.factor = pf
  )

  sel <- selected_base_vars(coef(cvfit, s = cvfit$lambda.1se), mm, predictors, categorical_predictors)
  if (!length(sel)) return(list(selected = character(0), model = NULL))

  rhs <- paste(sel, collapse = " + ")
  fml <- as.formula(paste0(outcome, " ~ ", rhs, " | ", country_col, " + ", year_col))

  if (fam == "gaussian") {
    df <- ensure_numeric_outcome(df, outcome)
    m <- fixest::feols(
      fml, data = df,
      cluster = if (two_way_cluster) as.formula(paste0("~", country_col, " + ", year_col))
                else as.formula(paste0("~", country_col))
    )
  } else {
    if (!is.factor(df[[outcome]])) df[[outcome]] <- factor(df[[outcome]])
    m <- fixest::feglm(
      fml, data = df, family = binomial(),
      cluster = if (two_way_cluster) as.formula(paste0("~", country_col, " + ", year_col))
                else as.formula(paste0("~", country_col))
    )
  }
  list(selected = sel, model = m)
}

# ================================
# Example: plug in your objects
# ================================
predictors_baseline <- c(
  "ODA_RECEIVED_PER_CAPITA","GDP_GROWTH","GDP_PER_CAPITA","GDP_DEFLATOR",
  "POLITICAL_REGIME","ELECTORAL_DEMOCRACY_SCORE","LIBERAL_DEMOCRACY_SCORE",
  "TERRITORIAL_FRAGMENTATION","INSTITUTIONAL_DEMOCRACY_SOCRE","INSTITUTIONAL_AUTOCRACY_SCORE",
  "COMBINED_POLITY_SCORE","REGIME_DURABILITY_YEARS","INSTITUTIONAL_EXECUTIVE_RECRUTIMENT",
  "POLITICAL_COMPETITION_SCORE","PARTIAL_DEMOCRACY_WITH_FACTIONALISM",
  "CONFLICT_INTENSITY_YEAR","CONFLICT_CUMULATIVE_INTENSITY_ACROSS_YEARS",
  "N_WAR_FRONTS","MAX_CONFLICT_INTENSITY","AVG_CONFLICT_INTENSITY","N_TOTAL_TROOPS"
)

categoricals_baseline <- c(
  # e.g., "POLITICAL_REGIME","PARTIAL_DEMOCRACY_WITH_FACTIONALISM"
)

# (1) Cross-fitting inference (FE-OLS)
set.seed(42)
cf_ols <- crossfit_inference_panel(
  data = final_clean_percentiles_data_normalized_1,
  outcome = "VDEM_STATUS_IDEAL",
  predictors = predictors_baseline,
  country_col = "COUNTRY_NAME", year_col = "YEAR",
  family = "gaussian",
  categorical_predictors = categoricals_baseline,
  alpha = 1,
  include_fe_in_refit = TRUE,
  two_way_cluster = TRUE,
  min_vars = 2,                 # require at least 2 controls (adjust)
  selection_alphas = c(1, .5),  # try EN if pure LASSO selects none
  fallback_k = 3                # fallback: top-3 FE-partial corr
)
cf_ols

# (2) Double selection (FE-OLS) for a focal regressor
ds_result <- double_selection_fe(
  data = final_clean_percentiles_data_normalized_1,
  outcome = "VDEM_STATUS_IDEAL",
  focal = "ELECTORAL_DEMOCRACY_SCORE",
  controls = setdiff(predictors_baseline, "ELECTORAL_DEMOCRACY_SCORE"),
  country_col = "COUNTRY_NAME", year_col = "YEAR",
  categorical_predictors = categoricals_baseline,
  alpha = 1, family = "gaussian", two_way_cluster = TRUE
)
ds_result

# (3) Relaxed LASSO refit (parsimony; NOT for p-values)
relaxed <- relaxed_lasso_refit(
  data = final_clean_percentiles_data_normalized_1,
  outcome = "VDEM_STATUS_IDEAL",
  predictors = predictors_baseline,
  country_col = "COUNTRY_NAME", year_col = "YEAR",
  categorical_predictors = categoricals_baseline,
  family = "gaussian", alpha = 1, two_way_cluster = TRUE
)
relaxed$selected
if (!is.null(relaxed$model)) summary(relaxed$model)
```

## 6.1. Pure LASSO

```{r, echo = FALSE, message = FALSE, warning = FALSE}

# ------------------------------------------------------------
# Stability selection for panel data (country-block subsamples)
# ------------------------------------------------------------
stability_selection_panel <- function(
  data,
  predictors,
  outcome            = "VDEM_STATUS_IDEAL",
  country_col        = "COUNTRY_NAME",
  year_col           = "YEAR",
  holdout_year       = NULL,       # if not NULL, only use rows <= holdout_year
  B                  = 500,        # # subsamples (200–1000 typical)
  subsample_frac     = 0.5,        # CPSS: 0.5 is standard
  alpha              = 1,          # 1 = LASSO
  use_lambda         = c("1se","min"),
  glmnet_standardize = FALSE,      # set TRUE if you want glmnet to re-standardize
  seed               = 123
) {
  use_lambda <- match.arg(use_lambda)

  # --- prepare sample (no leakage if you pass holdout_year) ---
  df <- data %>%
    dplyr::select(all_of(c(country_col, year_col, predictors, outcome))) %>%
    tidyr::drop_na()

  if (!is.null(holdout_year))
    df <- dplyr::filter(df, .data[[year_col]] <= holdout_year)

  # binary factor outcome
  df[[outcome]] <- droplevels(factor(df[[outcome]]))
  stopifnot(nlevels(df[[outcome]]) == 2L)
  lv <- levels(df[[outcome]]); pos <- lv[2]

  # design (no intercept)
  form <- as.formula(paste0(outcome, " ~ ", paste(predictors, collapse = " + "), " - 1"))
  X_all <- model.matrix(form, data = df)
  y_all <- droplevels(df[[outcome]])
  cntry <- df[[country_col]]

  # container for counts and avg selected size (q-hat)
  p <- ncol(X_all)
  select_counts <- setNames(numeric(p), colnames(X_all))
  q_vec <- numeric(B)

  set.seed(seed)
  countries <- unique(cntry)
  m <- ceiling(length(countries) * subsample_frac)  # # countries per subsample

  for (b in seq_len(B)) {
    # ---- country-block subsample ----
    c_take <- sample(countries, m, replace = FALSE)
    idx    <- cntry %in% c_take
    Xb     <- X_all[idx, , drop = FALSE]
    yb     <- droplevels(y_all[idx])

    # class weights in the subsample
    n_pos <- sum(yb == pos); n_neg <- sum(yb != pos)
    w <- ifelse(yb == pos, n_neg / max(1, n_pos), 1)

    # ---- fit glmnet on subsample ----
    fit <- try(
      cv.glmnet(
        x = Xb, y = yb,
        family = "binomial", alpha = alpha,
        type.measure = "auc",
        nlambda = 200, lambda.min.ratio = 1e-5,
        weights = w,
        standardize = glmnet_standardize
      ),
      silent = TRUE
    )
    if (inherits(fit, "try-error")) next

    lam <- if (use_lambda == "1se") fit$lambda.1se else fit$lambda.min
    beta <- as.matrix(coef(fit, s = lam))[-1, , drop = FALSE]  # drop intercept
    sel  <- as.numeric(beta != 0)
    select_counts <- select_counts + sel
    q_vec[b] <- sum(sel)
  }

  freq <- select_counts / B
  qhat <- mean(q_vec[q_vec > 0], na.rm = TRUE)  # avg selected per subsample
  if (is.na(qhat)) qhat <- 0

  out <- data.frame(variable = names(freq), freq = as.numeric(freq)) %>%
    arrange(desc(freq))

  attr(out, "B") <- B
  attr(out, "qhat") <- qhat
  attr(out, "p") <- p
  attr(out, "use_lambda") <- use_lambda
  attr(out, "alpha") <- alpha
  attr(out, "subsample_frac") <- subsample_frac
  out
}

# ------------------------------------------------------------
# PFER helper (Meinshausen & Bühlmann 2010) for CPSS (0.5 subsampling)
#   PFER <= (q^2) / ((2*pi_thr - 1) * p)
# Given p, qhat, pick pi_thr to target PFER target (e.g., 1)
# ------------------------------------------------------------
pi_from_pfer <- function(p, qhat, pfer_target = 1) {
  if (qhat <= 0) return(NA_real_)
  # solve for pi in PFER = q^2 / ((2*pi -1) * p)
  # => (2*pi - 1) = q^2 / (PFER * p)
  rhs <- (qhat^2) / (pfer_target * p)
  pi_thr <- (rhs + 1) / 2
  max(min(pi_thr, 0.99), 0.51)  # clamp to a sensible range (>0.5)
}

pfer_value <- function(p, qhat, pi_thr) {
  if (pi_thr <= 0.5) return(Inf)
  (qhat^2) / ((2*pi_thr - 1) * p)
}

# ------------------------------------------------------------
# Take stability table + cutoff -> stable set; refit clustered FE GLM
# ------------------------------------------------------------
refit_on_stable_set <- function(
  stability_table,
  data,
  outcome, country_col, year_col,
  cutoff_pi = NULL,            # if NULL, compute from PFER target
  pfer_target = 1,
  add_fe = TRUE,               # country + year FE
  cluster_two_way = TRUE       # cluster by country + year
) {
  B     <- attr(stability_table, "B")
  qhat  <- attr(stability_table, "qhat")
  p     <- attr(stability_table, "p")

  if (is.null(cutoff_pi)) {
    cutoff_pi <- pi_from_pfer(p, qhat, pfer_target = pfer_target)
  }
  stable_vars <- stability_table %>% filter(freq >= cutoff_pi) %>% pull(variable)

  # Build FE formula
  rhs <- if (length(stable_vars)) paste(stable_vars, collapse = " + ") else "1"
  fe_part <- if (add_fe) paste0(" | ", country_col, " + ", year_col) else ""
  fml <- as.formula(paste0(outcome, " ~ ", rhs, fe_part))

  # prep data (drop NA)
  df <- data %>%
    dplyr::select(all_of(c(country_col, year_col, outcome, stable_vars))) %>%
    tidyr::drop_na()
  df[[outcome]] <- factor(df[[outcome]])

  # refit (binomial)
  m <- fixest::feglm(
    fml, data = df, family = binomial(),
    cluster = if (cluster_two_way) as.formula(paste0("~", country_col, " + ", year_col))
              else as.formula(paste0("~", country_col))
  )

  list(
    cutoff_pi = cutoff_pi,
    pfer_implied = pfer_value(p, qhat, cutoff_pi),
    qhat = qhat, B = B, p = p,
    stable_vars = stable_vars,
    model = m,
    table = stability_table
  )
}

# ===========================
# Example usage
# ===========================
# Your standardized predictors (exactly as in your previous runs)
predictors_baseline <- c(
  "ODA_RECEIVED_PER_CAPITA","GDP_GROWTH","GDP_PER_CAPITA","GDP_DEFLATOR",
  "POLITICAL_REGIME","ELECTORAL_DEMOCRACY_SCORE","LIBERAL_DEMOCRACY_SCORE",
  "TERRITORIAL_FRAGMENTATION","INSTITUTIONAL_DEMOCRACY_SOCRE","INSTITUTIONAL_AUTOCRACY_SCORE",
  "COMBINED_POLITY_SCORE","REGIME_DURABILITY_YEARS","INSTITUTIONAL_EXECUTIVE_RECRUTIMENT",
  "POLITICAL_COMPETITION_SCORE","PARTIAL_DEMOCRACY_WITH_FACTIONALISM",
  "CONFLICT_INTENSITY_YEAR","CONFLICT_CUMULATIVE_INTENSITY_ACROSS_YEARS",
  "N_WAR_FRONTS","MAX_CONFLICT_INTENSITY","AVG_CONFLICT_INTENSITY","N_TOTAL_TROOPS"
)

# 1) Run stability selection on the "training era" (e.g., <= 2016)
set.seed(123)
stab <- stability_selection_panel(
  data = final_clean_percentiles_data_normalized_1,
  predictors = predictors_baseline,
  outcome = "VDEM_STATUS_IDEAL",
  country_col = "COUNTRY_NAME",
  year_col    = "YEAR",
  holdout_year = 2016,     # avoids peeking at later years
  B = 500,
  subsample_frac = 0.5,
  alpha = 1,               # pure LASSO
  use_lambda = "1se",      # conservative; try "min" if too sparse
  glmnet_standardize = FALSE
)

head(stab, 10)                 # variables ranked by selection frequency
attr(stab, "qhat"); attr(stab, "p")

# 2) Pick a cutoff by targeting PFER <= 1 (or set a manual cutoff, e.g., 0.6)
cut_pi <- pi_from_pfer(p = attr(stab, "p"), qhat = attr(stab, "qhat"), pfer_target = 1)
cut_pi

# 3) Refit on the stable set (country & year FE, clustered SEs)
stable_fit <- refit_on_stable_set(
  stability_table = stab,
  data = final_clean_percentiles_data_normalized_1,
  outcome = "VDEM_STATUS_IDEAL",
  country_col = "COUNTRY_NAME", year_col = "YEAR",
  cutoff_pi = cut_pi,      # or NULL to compute from PFER target again
  pfer_target = 1,
  add_fe = TRUE, cluster_two_way = TRUE
)

stable_fit$cutoff_pi
stable_fit$pfer_implied
stable_fit$stable_vars
summary(stable_fit$model)
```

## 6.1. Post-Double Selection

```{r, echo = FALSE, message = FALSE, warning = FALSE}

# ------------------------------------------------------------
# Post-Double Selection (PDS) with cross-fitting for FE logit
# ------------------------------------------------------------

pds_logit_fe <- function(
  data,
  y,                    # binary outcome name (0/1 or factor with 2 levels)
  D,                    # character vector of predictors of interest for inference
  W,                    # character vector of nuisance controls for selection
  id,                   # clustering/FE unit (e.g., COUNTRY_NAME)
  time = NULL,          # optional time FE (e.g., YEAR); set NULL to omit
  K = 5,                # group folds by id for cross-fitting
  alpha = 1,            # LASSO (can pass 0<alpha<1 for EN)
  lambda_rule = c("1se","min"),  # selection stringency
  standardize = TRUE,
  seed = 42
){
  stopifnot(all(c(y, D, W, id) %in% names(data)))
  lambda_rule <- match.arg(lambda_rule)

  # Clean & ensure binary y
  df <- data %>% select(all_of(unique(c(y, D, W, id, time)))) %>% tidyr::drop_na()
  if (is.factor(df[[y]])) {
    if (nlevels(df[[y]]) != 2L) stop("Outcome must have 2 levels.")
    ybin <- as.integer(df[[y]] == levels(df[[y]])[2L])
  } else {
    if (!all(df[[y]] %in% c(0,1))) stop("Outcome must be coded 0/1 or a 2-level factor.")
    ybin <- df[[y]]
  }

  # Build model.matrix for W only (selection step) with 0 intercept
  W_form <- if (length(W)) as.formula(paste0("~ 0 + ", paste(W, collapse = " + "))) else ~ 0
  XW <- model.matrix(W_form, data = df)

  # Group folds by id
  set.seed(seed)
  ids <- unique(df[[id]])
  fold_map <- setNames(sample(rep(1:K, length.out = length(ids))), ids)
  foldid <- as.integer(fold_map[df[[id]]])

  # Helper: get nonzero names
  nonzero_names <- function(cvfit, s_choice) {
    s <- if (s_choice == "1se") cvfit$lambda.1se else cvfit$lambda.min
    nz <- coef(cvfit, s = s)
    nn <- rownames(nz)[as.numeric(nz) != 0]
    setdiff(nn, "(Intercept)")
  }

  # Cross-fitting: select on K training folds; union everything
  S_list <- vector("list", K)
  for (k in 1:K) {
    tr <- foldid != k
    if (!any(tr) || all(tr)) next

    XW_tr <- XW[tr, , drop = FALSE]
    y_tr  <- ybin[tr]

    # Outcome lasso: y ~ W  (binomial, AUC CV)
    if (ncol(XW_tr) > 0) {
      cv_y <- cv.glmnet(
        x = XW_tr, y = y_tr,
        family = "binomial", alpha = alpha,
        type.measure = "auc", standardize = standardize, nlambda = 200
      )
      S_y <- intersect(colnames(XW_tr), nonzero_names(cv_y, lambda_rule))
    } else {
      S_y <- character(0)
    }

    # Treatment lasso(s): D_j ~ W (family depends on D_j)
    S_d_all <- character(0)
    for (dj in D) {
      dj_vec <- df[[dj]][tr]
      fam <- if (all(dj_vec %in% c(0,1))) "binomial" else "gaussian"
      type_meas <- if (fam == "binomial") "auc" else "mse"
      # need finite variance:
      if (sd(dj_vec, na.rm = TRUE) == 0 || ncol(XW_tr) == 0) next

      cv_d <- cv.glmnet(
        x = XW_tr, y = dj_vec,
        family = fam, alpha = alpha,
        type.measure = type_meas, standardize = standardize, nlambda = 200
      )
      S_d <- intersect(colnames(XW_tr), nonzero_names(cv_d, lambda_rule))
      S_d_all <- union(S_d_all, S_d)
    }

    S_list[[k]] <- union(S_y, S_d_all)
  }

  S_union <- sort(unique(unlist(S_list)))
  message(sprintf("PDS selected %d controls (union over %d folds).", length(S_union), K))

  # Final inferential model: FE logit with clustered SEs
  fe_part <- if (!is.null(time)) paste0(id, " + ", time) else id
  rhs_D   <- if (length(D)) paste(D, collapse = " + ") else "1"
  rhs_W   <- if (length(S_union)) paste(S_union, collapse = " + ") else "1"
  fml     <- as.formula(paste0(y, " ~ ", rhs_D, " + ", rhs_W, " | ", fe_part))

  fe_fit <- fixest::feglm(
    fml, data = df, family = binomial(),
    cluster = as.formula(paste0("~", id))
    # for two-way clustering: cluster = ~ COUNTRY_NAME + YEAR
  )

  list(
    fe_fit   = fe_fit,
    selected = S_union,
    folds    = S_list,
    call     = match.call()
  )
}

# ------------------------------------------------------------------------------
# LIBERAL DEMOCRACY TEST -------------------------------------------------------
# ------------------------------------------------------------------------------

D_interest <- c("LIBERAL_DEMOCRACY_SCORE",
                "GDP_PER_CAPITA")

W_names    <- setdiff(predictors_baseline, D_interest)

pds_fit <- pds_logit_fe(
  data = standardized_data_final,
  y    = "VDEM_FRAGILE_IDEAL",
  D    = D_interest,
  W    = W_names,
  id   = "COUNTRY_NAME",
  # Including year fixed effects
  time = "YEAR",      
  K    = 5, alpha = 1, lambda_rule = "1se"
)

fixest::etable(pds_fit$fe_fit)

# ------------------------------------------------------------------------------
df <- standardized_data_final
D        <- "GDP_PER_CAPITA"
controls <- pds_fit$selected
rhs_vec  <- c(D, controls)

# main part: y ~ X (no FE here)
fml_main <- reformulate(termlabels = rhs_vec, response = "VDEM_FRAGILE_IDEAL")

# LPM + FE with clustered SEs
lpm_pds <- fixest::feols(
  fml_main,
  data   = df,
  fixef  = c("COUNTRY_NAME", "YEAR"),   # <- character vector
  cluster = c("COUNTRY_NAME")           # or two-way: c("COUNTRY_NAME","YEAR")
)
fixest::etable(lpm_pds)

```

# Appendix A

## A.1. Logit and Binomial Coefficients: Relaxed Classification

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# ------------------------------------------------------------
# 1. Build RHS string (typos intact) + year FE ---------------
# ------------------------------------------------------------

form_baseline_relaxed <- as.formula(
  paste("VDEM_FRAGILE_BASELINE ~", paste(baseline_vars, 
                                         collapse = " + "))
)

# ------------------------------------------------------------
# 2. OLS (cluster-robust at COUNTRY_NAME) --------------------
# ------------------------------------------------------------
linear_model_relaxed <- lm(form_baseline, 
                           data = standardized_data_final)

clustered_se_linear_relaxed <- sqrt(diag(vcovCL(linear_model_relaxed, 
                                                cluster = ~ COUNTRY_NAME)))

# ------------------------------------------------------------
# 3. Logit (cluster-robust at COUNTRY_NAME) ------------------
# ------------------------------------------------------------
logit_model_relaxed  <- glm(form_baseline_relaxed,
                            family = binomial(link = "logit"),
                            data   = standardized_data_final)

clustered_se_logit_relaxed <- sqrt(diag(vcovCL(logit_model_relaxed, 
                                               cluster = ~ COUNTRY_NAME)))

# ------------------------------------------------------------
# 4. Fixed-effects (within) + Driscoll–Kraay SEs -------------
# ------------------------------------------------------------
fixed_effects_model_relaxed <- plm(form_baseline_relaxed,
                                   data  = standardized_data_final,
                                   model = "within",
                                   index = c("ISO_CODE_3", "YEAR"))

fe_dk_se_relaxed <- sqrt(diag(vcovSCC(fixed_effects_model_relaxed,
                                      maxlag = 3,
                                      type = "HC1")))

# ------------------------------------------------------------
# 5. Fit statistics ------------------------------------------
# ------------------------------------------------------------
R2_linear_relaxed <- summary(linear_model_relaxed)$r.squared

logit_null_relaxed <- glm(VDEM_FRAGILE_BASELINE ~ 1,
                          family = binomial,
                          data   = standardized_data_final)

McFadden_R2_relaxed <- 1 - (as.numeric(logLik(logit_model_relaxed)) /
                            as.numeric(logLik(logit_null_relaxed)))

add_stats_relaxed <- list(
  c("R-squared (OLS)",          sprintf("%.3f", R2_linear_relaxed),      ""),
  c("Pseudo R-squared (Logit)", "",                              sprintf("%.3f", McFadden_R2_relaxed)),
  c("AIC",                      sprintf("%.2f", AIC(linear_model_relaxed)), sprintf("%.2f", AIC(logit_model_relaxed))),
  c("BIC",                      sprintf("%.2f", BIC(linear_model_relaxed)), sprintf("%.2f", BIC(logit_model_relaxed)))
)
```

```{r, linear-binomial-relaxed, echo = FALSE, message = FALSE, warning = FALSE, results = 'asis'}
# ------------------------------------------------------------
# 6. LaTeX table (labels unchanged, year FE omitted) ---------
# ------------------------------------------------------------
stargazer(linear_model_relaxed,
          logit_model_relaxed,
          fixed_effects_model_relaxed,
          se = list(clustered_se_linear_relaxed,
                    clustered_se_logit_relaxed,
                    fe_dk_se_relaxed),
          covariate.labels = c("Combined polity score",
                               "Institutional autocracy score",
                               "GDP growth",
                               "MAX conflict intensity",
                               "N total troops involved",
                               "Territorial fragmentation",
                               "Executive recruitment",
                               "Institutional democracy score",
                               "Liberal democracy score",
                               "Regime durability in years",
                               "GDP per capita growth",
                               "Political regime",
                               "N war of fronts",
                               "ODA received per capita",
                               "Conflitc cumul. intensity across years",
                               "Electoral democracy score",
                               "Political competition score",
                               "Conflict intensity year",
                               "AVG conflict intensity",
                               "GDP deflator",
                               "Partial democracy with factionalism",
                               "Constant"),
          omit = "factor\\(YEAR\\)",    
          header = FALSE,
          no.space = TRUE,
          font.size = "small",
          add.lines = add_stats_relaxed,
          title = "Linear, Logistic and FE Estimates of Fragility Status (Relaxed Classification)",
          dep.var.caption = "Fragility Status",
          dep.var.labels = "")
```

## A.2. Logit and Binomial Coefficients: Strict Classification

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# ------------------------------------------------------------
# 1. Build RHS string (typos intact) + year FE ---------------
# ------------------------------------------------------------

form_baseline_strict <- as.formula(
  paste("VDEM_FRAGILE_ENDLINE ~", paste(baseline_vars, 
                                         collapse = " + "))
)

# ------------------------------------------------------------
# 2. OLS (cluster-robust at COUNTRY_NAME) --------------------
# ------------------------------------------------------------
linear_model_strict <- lm(form_baseline, 
                          data = standardized_data_final)

clustered_se_linear_strict <- sqrt(diag(vcovCL(linear_model_strict, 
                                               cluster = ~ COUNTRY_NAME)))

# ------------------------------------------------------------
# 3. Logit (cluster-robust at COUNTRY_NAME) ------------------
# ------------------------------------------------------------
logit_model_strict  <- glm(form_baseline_strict,
                           family = binomial(link = "logit"),
                           data   = standardized_data_final)

clustered_se_logit_strict <- sqrt(diag(vcovCL(logit_model_strict, 
                                              cluster = ~ COUNTRY_NAME)))

# ------------------------------------------------------------
# 4. Fixed-effects (within) + Driscoll–Kraay SEs -------------
# ------------------------------------------------------------
fixed_effects_model_strict <- plm(form_baseline_strict,
                                  data  = standardized_data_final,
                                  model = "within",
                                  index = c("ISO_CODE_3", "YEAR"))

fe_dk_se_strict <- sqrt(diag(vcovSCC(fixed_effects_model_strict,
                                     maxlag = 3,
                                     type = "HC1")))

# ------------------------------------------------------------
# 5. Fit statistics ------------------------------------------
# ------------------------------------------------------------
R2_linear_strict <- summary(linear_model_strict)$r.squared

logit_null_strict <- glm(VDEM_FRAGILE_BASELINE ~ 1,
                         family = binomial,
                         data   = standardized_data_final)

McFadden_R2_strict <- 1 - (as.numeric(logLik(logit_model_strict)) /
                           as.numeric(logLik(logit_null_strict)))

add_stats_strict <- list(
  c("R-squared (OLS)",          sprintf("%.3f", R2_linear_strict),      ""),
  c("Pseudo R-squared (Logit)", "",                              sprintf("%.3f", McFadden_R2_strict)),
  c("AIC",                      sprintf("%.2f", AIC(linear_model_strict)), sprintf("%.2f", AIC(logit_model_strict))),
  c("BIC",                      sprintf("%.2f", BIC(linear_model_strict)), sprintf("%.2f", BIC(logit_model_strict)))
)
```

```{r, linear-binomial-strict, echo = FALSE, message = FALSE, warning = FALSE, results = 'asis'}
# ------------------------------------------------------------
# 6. LaTeX table (labels unchanged, year FE omitted) ---------
# ------------------------------------------------------------
stargazer(linear_model_strict,
          logit_model_strict,
          fixed_effects_model_strict,
          se = list(clustered_se_linear_strict,
                    clustered_se_logit_strict,
                    fe_dk_se_strict),
          covariate.labels = c("Combined polity score",
                               "Institutional autocracy score",
                               "GDP growth",
                               "MAX conflict intensity",
                               "N total troops involved",
                               "Territorial fragmentation",
                               "Executive recruitment",
                               "Institutional democracy score",
                               "Liberal democracy score",
                               "Regime durability in years",
                               "GDP per capita growth",
                               "Political regime",
                               "N war of fronts",
                               "ODA received per capita",
                               "Conflitc cumul. intensity across years",
                               "Electoral democracy score",
                               "Political competition score",
                               "Conflict intensity year",
                               "AVG conflict intensity",
                               "GDP deflator",
                               "Partial democracy with factionalism",
                               "Constant"),
          omit = "factor\\(YEAR\\)",     
          header = FALSE,
          no.space = TRUE,
          font.size = "small",
          add.lines = add_stats_strict,
          title = "Linear, Logistic and FE Estimates of Fragility Status (Strict Classification)",
          dep.var.caption = "Fragility Status",
          dep.var.labels = "")
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}

# ==========================================================
# Fragility Exit Hazard — WP-style core ID + robustness
# Data object: standardized_data_final
# ==========================================================

suppressPackageStartupMessages({
  library(dplyr)
  library(fixest)      # feglm (GLM) with clustering and FE
  library(brglm2)      # bias-reduced logistic regression
  library(glmmTMB)     # random-intercept logit
  library(margins)     # marginal effects
  library(sandwich)    # clustered vcov
  library(lmtest)      # coeftest with custom vcov
  library(forcats)
})

# ------------------------------------------------------------------------------
# Settings ---------------------------------------------------------------------
# ------------------------------------------------------------------------------
COUNTRY <- "COUNTRY_NAME"
YEAR    <- "YEAR"
# "Fragile","Transitioning","Non Fragile"
STATUS  <- "VDEM_STATUS_IDEAL"     
# Lag coverage threshold
COVERAGE_MIN <- 0.70               
# sustained-exit year horizons for robustness
K_GRID       <- c(3L, 5L, 7L)      
# 2% leverage drop in influence check
DROP_TOP_LEVERAGE <- 0.02          

# Full predictors menu (we will prune by coverage and then select a CORE)
PREDICTORS <- c(
  "ODA_RECEIVED_PER_CAPITA",
  "GDP_GROWTH",
  "GDP_PER_CAPITA",
  "GDP_DEFLATOR",
  "POLITICAL_REGIME",
  "ELECTORAL_DEMOCRACY_SCORE",
  "LIBERAL_DEMOCRACY_SCORE",
  "TERRITORIAL_FRAGMENTATION",
  "INSTITUTIONAL_DEMOCRACY_SOCRE",
  "INSTITUTIONAL_AUTOCRACY_SCORE",
  "COMBINED_POLITY_SCORE",
  "REGIME_DURABILITY_YEARS",
  "INSTITUTIONAL_EXECUTIVE_RECRUTIMENT",
  "POLITICAL_COMPETITION_SCORE",
  "PARTIAL_DEMOCRACY_WITH_FACTIONALISM",
  "CONFLICT_INTENSITY_YEAR",
  "CONFLICT_CUMULATIVE_INTENSITY_ACROSS_YEARS",
  "N_WAR_FRONTS",
  "MAX_CONFLICT_INTENSITY",
  "AVG_CONFLICT_INTENSITY",
  "N_TOTAL_TROOPS"
)

# Theory-first CORE spec (kept if coverage permits)
CORE_NAMES <- c(
  "LIBERAL_DEMOCRACY_SCORE_L1",
  "REGIME_DURABILITY_YEARS_L1",
  "CONFLICT_INTENSITY_YEAR_L1",
  "AVG_CONFLICT_INTENSITY_L1",
  "GDP_GROWTH_L1",            
)

# -----------------
# Helpers
# -----------------
as_exit_dummy <- function(vec, k = 5L) {
  n <- length(vec); exit <- integer(n); nf <- (vec == "Non Fragile")
  for (t in seq_len(n)) if (nf[t] && t + k - 1 <= n && all(nf[t:(t + k - 1)])) { exit[t] <- 1L; break }
  exit
}

build_hazard_raw <- function(df, predictors, k_sustain) {
  df <- df %>% arrange(.data[[COUNTRY]], .data[[YEAR]])
  df <- df %>%
    group_by(.data[[COUNTRY]]) %>%
    mutate(EXIT_SUSTAINED = as_exit_dummy(.data[[STATUS]], k = k_sustain)) %>%
    ungroup() %>%
    group_by(.data[[COUNTRY]]) %>%
    mutate(
      ever_exited = any(EXIT_SUSTAINED == 1L),
      first_exit_year = dplyr::if_else(
        ever_exited, min(.data[[YEAR]][EXIT_SUSTAINED == 1L], na.rm = TRUE), NA_integer_
      ),
      at_risk = dplyr::if_else(
        .data[[STATUS]] %in% c("Fragile","Transitioning") &
          (is.na(first_exit_year) | .data[[YEAR]] < first_exit_year),
        1L, 0L
      )
    ) %>% ungroup()

  haz <- df %>% filter(at_risk == 1L | EXIT_SUSTAINED == 1L)

  # lag predictors by 1 year (keep NA for coverage audit)
  predictors <- intersect(predictors, names(haz))
  if (!length(predictors)) stop("None of the listed predictors are in the data.")
  haz <- haz %>%
    group_by(.data[[COUNTRY]]) %>%
    mutate(across(all_of(predictors), ~ dplyr::lag(.x, 1L), .names = "{.col}_L1")) %>%
    ungroup()

  # Clean factor lags (avoid weird numeric-level labels / NA as level)
  refactor_if_exists <- function(df, var) {
    if (var %in% names(df)) df[[var]] <- fct_drop(factor(df[[var]], exclude = NULL))
    df
  }
  haz <- refactor_if_exists(haz, "POLITICAL_REGIME_L1")
  haz <- refactor_if_exists(haz, "PARTIAL_DEMOCRACY_WITH_FACTIONALISM_L1")

  haz
}

prune_by_coverage <- function(haz_raw, predictors, cov_min) {
  lag_cols <- paste0(intersect(predictors, names(haz_raw)), "_L1")
  lag_cols <- intersect(lag_cols, names(haz_raw))
  if (!length(lag_cols)) stop("After lagging, no candidate predictors remained.")

  coverage <- sapply(lag_cols, function(v) mean(!is.na(haz_raw[[v]])))
  keep_lag <- names(coverage)[coverage >= cov_min]
  if (!length(keep_lag)) stop(paste0("No lagged predictors meet coverage >= ", cov_min))

  haz <- haz_raw %>% filter(!if_any(all_of(keep_lag), is.na))

  # drop zero-variance kept lags
  nzv <- vapply(haz[keep_lag], function(x) { ux <- unique(x[!is.na(x)]); length(ux) > 1 }, logical(1))
  list(haz = haz, keep_lag = keep_lag[nzv], coverage = sort(coverage, decreasing = TRUE))
}

fit_clustered <- function(model) {
  # helper to print clustered coeftest for glm/brglm2
  vc <- vcovCL(model, cluster = model$data[[COUNTRY]])
  coeftest(model, vcov = vc)
}

influence_drop <- function(glm_model, drop_frac = 0.02) {
  h <- hatvalues(glm_model)
  cutoff <- quantile(h, 1 - drop_frac, na.rm = TRUE)
  which(h >= cutoff)
}

# -----------------
# Load & prep
# -----------------
stopifnot(exists("standardized_data_final"))
df <- standardized_data_final %>%
  mutate(
    !!YEAR   := suppressWarnings(as.integer(.data[[YEAR]])),
    !!STATUS := as.character(.data[[STATUS]])
  )

# regime-like vars as factors (pre-lag)
fac_candidates <- c("POLITICAL_REGIME","PARTIAL_DEMOCRACY_WITH_FACTIONALISM")
for (v in fac_candidates) if (v %in% names(df)) df[[v]] <- factor(df[[v]], exclude = NULL)

# standardize numeric predictors (pre-lag)
num_vars <- setdiff(PREDICTORS, fac_candidates)
num_vars <- intersect(num_vars, names(df))
for (v in num_vars) if (is.numeric(df[[v]])) df[[v]] <- as.numeric(scale(df[[v]]))

cat("\n[Audit] Rows in original df:", nrow(df), "\n")

# -----------------
# Run the full pipeline for each K in K_GRID
# -----------------
for (K in K_GRID) {
  cat("\n================= K =", K, "=================\n")
  haz_raw <- build_hazard_raw(df, predictors = PREDICTORS, k_sustain = K)
  cat("[Audit] Risk-set + event rows (pre-lag-filter):", nrow(haz_raw), "\n")

  # Coverage audit (top 10)
  lag_cols_all <- paste0(intersect(PREDICTORS, names(haz_raw)), "_L1")
  lag_cols_all <- intersect(lag_cols_all, names(haz_raw))
  lag_cov <- sapply(lag_cols_all, function(v) mean(!is.na(haz_raw[[v]])))
  cat("[Audit] Lagged non-missing (top 10):\n")
  print(sort(round(lag_cov, 3), decreasing = TRUE)[1:min(10, length(lag_cov))])

  pruned <- prune_by_coverage(haz_raw, predictors = PREDICTORS, cov_min = COVERAGE_MIN)
  haz      <- pruned$haz
  keep_lag <- pruned$keep_lag
  cat("[Pruning] Kept lagged predictors (", length(keep_lag), ")\n", sep = "")
  cat(paste(keep_lag, collapse = ", "), "\n")
  cat("[Pruning] Rows after pruning:", nrow(haz), "\n")

  # ---- CORE model selection (based on theory & your early signal)
  core <- intersect(CORE_NAMES, keep_lag)
  # add GDP_DEFLATOR_L1 if available (proxy for nominal/ToT)
  if ("GDP_DEFLATOR_L1" %in% keep_lag) core <- union(core, "GDP_DEFLATOR_L1")
  if (length(core) < 3L) {
    # fall back to a larger kept set if coverage excluded too much
    core <- unique(c(core, head(setdiff(keep_lag, core), 5L)))
  }
  cat("[Core] Variables used:", paste(core, collapse = ", "), "\n")

  # -----------------
  # POOLED (year FE on by default), clustered by country
  # -----------------
  f_core_yearfe <- as.formula(paste("EXIT_SUSTAINED ~", paste(core, collapse = " + "), "+ i(", YEAR, ")", sep = ""))
  m_pool_core <- feglm(f_core_yearfe, data = haz, family = "logit", cluster = COUNTRY)
  cat("\n=== Pooled logit + Year FE (clustered) — CORE ===\n")
  print(summary(m_pool_core))

  # Influence diag: drop top 2% leverage and refit pooled without year FE (GLM) for transparency
  f_core <- as.formula(paste("EXIT_SUSTAINED ~", paste(core, collapse = " + ")))
  m_glm_core <- glm(f_core, data = haz, family = binomial("logit"))
  idx_hi <- influence_drop(m_glm_core, drop_frac = DROP_TOP_LEVERAGE)
  if (length(idx_hi)) {
    haz_rb <- haz[-idx_hi, , drop = FALSE]
    m_pool_core_rb <- feglm(f_core_yearfe, data = haz_rb, family = "logit", cluster = COUNTRY)
    cat("\n--- Refit after dropping top", round(100*DROP_TOP_LEVERAGE,1), "% leverage (n drop =", length(idx_hi), ") ---\n")
    print(summary(m_pool_core_rb))
  }

  # -----------------
  # RARE-EVENTS (bias-reduced) + clustered SEs — CORE
  # -----------------
  m_rare_core <- glm(f_core, data = haz, family = binomial("logit"),
                     method = "brglmFit", control = brglmControl(type = "AS_mixed"))
  cat("\n=== Rare-events (bias-reduced) — CORE ===\n")
  print(summary(m_rare_core))
  cat("\n--- Rare-events + clustered SEs (country) ---\n")
  print(fit_clustered(m_rare_core))
  
  co  <- coef(m_rare_core)
  V   <- vcovCL(m_rare_core, cluster = m_rare_core$data[[COUNTRY]])
  se  <- sqrt(diag(V))
  tmp <- data.frame(K = K, term = names(co), beta = unname(co), se = unname(se), row.names = NULL)
  
  if (!exists("COLLECT")) COLLECT <- tmp else COLLECT <- bind_rows(COLLECT, tmp)


  # -----------------
  # RANDOM-INTERCEPT (country) — CORE (graceful fallback)
  # -----------------
  f_re <- as.formula(paste("EXIT_SUSTAINED ~", paste(core, collapse = " + "), "+ (1 |", COUNTRY, ")"))
  m_re_core <- try(
    glmmTMB(f_re, data = haz, family = binomial(),
            control = glmmTMBControl(optimizer = optim, optArgs = list(method = "BFGS"))),
    silent = TRUE
  )
  cat("\n=== Random-intercept (country) — CORE ===\n")
  if (inherits(m_re_core, "try-error")) {
    cat("glmmTMB failed to converge from default starts (likely sparse categories / quasi-separation).\n")
  } else {
    print(summary(m_re_core))
  }

  # -----------------
  # AMEs for CORE pooled GLM (no year FE; AMEs are about X, not time dummies)
  # -----------------
  ame <- margins(m_glm_core)
  cat("\n=== Average Marginal Effects — CORE GLM ===\n")
  print(summary(ame))

  cat("\n================= End K =", K, "=================\n")
}


```

## Full Predictors 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# ================================================================
# Discrete-time hazard of sustained exit from fragility (WP-ready)
# ================================================================

suppressPackageStartupMessages({
  library(dplyr); library(tidyr); library(purrr); library(forcats)
  library(stringr)
  library(fixest)        # feglm (logit with FEs + i())
  library(brglm2)        # bias-reduced GLM
  library(glmmTMB)       # random intercepts (frailty)
  library(sandwich); library(lmtest)  # vcovCL + coeftest
  library(margins)       # AMEs
  library(splines)       # ns()
  # Optional diagnostics:
  have_pROC  <- rlang::is_installed("pROC")
  have_PRROC <- rlang::is_installed("PRROC")
  if (have_pROC)  library(pROC)
  if (have_PRROC) library(PRROC)
})

# ---------------- Settings ----------------
COUNTRY <- "COUNTRY_NAME"
YEAR    <- "YEAR"
STATUS  <- "VDEM_STATUS_IDEAL"  # "Fragile","Transitioning","Non Fragile"

COVERAGE_MIN       <- 0.70
K_GRID             <- c(3L, 5L, 7L)
DROP_TOP_LEVERAGE  <- 0.02

PREDICTORS <- c(
  "ODA_RECEIVED_PER_CAPITA",
  "GDP_GROWTH",
  "GDP_PER_CAPITA",
  "GDP_DEFLATOR",
  "POLITICAL_REGIME",
  "ELECTORAL_DEMOCRACY_SCORE",
  "LIBERAL_DEMOCRACY_SCORE",
  "TERRITORIAL_FRAGMENTATION",
  "INSTITUTIONAL_DEMOCRACY_SOCRE",     # keep literal if that's in your data
  "INSTITUTIONAL_AUTOCRACY_SCORE",
  "REGIME_DURABILITY_YEARS",
  "POLITICAL_COMPETITION_SCORE",
  "PARTIAL_DEMOCRACY_WITH_FACTIONALISM",
  "CONFLICT_INTENSITY_YEAR",
  "CONFLICT_CUMULATIVE_INTENSITY_ACROSS_YEARS",
  "N_WAR_FRONTS",
  "MAX_CONFLICT_INTENSITY",
  "AVG_CONFLICT_INTENSITY",
  "N_TOTAL_TROOPS"
)

CORE_NAMES <- c(
  "ODA_RECEIVED_PER_CAPITA_L1",
  "GDP_GROWTH_L1",
  "GDP_PER_CAPITA_L1",
  "GDP_DEFLATOR_L1",
  "ELECTORAL_DEMOCRACY_SCORE_L1",
  "LIBERAL_DEMOCRACY_SCORE_L1",
  "TERRITORIAL_FRAGMENTATION_L1",
  "INSTITUTIONAL_DEMOCRACY_SOCRE_L1",
  "INSTITUTIONAL_AUTOCRACY_SCORE_L1",
  "REGIME_DURABILITY_YEARS_L1",
  "POLITICAL_COMPETITION_SCORE_L1",
  "CONFLICT_INTENSITY_YEAR_L1",
  "CONFLICT_CUMULATIVE_INTENSITY_ACROSS_YEARS_L1",
  "N_WAR_FRONTS_L1",
  "MAX_CONFLICT_INTENSITY_L1",
  "AVG_CONFLICT_INTENSITY_L1",
  "N_TOTAL_TROOPS_L1"
)

# ---------------- Helpers ----------------
as_exit_dummy <- function(vec, k = 5L) {
  n <- length(vec); exit <- integer(n); nf <- (vec == "Non Fragile")
  for (t in seq_len(n)) if (nf[t] && t + k - 1 <= n && all(nf[t:(t + k - 1)])) { exit[t] <- 1L; break }
  exit
}

build_hazard_raw <- function(df, predictors, k_sustain) {
  df <- df %>% arrange(.data[[COUNTRY]], .data[[YEAR]])
  df <- df %>%
    group_by(.data[[COUNTRY]]) %>%
    mutate(EXIT_SUSTAINED = as_exit_dummy(.data[[STATUS]], k = k_sustain)) %>%
    ungroup() %>%
    group_by(.data[[COUNTRY]]) %>%
    mutate(
      ever_exited = any(EXIT_SUSTAINED == 1L),
      first_exit_year = dplyr::if_else(
        ever_exited, min(.data[[YEAR]][EXIT_SUSTAINED == 1L], na.rm = TRUE), NA_integer_
      ),
      at_risk = dplyr::if_else(
        .data[[STATUS]] %in% c("Fragile","Transitioning") &
          (is.na(first_exit_year) | .data[[YEAR]] < first_exit_year),
        1L, 0L
      )
    ) %>% ungroup()

  haz <- df %>% filter(at_risk == 1L | EXIT_SUSTAINED == 1L)

  predictors <- intersect(predictors, names(haz))
  if (!length(predictors)) stop("None of the listed predictors are in the data.")

  haz <- haz %>%
    group_by(.data[[COUNTRY]]) %>%
    mutate(across(all_of(predictors), ~ dplyr::lag(.x, 1L), .names = "{.col}_L1")) %>%
    ungroup()

  # clean factor lags
  refactor_if_exists <- function(df, var) {
    if (var %in% names(df)) df[[var]] <- fct_drop(factor(df[[var]], exclude = NULL))
    df
  }
  haz <- refactor_if_exists(haz, "POLITICAL_REGIME_L1")
  haz <- refactor_if_exists(haz, "PARTIAL_DEMOCRACY_WITH_FACTIONALISM_L1")
  haz
}

prune_by_coverage <- function(haz_raw, predictors, cov_min) {
  lag_cols <- paste0(intersect(predictors, names(haz_raw)), "_L1")
  lag_cols <- intersect(lag_cols, names(haz_raw))
  if (!length(lag_cols)) stop("After lagging, no candidate predictors remained.")

  coverage <- sapply(lag_cols, function(v) mean(!is.na(haz_raw[[v]])))
  keep_lag <- names(coverage)[coverage >= cov_min]
  if (!length(keep_lag)) stop(paste0("No lagged predictors meet coverage >= ", cov_min))

  # strict complete-case on kept lags
  haz <- haz_raw %>% filter(!if_any(all_of(keep_lag), is.na))

  # drop zero-variance among kept lags
  nzv <- vapply(haz[keep_lag], function(x) { ux <- unique(x[!is.na(x)]); length(ux) > 1 }, logical(1))
  list(haz = haz, keep_lag = keep_lag[nzv], coverage = sort(coverage, decreasing = TRUE))
}

fit_clustered <- function(model) {
  vc <- vcovCL(model, cluster = model$data[[COUNTRY]])
  coeftest(model, vcov = vc)
}

influence_drop <- function(glm_model, drop_frac = 0.02) {
  h <- hatvalues(glm_model)
  cutoff <- stats::quantile(h, 1 - drop_frac, na.rm = TRUE)
  which(h >= cutoff)
}

diag_metrics <- function(y, p) {
  out <- list(
    brier = mean((y - p)^2)
  )
  if (have_pROC)  out$auc_roc <- as.numeric(pROC::roc(y, p, quiet = TRUE)$auc)
  if (have_PRROC) out$auc_pr  <- PRROC::pr.curve(scores.class0 = p[y == 1],
                                                 scores.class1 = p[y == 0])$auc.integral
  out
}

# ---------------- Load & prep ----------------
stopifnot(exists("standardized_data_final"))

df <- standardized_data_final %>%
  mutate(
    !!YEAR   := suppressWarnings(as.integer(.data[[YEAR]])),
    !!STATUS := as.character(.data[[STATUS]])
  )

# sanity: warn if any named predictors missing (pre-lag)
missing_preds <- setdiff(PREDICTORS, names(df))
if (length(missing_preds)) warning("Predictors missing in data (pre-lag): ",
                                   paste(missing_preds, collapse=", "))

# factorize regime-like vars pre-lag
fac_candidates <- c("POLITICAL_REGIME","PARTIAL_DEMOCRACY_WITH_FACTIONALISM")
for (v in fac_candidates) if (v %in% names(df)) df[[v]] <- factor(df[[v]], exclude = NULL)

# standardize numeric predictors pre-lag (global z-scores)
num_vars <- setdiff(PREDICTORS, fac_candidates)
num_vars <- intersect(num_vars, names(df))
for (v in num_vars) if (is.numeric(df[[v]])) df[[v]] <- as.numeric(scale(df[[v]]))

cat("\n[Audit] Rows in original df:", nrow(df), "\n")

# ---------------- Master loop ----------------
COLLECT <- NULL
MODELS  <- list()

for (K in K_GRID) {
  cat("\n================= K =", K, "=================\n")

  haz_raw <- build_hazard_raw(df, predictors = PREDICTORS, k_sustain = K)
  cat("[Audit] Risk-set + event rows (pre-lag-filter):", nrow(haz_raw), "\n")

  # coverage audit
  lag_cols_all <- paste0(intersect(PREDICTORS, names(haz_raw)), "_L1")
  lag_cols_all <- intersect(lag_cols_all, names(haz_raw))
  lag_cov <- sapply(lag_cols_all, function(v) mean(!is.na(haz_raw[[v]])))
  cat("[Audit] Lagged non-missing (top 10):\n")
  print(sort(round(lag_cov, 3), decreasing = TRUE)[1:min(10, length(lag_cov))])

  pruned <- prune_by_coverage(haz_raw, predictors = PREDICTORS, cov_min = COVERAGE_MIN)
  haz      <- pruned$haz
  keep_lag <- pruned$keep_lag
  cat("[Pruning] Kept lagged predictors (", length(keep_lag), ")\n", sep = "")
  cat(paste(keep_lag, collapse = ", "), "\n")
  cat("[Pruning] Rows after pruning:", nrow(haz), "\n")

  # --------- Add duration dependence (spell clock) ----------
  haz <- haz %>%
    arrange(.data[[COUNTRY]], .data[[YEAR]]) %>%
    group_by(.data[[COUNTRY]]) %>%
    mutate(new_spell = (lag(!!sym(STATUS), default = first(.data[[STATUS]])) == "Non Fragile") &
                       .data[[STATUS]] %in% c("Fragile","Transitioning"),
           spell_id  = cumsum(replace_na(new_spell, FALSE))) %>%
    group_by(.data[[COUNTRY]], spell_id) %>%
    mutate(t_at_risk = row_number() - 1L) %>%
    ungroup()

  # --------- End-of-panel trim to avoid immortal-time bias ----------
  max_year <- max(haz[[YEAR]], na.rm = TRUE)
  haz <- haz %>% filter(.data[[YEAR]] <= (max_year - (K - 1L)))
  cat("[Trim] Dropped end-of-panel years to avoid immortal-time bias. Remaining rows:", nrow(haz), "\n")

  # --------- CORE selection ----------
  core <- intersect(CORE_NAMES, keep_lag)
  if ("GDP_DEFLATOR_L1" %in% keep_lag) core <- union(core, "GDP_DEFLATOR_L1")
  if (length(core) < 3L) core <- unique(c(core, head(setdiff(keep_lag, core), 5L)))
  cat("[Core] Variables used:", paste(core, collapse = ", "), "\n")

  # --------- Models ----------
  # (1) Pooled logit w/ Year FE + ns(t_at_risk,3), clustered by country
  f_core_yearfe <- as.formula(paste(
  "EXIT_SUSTAINED ~", paste(core, collapse = " + "),
  "+ splines::ns(t_at_risk, 3) + i(", YEAR, ")", sep = ""
  ))
  m_pool_core <- fixest::feglm(f_core_yearfe, data = haz, family = "logit", cluster = COUNTRY)
  cat("\n=== Pooled logit + Year FE + duration spline (clustered) — CORE ===\n")
  print(summary(m_pool_core))

  # (1b) Influence refit (drop top leverage) — GLM without year FE (transparent influence check)
  f_core <- as.formula(paste(
  "EXIT_SUSTAINED ~", paste(core, collapse = " + "),
  "+ splines::ns(t_at_risk, 3)"
  ))
  m_glm_core <- glm(f_core, data = haz, family = binomial("logit"))
  idx_hi <- influence_drop(m_glm_core, drop_frac = DROP_TOP_LEVERAGE)
  if (length(idx_hi)) {
    haz_rb <- haz[-idx_hi, , drop = FALSE]
    m_pool_core_rb <- fixest::feglm(f_core_yearfe, data = haz_rb, family = "logit", cluster = COUNTRY)
    cat("\n--- Refit after dropping top", round(100*DROP_TOP_LEVERAGE,1), "% leverage (n drop =",
        length(idx_hi), ") ---\n")
    print(summary(m_pool_core_rb))
  }

  # (2) Rare-events (bias-reduced) + clustered SEs — CORE
  m_rare_core <- glm(f_core, data = haz, family = binomial("logit"),
                     method = "brglmFit", control = brglmControl(type = "AS_mixed"))
  cat("\n=== Rare-events (bias-reduced) — CORE ===\n")
  print(summary(m_rare_core))
  cat("\n--- Rare-events + clustered SEs (country) ---\n")
  print(fit_clustered(m_rare_core))

  # collect coef + clustered SE for rare-events model
  co  <- coef(m_rare_core)
  V   <- vcovCL(m_rare_core, cluster = m_rare_core$data[[COUNTRY]])
  se  <- sqrt(diag(V))
  tmp <- data.frame(K = K, term = names(co), beta = unname(co), se = unname(se), row.names = NULL)
  COLLECT <- bind_rows(COLLECT, tmp)

  # (3) Random-intercept (country frailty) — CORE
  cat("\n=== Random-intercept (country) — CORE ===\n")
  m_re_core <- try(
    glmmTMB(f_core, data = haz, family = binomial(),
            control = glmmTMBControl(
              optimizer = optim, optArgs = list(method = "BFGS"),
              optCtrl = list(maxit = 1e4)
            )),
    silent = TRUE
  )
  if (inherits(m_re_core, "try-error")) {
    cat("glmmTMB failed to converge (sparse categories / quasi-separation likely).\n")
  } else {
    print(summary(m_re_core))
  }

  # (4) Conditional FE logit (country FE absorbed) — robustness
  cat("\n=== Conditional FE logit (absorbing country + year) — CORE ===\n")
  m_condfe <- try(
    fixest::feglm(
      as.formula(paste(
        "EXIT_SUSTAINED ~", paste(core, collapse=" + "),
        "+ ns(t_at_risk,3) |", COUNTRY, "+", YEAR)),
      data = haz, family = "logit", cluster = COUNTRY
    ),
    silent = TRUE
  )
  if (inherits(m_condfe, "try-error")) {
    cat("Conditional FE logit failed (possibly all-country separation in some years).\n")
  } else {
    print(summary(m_condfe))
  }

  # (5) AMEs (from plain GLM without year FE; AMEs are about X, not time dummies)
  cat("\n=== Average Marginal Effects — CORE GLM ===\n")
  ame <- margins::margins(m_glm_core, data = haz)  # <-- add data=
  print(summary(ame))

  # (6) Predictive diagnostics
  phat <- predict(m_glm_core, type = "response")
  y    <- haz$EXIT_SUSTAINED
  dm   <- diag_metrics(y, phat)
  cat("\n=== Diagnostics (GLM w/ duration) ===\n")
  print(dm)

  # store models
  MODELS[[paste0("K", K, "_poolFE")]]  <- m_pool_core
  MODELS[[paste0("K", K, "_rare")]]    <- m_rare_core
  MODELS[[paste0("K", K, "_glm")]]     <- m_glm_core
  MODELS[[paste0("K", K, "_frailty")]] <- m_re_core
  MODELS[[paste0("K", K, "_condFE")]]  <- m_condfe

  cat("\n================= End K =", K, "=================\n")
}

cat("\n[Done] Coefficient collection available in `COLLECT`; models in `MODELS`.\n")

# Example: tidy coef table across K for a few key variables
if (exists("COLLECT")) {
  key_terms <- c("(Intercept)", CORE_NAMES[CORE_NAMES %in% COLLECT$term], "ns(t_at_risk, 3)1", "ns(t_at_risk, 3)2", "ns(t_at_risk, 3)3")
  print(
    COLLECT %>%
      filter(term %in% key_terms) %>%
      mutate(z = beta / se, p = 2*pnorm(abs(z), lower.tail = FALSE)) %>%
      arrange(term, K)
  )
}

```



```{r, hazard, echo = FALSE, message = FALSE, warning = FALSE, results = 'asis'}

stopifnot(exists("COLLECT"))

# Keep only the CORE variables you plotted in the models
core_terms <- c(
  "LIBERAL_DEMOCRACY_SCORE_L1",
  "REGIME_DURABILITY_YEARS_L1",
  "CONFLICT_INTENSITY_YEAR_L1",
  "AVG_CONFLICT_INTENSITY_L1",
  "GDP_GROWTH_L1",
  "GDP_DEFLATOR_L1"
)

# Nice labels for rows
var_labs <- c(
  LIBERAL_DEMOCRACY_SCORE_L1   = "Liberal democracy (t-1)",
  REGIME_DURABILITY_YEARS_L1   = "Regime durability, years (t-1)",
  CONFLICT_INTENSITY_YEAR_L1   = "Conflict intensity (acute, t-1)",
  AVG_CONFLICT_INTENSITY_L1    = "Conflict intensity (chronic avg., t-1)",
  GDP_GROWTH_L1                = "Real GDP per capita growth (t-1)",
  GDP_DEFLATOR_L1              = "GDP deflator / price tailwind (t-1)"
)

# Compute ORs and 95% clustered CIs
tab <- COLLECT %>%
  filter(term %in% core_terms) %>%
  mutate(
    OR  = exp(beta),
    LCL = exp(beta - 1.96 * se),
    UCL = exp(beta + 1.96 * se),
    cell = sprintf("%.2f [%.2f, %.2f]", OR, LCL, UCL),
    term_label = var_labs[term]
  ) %>%
  select(term_label, K, cell)

# If a core variable was dropped for some K (coverage/collinearity), mark as blank
all_rows <- tibble(term_label = unname(var_labs))
all_cols <- tibble(K = sort(unique(tab$K)))
tab_full <- tidyr::complete(tab, term_label = all_rows$term_label, K = all_cols$K, fill = list(cell = ""))

# Wide layout: one column per K
tab_wide <- tab_full %>%
  tidyr::pivot_wider(names_from = K, values_from = cell, names_prefix = "K = ")

# Order rows like CORE
tab_wide <- tab_wide %>%
  mutate(row_order = match(term_label, unname(var_labs))) %>%
  arrange(row_order) %>%
  select(-row_order)

# --- 3) Render LaTeX (booktabs) and write to file ---

library(kableExtra)

latex_tbl <- kable(
  tab_wide,
  format = "latex",
  booktabs = TRUE,
  linesep = "",
  caption = "Sustained Exit from Fragility: Odds Ratios with 95\\% Clustered CIs (CORE rare-events logit, clustered by country)",
  col.names = c("Predictors", "K = 3", "K = 5", "K = 7"),
  escape = FALSE,
  align = c("l","c","c","c")
) %>%
  kable_styling(latex_options = c("hold_position","striped")) %>%
  add_header_above(c(" " = 1, "Sustain horizon (years)" = 3)) %>%
  footnote(
    general = "Entries are odds ratios with 95\\% clustered confidence intervals. Models are bias-reduced (brglm2, AS_mixed). Covariates standardized before lagging. Outcome: first sustained exit (Non-Fragile) at t persisting for K years. Sample includes risk set (Fragile/Transitioning) and the first exit year.",
    threeparttable = TRUE, escape = FALSE
  )

cat(latex_tbl)

```

## Final Table

```{r, echo = FALSE, message = FALSE}
# ============================================================
# OR table (ALL predictors) from COLLECT — vectorized + robust
# ============================================================
stopifnot(exists("COLLECT"))

suppressPackageStartupMessages({
  library(dplyr); library(tidyr); library(kableExtra); library(stringr)
})

# 1) Terms to include: drop intercept, spline basis, and year FE artifacts
omit_patterns <- c("^\\(Intercept\\)$", "splines::ns\\(", "^ns\\(", "^factor\\(", "^i\\(")
omit_re <- paste(omit_patterns, collapse="|")

terms_all <- COLLECT %>%
  mutate(term = as.character(term)) %>%
  filter(!str_detect(term, omit_re))

# 2) Pretty labels (override where you want; others auto-format)
var_labs_override <- c(
  ODA_RECEIVED_PER_CAPITA_L1 = "ODA per capita (t-1)",
  GDP_GROWTH_L1              = "Real GDP per capita growth (t-1)",
  GDP_PER_CAPITA_L1          = "GDP per capita (t-1)",
  GDP_DEFLATOR_L1            = "GDP deflator / price tailwind (t-1)",
  POLITICAL_REGIME_L1        = "Political regime (t-1)",
  ELECTORAL_DEMOCRACY_SCORE_L1 = "Electoral democracy (t-1)",
  LIBERAL_DEMOCRACY_SCORE_L1   = "Liberal democracy (t-1)",
  TERRITORIAL_FRAGMENTATION_L1 = "Territorial fragmentation (t-1)",
  INSTITUTIONAL_DEMOCRACY_SOCRE_L1 = "Institutional democracy (t-1)",
  INSTITUTIONAL_AUTOCRACY_SCORE_L1 = "Institutional autocracy (t-1)",
  REGIME_DURABILITY_YEARS_L1      = "Regime durability, years (t-1)",
  POLITICAL_COMPETITION_SCORE_L1  = "Political competition (t-1)",
  PARTIAL_DEMOCRACY_WITH_FACTIONALISM_L1 = "Partial democracy w/ factionalism (t-1)",
  CONFLICT_INTENSITY_YEAR_L1      = "Conflict intensity (acute, t-1)",
  CONFLICT_CUMULATIVE_INTENSITY_ACROSS_YEARS_L1 = "Cumulative conflict intensity (t-1)",
  N_WAR_FRONTS_L1                 = "# war fronts (t-1)",
  MAX_CONFLICT_INTENSITY_L1       = "Max conflict intensity (t-1)",
  AVG_CONFLICT_INTENSITY_L1       = "Conflict intensity (chronic avg., t-1)",
  N_TOTAL_TROOPS_L1               = "Total troops (t-1)"
)

auto_label <- function(x) {
  x %>%
    str_replace("_L1$", " (t-1)") %>%
    str_replace_all("_", " ") %>%
    str_to_sentence()
}

# Vectorized labeler: named lookup with [v], fill NAs with auto labels
make_label <- function(v) {
  v <- as.character(v)
  lbl <- unname(var_labs_override[v])      # returns named vector with NAs for misses
  miss <- is.na(lbl)
  lbl[miss] <- auto_label(v[miss])
  lbl
}

# 3) Format OR [LCL, UCL], bold if CI excludes 1
fmt_or_ci <- function(b, s) {
  OR  <- exp(b); LCL <- exp(b - 1.96*s); UCL <- exp(b + 1.96*s)
  cell <- sprintf("%.2f [%.2f, %.2f]", OR, LCL, UCL)
  if (!is.na(LCL) && !is.na(UCL) && (UCL < 1 || LCL > 1)) cell <- paste0("\\textbf{", cell, "}")
  cell
}

# 4) Build long table (K x term)
tab_long <- terms_all %>%
  mutate(
    term_label = make_label(term),
    cell = mapply(fmt_or_ci, beta, se)
  ) %>%
  select(term_label, K, cell)

# Ensure K columns exist in order 3/5/7 and blanks where missing
all_rows <- tibble(term_label = sort(unique(tab_long$term_label)))
all_cols <- tibble(K = c(3L, 5L, 7L))
tab_full <- tidyr::complete(tab_long, term_label = all_rows$term_label, K = all_cols$K, fill = list(cell = ""))

# 5) Optional buckets for section headers
bucket_of <- function(lbl) {
  lbl <- as.character(lbl)
  out <- rep("Other", length(lbl))
  out[stringr::str_detect(lbl, stringr::regex("conflict|war|troop", ignore_case = TRUE))] <- "Conflict & security"
  out[stringr::str_detect(lbl, stringr::regex("gdp|price|oda|per capita|growth|deflator", ignore_case = TRUE))] <- "Economy"
  out[stringr::str_detect(lbl, stringr::regex("democ|autoc|polity|regime|competition|recruitment|fragment", ignore_case = TRUE))] <- "Institutions & politics"
  factor(out, levels = c("Economy","Institutions & politics","Conflict & security","Other"))
}

tab_wide <- tab_full %>%
  dplyr::mutate(
    K = factor(K, levels = c(3,5,7), labels = c("K = 3","K = 5","K = 7")),
    bucket = bucket_of(term_label)
  ) %>%
  tidyr::pivot_wider(names_from = K, values_from = cell) %>%
  dplyr::arrange(bucket, term_label)

# 6) Render LaTeX with grouped sections (robust start/end calc)
kb <- kable(
  tab_wide %>% select(term_label, `K = 3`, `K = 5`, `K = 7`),
  format = "latex",
  booktabs = TRUE,
  linesep = "",
  caption = "Sustained Exit from Fragility: Odds Ratios with 95\\% Clustered CIs (ALL predictors, rare-events logit)",
  col.names = c("Predictors", "K = 3", "K = 5", "K = 7"),
  escape = FALSE,
  align = c("l","c","c","c"),
  label = "tab:all_or"
) %>%
  kable_styling(latex_options = c("hold_position","striped","scale_down"),
                font_size = 8) %>%                                 # smaller font + auto-resize
  column_spec(1, width = "4.5cm") %>%                              # wrap predictor names
  column_spec(2, width = "3.2cm") %>%                              # narrow K columns
  column_spec(3, width = "3.2cm") %>%
  column_spec(4, width = "3.2cm") %>%
  add_header_above(c(" " = 1, "Sustain horizon (years)" = 3))

# Add group headers without rle()
bucket_levels <- c("Economy","Institutions & politics","Conflict & security","Other")
# make sure ordering is locked
tab_wide$bucket <- factor(tab_wide$bucket, levels = bucket_levels)

idx_by_bucket <- split(seq_len(nrow(tab_wide)), tab_wide$bucket)

for (lev in bucket_levels) {
  idx <- idx_by_bucket[[lev]]
  if (!is.null(idx) && length(idx)) {
    kb <- pack_rows(kb, lev, start = min(idx), end = max(idx), indent = FALSE)
  }
}

kb <- footnote(
  kb,
  general = "Entries are odds ratios with 95% clustered confidence intervals. Models are bias-reduced (brglm2, ASmixed). Covariates standardized before lagging. Spline of time-at-risk included; estimates correspond to the rare-events specification collected in COLLECT (no year FE). For the year-FE version, rebuild with COLLECTYFE and rerun. Outcome: first sustained exit (Non-Fragile) at t persisting for K years; sample is the risk set plus first exit year. We exclude Executive Recruitment and Polity Score from the core specification because their coefficients are not well identified: Executive Recruitment exhibits complete or quasi-separation in the risk set, yielding boundary estimates and uninformative confidence intervals, while Polity Score is highly collinear with our V-Dem democracy measures. Both variables are reported in Appendix Table A.x for completeness.",
  threeparttable = TRUE, escape = FALSE
)

cat(kb)

```

## Two Year Lags

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# ---------------- Settings ----------------
COUNTRY <- "COUNTRY_NAME"
YEAR    <- "YEAR"
STATUS  <- "VDEM_STATUS_IDEAL"  # "Fragile","Transitioning","Non Fragile"

COVERAGE_MIN       <- 0.70
K_GRID             <- c(3L, 5L, 7L)
DROP_TOP_LEVERAGE  <- 0.02

PREDICTORS <- c(
  "ODA_RECEIVED_PER_CAPITA",
  "GDP_GROWTH",
  "GDP_PER_CAPITA",
  "GDP_DEFLATOR",
  "POLITICAL_REGIME",
  "ELECTORAL_DEMOCRACY_SCORE",
  "LIBERAL_DEMOCRACY_SCORE",
  "TERRITORIAL_FRAGMENTATION",
  "INSTITUTIONAL_DEMOCRACY_SOCRE",     # keep literal if that's in your data
  "INSTITUTIONAL_AUTOCRACY_SCORE",
  "REGIME_DURABILITY_YEARS",
  "POLITICAL_COMPETITION_SCORE",
  "PARTIAL_DEMOCRACY_WITH_FACTIONALISM",
  "CONFLICT_INTENSITY_YEAR",
  "CONFLICT_CUMULATIVE_INTENSITY_ACROSS_YEARS",
  "N_WAR_FRONTS",
  "MAX_CONFLICT_INTENSITY",
  "AVG_CONFLICT_INTENSITY",
  "N_TOTAL_TROOPS"
)

CORE_NAMES <- c(
  "ODA_RECEIVED_PER_CAPITA_L1",
  "GDP_GROWTH_L1",
  "GDP_PER_CAPITA_L1",
  "GDP_DEFLATOR_L1",
  "ELECTORAL_DEMOCRACY_SCORE_L1",
  "LIBERAL_DEMOCRACY_SCORE_L1",
  "TERRITORIAL_FRAGMENTATION_L1",
  "INSTITUTIONAL_DEMOCRACY_SOCRE_L1",
  "INSTITUTIONAL_AUTOCRACY_SCORE_L1",
  "REGIME_DURABILITY_YEARS_L1",
  "POLITICAL_COMPETITION_SCORE_L1",
  "CONFLICT_INTENSITY_YEAR_L1",
  "CONFLICT_CUMULATIVE_INTENSITY_ACROSS_YEARS_L1",
  "N_WAR_FRONTS_L1",
  "MAX_CONFLICT_INTENSITY_L1",
  "AVG_CONFLICT_INTENSITY_L1",
  "N_TOTAL_TROOPS_L1"
)

# ---------------- Helpers ----------------
as_exit_dummy <- function(vec, k = 5L) {
  n <- length(vec); exit <- integer(n); nf <- (vec == "Non Fragile")
  for (t in seq_len(n)) if (nf[t] && t + k - 1 <= n && all(nf[t:(t + k - 1)])) { exit[t] <- 1L; break }
  exit
}

build_hazard_raw <- function(df, predictors, k_sustain) {
  df <- df %>% arrange(.data[[COUNTRY]], .data[[YEAR]])
  df <- df %>%
    group_by(.data[[COUNTRY]]) %>%
    mutate(EXIT_SUSTAINED = as_exit_dummy(.data[[STATUS]], k = k_sustain)) %>%
    ungroup() %>%
    group_by(.data[[COUNTRY]]) %>%
    mutate(
      ever_exited = any(EXIT_SUSTAINED == 1L),
      first_exit_year = dplyr::if_else(
        ever_exited, min(.data[[YEAR]][EXIT_SUSTAINED == 1L], na.rm = TRUE), NA_integer_
      ),
      at_risk = dplyr::if_else(
        .data[[STATUS]] %in% c("Fragile","Transitioning") &
          (is.na(first_exit_year) | .data[[YEAR]] < first_exit_year),
        1L, 0L
      )
    ) %>% ungroup()

  haz <- df %>% filter(at_risk == 1L | EXIT_SUSTAINED == 1L)

  predictors <- intersect(predictors, names(haz))
  if (!length(predictors)) stop("None of the listed predictors are in the data.")

  haz <- haz %>%
    group_by(.data[[COUNTRY]]) %>%
    mutate(across(all_of(predictors), ~ dplyr::lag(.x, 2L), .names = "{.col}_L1")) %>%
    ungroup()

  # clean factor lags
  refactor_if_exists <- function(df, var) {
    if (var %in% names(df)) df[[var]] <- fct_drop(factor(df[[var]], exclude = NULL))
    df
  }
  haz <- refactor_if_exists(haz, "POLITICAL_REGIME_L1")
  haz <- refactor_if_exists(haz, "PARTIAL_DEMOCRACY_WITH_FACTIONALISM_L1")
  haz
}

prune_by_coverage <- function(haz_raw, predictors, cov_min) {
  lag_cols <- paste0(intersect(predictors, names(haz_raw)), "_L1")
  lag_cols <- intersect(lag_cols, names(haz_raw))
  if (!length(lag_cols)) stop("After lagging, no candidate predictors remained.")

  coverage <- sapply(lag_cols, function(v) mean(!is.na(haz_raw[[v]])))
  keep_lag <- names(coverage)[coverage >= cov_min]
  if (!length(keep_lag)) stop(paste0("No lagged predictors meet coverage >= ", cov_min))

  # strict complete-case on kept lags
  haz <- haz_raw %>% filter(!if_any(all_of(keep_lag), is.na))

  # drop zero-variance among kept lags
  nzv <- vapply(haz[keep_lag], function(x) { ux <- unique(x[!is.na(x)]); length(ux) > 1 }, logical(1))
  list(haz = haz, keep_lag = keep_lag[nzv], coverage = sort(coverage, decreasing = TRUE))
}

fit_clustered <- function(model) {
  vc <- vcovCL(model, cluster = model$data[[COUNTRY]])
  coeftest(model, vcov = vc)
}

influence_drop <- function(glm_model, drop_frac = 0.02) {
  h <- hatvalues(glm_model)
  cutoff <- stats::quantile(h, 1 - drop_frac, na.rm = TRUE)
  which(h >= cutoff)
}

diag_metrics <- function(y, p) {
  out <- list(
    brier = mean((y - p)^2)
  )
  if (have_pROC)  out$auc_roc <- as.numeric(pROC::roc(y, p, quiet = TRUE)$auc)
  if (have_PRROC) out$auc_pr  <- PRROC::pr.curve(scores.class0 = p[y == 1],
                                                 scores.class1 = p[y == 0])$auc.integral
  out
}

# ---------------- Load & prep ----------------
stopifnot(exists("standardized_data_final"))

df <- standardized_data_final %>%
  mutate(
    !!YEAR   := suppressWarnings(as.integer(.data[[YEAR]])),
    !!STATUS := as.character(.data[[STATUS]])
  )

# sanity: warn if any named predictors missing (pre-lag)
missing_preds <- setdiff(PREDICTORS, names(df))
if (length(missing_preds)) warning("Predictors missing in data (pre-lag): ",
                                   paste(missing_preds, collapse=", "))

# factorize regime-like vars pre-lag
fac_candidates <- c("POLITICAL_REGIME","PARTIAL_DEMOCRACY_WITH_FACTIONALISM")
for (v in fac_candidates) if (v %in% names(df)) df[[v]] <- factor(df[[v]], exclude = NULL)

# standardize numeric predictors pre-lag (global z-scores)
num_vars <- setdiff(PREDICTORS, fac_candidates)
num_vars <- intersect(num_vars, names(df))
for (v in num_vars) if (is.numeric(df[[v]])) df[[v]] <- as.numeric(scale(df[[v]]))

cat("\n[Audit] Rows in original df:", nrow(df), "\n")

# ---------------- Master loop ----------------
COLLECT_two_year <- NULL
MODELS  <- list()

for (K in K_GRID) {
  cat("\n================= K =", K, "=================\n")

  haz_raw <- build_hazard_raw(df, predictors = PREDICTORS, k_sustain = K)
  cat("[Audit] Risk-set + event rows (pre-lag-filter):", nrow(haz_raw), "\n")

  # coverage audit
  lag_cols_all <- paste0(intersect(PREDICTORS, names(haz_raw)), "_L1")
  lag_cols_all <- intersect(lag_cols_all, names(haz_raw))
  lag_cov <- sapply(lag_cols_all, function(v) mean(!is.na(haz_raw[[v]])))
  cat("[Audit] Lagged non-missing (top 10):\n")
  print(sort(round(lag_cov, 3), decreasing = TRUE)[1:min(10, length(lag_cov))])

  pruned <- prune_by_coverage(haz_raw, predictors = PREDICTORS, cov_min = COVERAGE_MIN)
  haz      <- pruned$haz
  keep_lag <- pruned$keep_lag
  cat("[Pruning] Kept lagged predictors (", length(keep_lag), ")\n", sep = "")
  cat(paste(keep_lag, collapse = ", "), "\n")
  cat("[Pruning] Rows after pruning:", nrow(haz), "\n")

  # --------- Add duration dependence (spell clock) ----------
  haz <- haz %>%
    arrange(.data[[COUNTRY]], .data[[YEAR]]) %>%
    group_by(.data[[COUNTRY]]) %>%
    mutate(new_spell = (lag(!!sym(STATUS), default = first(.data[[STATUS]])) == "Non Fragile") &
                       .data[[STATUS]] %in% c("Fragile","Transitioning"),
           spell_id  = cumsum(replace_na(new_spell, FALSE))) %>%
    group_by(.data[[COUNTRY]], spell_id) %>%
    mutate(t_at_risk = row_number() - 1L) %>%
    ungroup()

  # --------- End-of-panel trim to avoid immortal-time bias ----------
  max_year <- max(haz[[YEAR]], na.rm = TRUE)
  haz <- haz %>% filter(.data[[YEAR]] <= (max_year - (K - 1L)))
  cat("[Trim] Dropped end-of-panel years to avoid immortal-time bias. Remaining rows:", nrow(haz), "\n")

  # --------- CORE selection ----------
  core <- intersect(CORE_NAMES, keep_lag)
  if ("GDP_DEFLATOR_L1" %in% keep_lag) core <- union(core, "GDP_DEFLATOR_L1")
  if (length(core) < 3L) core <- unique(c(core, head(setdiff(keep_lag, core), 5L)))
  cat("[Core] Variables used:", paste(core, collapse = ", "), "\n")

  # --------- Models ----------
  # (1) Pooled logit w/ Year FE + ns(t_at_risk,3), clustered by country
  f_core_yearfe <- as.formula(paste(
  "EXIT_SUSTAINED ~", paste(core, collapse = " + "),
  "+ splines::ns(t_at_risk, 3) + i(", YEAR, ")", sep = ""
  ))
  m_pool_core <- fixest::feglm(f_core_yearfe, data = haz, family = "logit", cluster = COUNTRY)
  cat("\n=== Pooled logit + Year FE + duration spline (clustered) — CORE ===\n")
  print(summary(m_pool_core))

  # (1b) Influence refit (drop top leverage) — GLM without year FE (transparent influence check)
  f_core <- as.formula(paste(
  "EXIT_SUSTAINED ~", paste(core, collapse = " + "),
  "+ splines::ns(t_at_risk, 3)"
  ))
  m_glm_core <- glm(f_core, data = haz, family = binomial("logit"))
  idx_hi <- influence_drop(m_glm_core, drop_frac = DROP_TOP_LEVERAGE)
  if (length(idx_hi)) {
    haz_rb <- haz[-idx_hi, , drop = FALSE]
    m_pool_core_rb <- fixest::feglm(f_core_yearfe, data = haz_rb, family = "logit", cluster = COUNTRY)
    cat("\n--- Refit after dropping top", round(100*DROP_TOP_LEVERAGE,1), "% leverage (n drop =",
        length(idx_hi), ") ---\n")
    print(summary(m_pool_core_rb))
  }

  # (2) Rare-events (bias-reduced) + clustered SEs — CORE
  m_rare_core <- glm(f_core, data = haz, family = binomial("logit"),
                     method = "brglmFit", control = brglmControl(type = "AS_mixed"))
  cat("\n=== Rare-events (bias-reduced) — CORE ===\n")
  print(summary(m_rare_core))
  cat("\n--- Rare-events + clustered SEs (country) ---\n")
  print(fit_clustered(m_rare_core))

  # collect coef + clustered SE for rare-events model
  co  <- coef(m_rare_core)
  V   <- vcovCL(m_rare_core, cluster = m_rare_core$data[[COUNTRY]])
  se  <- sqrt(diag(V))
  tmp <- data.frame(K = K, term = names(co), beta = unname(co), se = unname(se), row.names = NULL)
  COLLECT_two_year <- bind_rows(COLLECT_two_year, tmp)

  # (3) Random-intercept (country frailty) — CORE
  cat("\n=== Random-intercept (country) — CORE ===\n")
  m_re_core <- try(
    glmmTMB(f_core, data = haz, family = binomial(),
            control = glmmTMBControl(
              optimizer = optim, optArgs = list(method = "BFGS"),
              optCtrl = list(maxit = 1e4)
            )),
    silent = TRUE
  )
  if (inherits(m_re_core, "try-error")) {
    cat("glmmTMB failed to converge (sparse categories / quasi-separation likely).\n")
  } else {
    print(summary(m_re_core))
  }

  # (4) Conditional FE logit (country FE absorbed) — robustness
  cat("\n=== Conditional FE logit (absorbing country + year) — CORE ===\n")
  m_condfe <- try(
    fixest::feglm(
      as.formula(paste(
        "EXIT_SUSTAINED ~", paste(core, collapse=" + "),
        "+ ns(t_at_risk,3) |", COUNTRY, "+", YEAR)),
      data = haz, family = "logit", cluster = COUNTRY
    ),
    silent = TRUE
  )
  if (inherits(m_condfe, "try-error")) {
    cat("Conditional FE logit failed (possibly all-country separation in some years).\n")
  } else {
    print(summary(m_condfe))
  }

  # (5) AMEs (from plain GLM without year FE; AMEs are about X, not time dummies)
  cat("\n=== Average Marginal Effects — CORE GLM ===\n")
  ame <- margins::margins(m_glm_core, data = haz)  # <-- add data=
  print(summary(ame))

  # (6) Predictive diagnostics
  phat <- predict(m_glm_core, type = "response")
  y    <- haz$EXIT_SUSTAINED
  dm   <- diag_metrics(y, phat)
  cat("\n=== Diagnostics (GLM w/ duration) ===\n")
  print(dm)

  # store models
  MODELS[[paste0("K", K, "_poolFE")]]  <- m_pool_core
  MODELS[[paste0("K", K, "_rare")]]    <- m_rare_core
  MODELS[[paste0("K", K, "_glm")]]     <- m_glm_core
  MODELS[[paste0("K", K, "_frailty")]] <- m_re_core
  MODELS[[paste0("K", K, "_condFE")]]  <- m_condfe

  cat("\n================= End K =", K, "=================\n")
}

cat("\n[Done] Coefficient collection available in `COLLECT`; models in `MODELS`.\n")

# Example: tidy coef table across K for a few key variables
if (exists("COLLECT_two_year")) {
  key_terms <- c("(Intercept)", CORE_NAMES[CORE_NAMES %in% COLLECT_two_year$term], "ns(t_at_risk, 3)1", "ns(t_at_risk, 3)2", "ns(t_at_risk, 3)3")
  print(
    COLLECT_two_year %>%
      filter(term %in% key_terms) %>%
      mutate(z = beta / se, p = 2*pnorm(abs(z), lower.tail = FALSE)) %>%
      arrange(term, K)
  )
}

# ------------------------------------------------------------------------------
# LATEX Table ------------------------------------------------------------------
# ------------------------------------------------------------------------------

stopifnot(exists("COLLECT_two_year"))

suppressPackageStartupMessages({
  library(dplyr); library(tidyr); library(kableExtra); library(stringr)
})

# 1) Terms to include: drop intercept, spline basis, and year FE artifacts
omit_patterns <- c("^\\(Intercept\\)$", "splines::ns\\(", "^ns\\(", "^factor\\(", "^i\\(")
omit_re <- paste(omit_patterns, collapse="|")

terms_all <- COLLECT_two_year %>%
  mutate(term = as.character(term)) %>%
  filter(!str_detect(term, omit_re))

# 2) Pretty labels (override where you want; others auto-format)
var_labs_override <- c(
  ODA_RECEIVED_PER_CAPITA_L1 = "ODA per capita (t-2)",
  GDP_GROWTH_L1              = "Real GDP per capita growth (t-2)",
  GDP_PER_CAPITA_L1          = "GDP per capita (t-2)",
  GDP_DEFLATOR_L1            = "GDP deflator / price tailwind (t-2)",
  POLITICAL_REGIME_L1        = "Political regime (t-2)",
  ELECTORAL_DEMOCRACY_SCORE_L1 = "Electoral democracy (t-2)",
  LIBERAL_DEMOCRACY_SCORE_L1   = "Liberal democracy (t-2)",
  TERRITORIAL_FRAGMENTATION_L1 = "Territorial fragmentation (t-2)",
  INSTITUTIONAL_DEMOCRACY_SOCRE_L1 = "Institutional democracy (t-2)",
  INSTITUTIONAL_AUTOCRACY_SCORE_L1 = "Institutional autocracy (t-2)",
  REGIME_DURABILITY_YEARS_L1      = "Regime durability, years (t-2)",
  POLITICAL_COMPETITION_SCORE_L1  = "Political competition (t-2)",
  PARTIAL_DEMOCRACY_WITH_FACTIONALISM_L1 = "Partial democracy w/ factionalism (t-2)",
  CONFLICT_INTENSITY_YEAR_L1      = "Conflict intensity (acute, t-2)",
  CONFLICT_CUMULATIVE_INTENSITY_ACROSS_YEARS_L1 = "Cumulative conflict intensity (t-2)",
  N_WAR_FRONTS_L1                 = "# war fronts (t-2)",
  MAX_CONFLICT_INTENSITY_L1       = "Max conflict intensity (t-2)",
  AVG_CONFLICT_INTENSITY_L1       = "Conflict intensity (chronic avg., t-2)",
  N_TOTAL_TROOPS_L1               = "Total troops (t-2)"
)

auto_label <- function(x) {
  x %>%
    str_replace("_L1$", " (t-1)") %>%
    str_replace_all("_", " ") %>%
    str_to_sentence()
}

# Vectorized labeler: named lookup with [v], fill NAs with auto labels
make_label <- function(v) {
  v <- as.character(v)
  lbl <- unname(var_labs_override[v])      # returns named vector with NAs for misses
  miss <- is.na(lbl)
  lbl[miss] <- auto_label(v[miss])
  lbl
}

# 3) Format OR [LCL, UCL], bold if CI excludes 1
fmt_or_ci <- function(b, s) {
  OR  <- exp(b); LCL <- exp(b - 1.96*s); UCL <- exp(b + 1.96*s)
  cell <- sprintf("%.2f [%.2f, %.2f]", OR, LCL, UCL)
  if (!is.na(LCL) && !is.na(UCL) && (UCL < 1 || LCL > 1)) cell <- paste0("\\textbf{", cell, "}")
  cell
}

# 4) Build long table (K x term)
tab_long <- terms_all %>%
  mutate(
    term_label = make_label(term),
    cell = mapply(fmt_or_ci, beta, se)
  ) %>%
  select(term_label, K, cell)

# Ensure K columns exist in order 3/5/7 and blanks where missing
all_rows <- tibble(term_label = sort(unique(tab_long$term_label)))
all_cols <- tibble(K = c(3L, 5L, 7L))
tab_full <- tidyr::complete(tab_long, term_label = all_rows$term_label, K = all_cols$K, fill = list(cell = ""))

# 5) Optional buckets for section headers
bucket_of <- function(lbl) {
  lbl <- as.character(lbl)
  out <- rep("Other", length(lbl))
  out[stringr::str_detect(lbl, stringr::regex("conflict|war|troop", ignore_case = TRUE))] <- "Conflict & security"
  out[stringr::str_detect(lbl, stringr::regex("gdp|price|oda|per capita|growth|deflator", ignore_case = TRUE))] <- "Economy"
  out[stringr::str_detect(lbl, stringr::regex("democ|autoc|polity|regime|competition|recruitment|fragment", ignore_case = TRUE))] <- "Institutions & politics"
  factor(out, levels = c("Economy","Institutions & politics","Conflict & security","Other"))
}

tab_wide <- tab_full %>%
  dplyr::mutate(
    K = factor(K, levels = c(3,5,7), labels = c("K = 3","K = 5","K = 7")),
    bucket = bucket_of(term_label)
  ) %>%
  tidyr::pivot_wider(names_from = K, values_from = cell) %>%
  dplyr::arrange(bucket, term_label)

# 6) Render LaTeX with grouped sections (robust start/end calc)
kb <- kable(
  tab_wide %>% select(term_label, `K = 3`, `K = 5`, `K = 7`),
  format = "latex",
  booktabs = TRUE,
  linesep = "",
  caption = "Sustained Exit from Fragility: Odds Ratios with 95\\% Clustered CIs (ALL predictors, rare-events logit)",
  col.names = c("Predictors", "K = 3", "K = 5", "K = 7"),
  escape = FALSE,
  align = c("l","c","c","c"),
  label = "tab:all_or"
) %>%
  kable_styling(latex_options = c("hold_position","striped","scale_down"),
                font_size = 8) %>%                                 # smaller font + auto-resize
  column_spec(1, width = "4.5cm") %>%                              # wrap predictor names
  column_spec(2, width = "3.2cm") %>%                              # narrow K columns
  column_spec(3, width = "3.2cm") %>%
  column_spec(4, width = "3.2cm") %>%
  add_header_above(c(" " = 1, "Sustain horizon (years)" = 3))

# Add group headers without rle()
bucket_levels <- c("Economy","Institutions & politics","Conflict & security","Other")
# make sure ordering is locked
tab_wide$bucket <- factor(tab_wide$bucket, levels = bucket_levels)

idx_by_bucket <- split(seq_len(nrow(tab_wide)), tab_wide$bucket)

for (lev in bucket_levels) {
  idx <- idx_by_bucket[[lev]]
  if (!is.null(idx) && length(idx)) {
    kb <- pack_rows(kb, lev, start = min(idx), end = max(idx), indent = FALSE)
  }
}

kb <- footnote(
  kb,
  general = "Entries are odds ratios with 95% clustered confidence intervals. Models are bias-reduced (brglm2, ASmixed). Covariates standardized before lagging. Spline of time-at-risk included; estimates correspond to the rare-events specification collected in COLLECT (no year FE). For the year-FE version, rebuild with COLLECTYFE and rerun. Outcome: first sustained exit (Non-Fragile) at t persisting for K years; sample is the risk set plus first exit year. We exclude Executive Recruitment and Polity Score from the core specification because their coefficients are not well identified: Executive Recruitment exhibits complete or quasi-separation in the risk set, yielding boundary estimates and uninformative confidence intervals, while Polity Score is highly collinear with our V-Dem democracy measures. Both variables are reported in Appendix Table A.x for completeness.",
  threeparttable = TRUE, escape = FALSE
)

cat(kb)
```

## Three-Year Lags

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# ---------------- Settings ----------------
COUNTRY <- "COUNTRY_NAME"
YEAR    <- "YEAR"
STATUS  <- "VDEM_STATUS_IDEAL"  # "Fragile","Transitioning","Non Fragile"

COVERAGE_MIN       <- 0.70
K_GRID             <- c(3L, 5L, 7L)
DROP_TOP_LEVERAGE  <- 0.02

PREDICTORS <- c(
  "ODA_RECEIVED_PER_CAPITA",
  "GDP_GROWTH",
  "GDP_PER_CAPITA",
  "GDP_DEFLATOR",
  "POLITICAL_REGIME",
  "ELECTORAL_DEMOCRACY_SCORE",
  "LIBERAL_DEMOCRACY_SCORE",
  "TERRITORIAL_FRAGMENTATION",
  "INSTITUTIONAL_DEMOCRACY_SOCRE",     # keep literal if that's in your data
  "INSTITUTIONAL_AUTOCRACY_SCORE",
  "REGIME_DURABILITY_YEARS",
  "POLITICAL_COMPETITION_SCORE",
  "PARTIAL_DEMOCRACY_WITH_FACTIONALISM",
  "CONFLICT_INTENSITY_YEAR",
  "CONFLICT_CUMULATIVE_INTENSITY_ACROSS_YEARS",
  "N_WAR_FRONTS",
  "MAX_CONFLICT_INTENSITY",
  "AVG_CONFLICT_INTENSITY",
  "N_TOTAL_TROOPS"
)

CORE_NAMES <- c(
  "ODA_RECEIVED_PER_CAPITA_L1",
  "GDP_GROWTH_L1",
  "GDP_PER_CAPITA_L1",
  "GDP_DEFLATOR_L1",
  "ELECTORAL_DEMOCRACY_SCORE_L1",
  "LIBERAL_DEMOCRACY_SCORE_L1",
  "TERRITORIAL_FRAGMENTATION_L1",
  "INSTITUTIONAL_DEMOCRACY_SOCRE_L1",
  "INSTITUTIONAL_AUTOCRACY_SCORE_L1",
  "REGIME_DURABILITY_YEARS_L1",
  "POLITICAL_COMPETITION_SCORE_L1",
  "CONFLICT_INTENSITY_YEAR_L1",
  "CONFLICT_CUMULATIVE_INTENSITY_ACROSS_YEARS_L1",
  "N_WAR_FRONTS_L1",
  "MAX_CONFLICT_INTENSITY_L1",
  "AVG_CONFLICT_INTENSITY_L1",
  "N_TOTAL_TROOPS_L1"
)

# ---------------- Helpers ----------------
as_exit_dummy <- function(vec, k = 5L) {
  n <- length(vec); exit <- integer(n); nf <- (vec == "Non Fragile")
  for (t in seq_len(n)) if (nf[t] && t + k - 1 <= n && all(nf[t:(t + k - 1)])) { exit[t] <- 1L; break }
  exit
}

build_hazard_raw <- function(df, predictors, k_sustain) {
  df <- df %>% arrange(.data[[COUNTRY]], .data[[YEAR]])
  df <- df %>%
    group_by(.data[[COUNTRY]]) %>%
    mutate(EXIT_SUSTAINED = as_exit_dummy(.data[[STATUS]], k = k_sustain)) %>%
    ungroup() %>%
    group_by(.data[[COUNTRY]]) %>%
    mutate(
      ever_exited = any(EXIT_SUSTAINED == 1L),
      first_exit_year = dplyr::if_else(
        ever_exited, min(.data[[YEAR]][EXIT_SUSTAINED == 1L], na.rm = TRUE), NA_integer_
      ),
      at_risk = dplyr::if_else(
        .data[[STATUS]] %in% c("Fragile","Transitioning") &
          (is.na(first_exit_year) | .data[[YEAR]] < first_exit_year),
        1L, 0L
      )
    ) %>% ungroup()

  haz <- df %>% filter(at_risk == 1L | EXIT_SUSTAINED == 1L)

  predictors <- intersect(predictors, names(haz))
  if (!length(predictors)) stop("None of the listed predictors are in the data.")

  haz <- haz %>%
    group_by(.data[[COUNTRY]]) %>%
    mutate(across(all_of(predictors), ~ dplyr::lag(.x, 3L), .names = "{.col}_L1")) %>%
    ungroup()

  # clean factor lags
  refactor_if_exists <- function(df, var) {
    if (var %in% names(df)) df[[var]] <- fct_drop(factor(df[[var]], exclude = NULL))
    df
  }
  haz <- refactor_if_exists(haz, "POLITICAL_REGIME_L1")
  haz <- refactor_if_exists(haz, "PARTIAL_DEMOCRACY_WITH_FACTIONALISM_L1")
  haz
}

prune_by_coverage <- function(haz_raw, predictors, cov_min) {
  lag_cols <- paste0(intersect(predictors, names(haz_raw)), "_L1")
  lag_cols <- intersect(lag_cols, names(haz_raw))
  if (!length(lag_cols)) stop("After lagging, no candidate predictors remained.")

  coverage <- sapply(lag_cols, function(v) mean(!is.na(haz_raw[[v]])))
  keep_lag <- names(coverage)[coverage >= cov_min]
  if (!length(keep_lag)) stop(paste0("No lagged predictors meet coverage >= ", cov_min))

  # strict complete-case on kept lags
  haz <- haz_raw %>% filter(!if_any(all_of(keep_lag), is.na))

  # drop zero-variance among kept lags
  nzv <- vapply(haz[keep_lag], function(x) { ux <- unique(x[!is.na(x)]); length(ux) > 1 }, logical(1))
  list(haz = haz, keep_lag = keep_lag[nzv], coverage = sort(coverage, decreasing = TRUE))
}

fit_clustered <- function(model) {
  vc <- vcovCL(model, cluster = model$data[[COUNTRY]])
  coeftest(model, vcov = vc)
}

influence_drop <- function(glm_model, drop_frac = 0.02) {
  h <- hatvalues(glm_model)
  cutoff <- stats::quantile(h, 1 - drop_frac, na.rm = TRUE)
  which(h >= cutoff)
}

diag_metrics <- function(y, p) {
  out <- list(
    brier = mean((y - p)^2)
  )
  if (have_pROC)  out$auc_roc <- as.numeric(pROC::roc(y, p, quiet = TRUE)$auc)
  if (have_PRROC) out$auc_pr  <- PRROC::pr.curve(scores.class0 = p[y == 1],
                                                 scores.class1 = p[y == 0])$auc.integral
  out
}

# ---------------- Load & prep ----------------
stopifnot(exists("standardized_data_final"))

df <- standardized_data_final %>%
  mutate(
    !!YEAR   := suppressWarnings(as.integer(.data[[YEAR]])),
    !!STATUS := as.character(.data[[STATUS]])
  )

# sanity: warn if any named predictors missing (pre-lag)
missing_preds <- setdiff(PREDICTORS, names(df))
if (length(missing_preds)) warning("Predictors missing in data (pre-lag): ",
                                   paste(missing_preds, collapse=", "))

# factorize regime-like vars pre-lag
fac_candidates <- c("POLITICAL_REGIME","PARTIAL_DEMOCRACY_WITH_FACTIONALISM")
for (v in fac_candidates) if (v %in% names(df)) df[[v]] <- factor(df[[v]], exclude = NULL)

# standardize numeric predictors pre-lag (global z-scores)
num_vars <- setdiff(PREDICTORS, fac_candidates)
num_vars <- intersect(num_vars, names(df))
for (v in num_vars) if (is.numeric(df[[v]])) df[[v]] <- as.numeric(scale(df[[v]]))

cat("\n[Audit] Rows in original df:", nrow(df), "\n")

# ---------------- Master loop ----------------
COLLECT_three_year <- NULL
MODELS  <- list()

for (K in K_GRID) {
  cat("\n================= K =", K, "=================\n")

  haz_raw <- build_hazard_raw(df, predictors = PREDICTORS, k_sustain = K)
  cat("[Audit] Risk-set + event rows (pre-lag-filter):", nrow(haz_raw), "\n")

  # coverage audit
  lag_cols_all <- paste0(intersect(PREDICTORS, names(haz_raw)), "_L1")
  lag_cols_all <- intersect(lag_cols_all, names(haz_raw))
  lag_cov <- sapply(lag_cols_all, function(v) mean(!is.na(haz_raw[[v]])))
  cat("[Audit] Lagged non-missing (top 10):\n")
  print(sort(round(lag_cov, 3), decreasing = TRUE)[1:min(10, length(lag_cov))])

  pruned <- prune_by_coverage(haz_raw, predictors = PREDICTORS, cov_min = COVERAGE_MIN)
  haz      <- pruned$haz
  keep_lag <- pruned$keep_lag
  cat("[Pruning] Kept lagged predictors (", length(keep_lag), ")\n", sep = "")
  cat(paste(keep_lag, collapse = ", "), "\n")
  cat("[Pruning] Rows after pruning:", nrow(haz), "\n")

  # --------- Add duration dependence (spell clock) ----------
  haz <- haz %>%
    arrange(.data[[COUNTRY]], .data[[YEAR]]) %>%
    group_by(.data[[COUNTRY]]) %>%
    mutate(new_spell = (lag(!!sym(STATUS), default = first(.data[[STATUS]])) == "Non Fragile") &
                       .data[[STATUS]] %in% c("Fragile","Transitioning"),
           spell_id  = cumsum(replace_na(new_spell, FALSE))) %>%
    group_by(.data[[COUNTRY]], spell_id) %>%
    mutate(t_at_risk = row_number() - 1L) %>%
    ungroup()

  # --------- End-of-panel trim to avoid immortal-time bias ----------
  max_year <- max(haz[[YEAR]], na.rm = TRUE)
  haz <- haz %>% filter(.data[[YEAR]] <= (max_year - (K - 1L)))
  cat("[Trim] Dropped end-of-panel years to avoid immortal-time bias. Remaining rows:", nrow(haz), "\n")

  # --------- CORE selection ----------
  core <- intersect(CORE_NAMES, keep_lag)
  if ("GDP_DEFLATOR_L1" %in% keep_lag) core <- union(core, "GDP_DEFLATOR_L1")
  if (length(core) < 3L) core <- unique(c(core, head(setdiff(keep_lag, core), 5L)))
  cat("[Core] Variables used:", paste(core, collapse = ", "), "\n")

  # --------- Models ----------
  # (1) Pooled logit w/ Year FE + ns(t_at_risk,3), clustered by country
  f_core_yearfe <- as.formula(paste(
  "EXIT_SUSTAINED ~", paste(core, collapse = " + "),
  "+ splines::ns(t_at_risk, 3) + i(", YEAR, ")", sep = ""
  ))
  m_pool_core <- fixest::feglm(f_core_yearfe, data = haz, family = "logit", cluster = COUNTRY)
  cat("\n=== Pooled logit + Year FE + duration spline (clustered) — CORE ===\n")
  print(summary(m_pool_core))

  # (1b) Influence refit (drop top leverage) — GLM without year FE (transparent influence check)
  f_core <- as.formula(paste(
  "EXIT_SUSTAINED ~", paste(core, collapse = " + "),
  "+ splines::ns(t_at_risk, 3)"
  ))
  m_glm_core <- glm(f_core, data = haz, family = binomial("logit"))
  idx_hi <- influence_drop(m_glm_core, drop_frac = DROP_TOP_LEVERAGE)
  if (length(idx_hi)) {
    haz_rb <- haz[-idx_hi, , drop = FALSE]
    m_pool_core_rb <- fixest::feglm(f_core_yearfe, data = haz_rb, family = "logit", cluster = COUNTRY)
    cat("\n--- Refit after dropping top", round(100*DROP_TOP_LEVERAGE,1), "% leverage (n drop =",
        length(idx_hi), ") ---\n")
    print(summary(m_pool_core_rb))
  }

  # (2) Rare-events (bias-reduced) + clustered SEs — CORE
  m_rare_core <- glm(f_core, data = haz, family = binomial("logit"),
                     method = "brglmFit", control = brglmControl(type = "AS_mixed"))
  cat("\n=== Rare-events (bias-reduced) — CORE ===\n")
  print(summary(m_rare_core))
  cat("\n--- Rare-events + clustered SEs (country) ---\n")
  print(fit_clustered(m_rare_core))

  # collect coef + clustered SE for rare-events model
  co  <- coef(m_rare_core)
  V   <- vcovCL(m_rare_core, cluster = m_rare_core$data[[COUNTRY]])
  se  <- sqrt(diag(V))
  tmp <- data.frame(K = K, term = names(co), beta = unname(co), se = unname(se), row.names = NULL)
  COLLECT_three_year <- bind_rows(COLLECT_three_year, tmp)

  # (3) Random-intercept (country frailty) — CORE
  cat("\n=== Random-intercept (country) — CORE ===\n")
  m_re_core <- try(
    glmmTMB(f_core, data = haz, family = binomial(),
            control = glmmTMBControl(
              optimizer = optim, optArgs = list(method = "BFGS"),
              optCtrl = list(maxit = 1e4)
            )),
    silent = TRUE
  )
  if (inherits(m_re_core, "try-error")) {
    cat("glmmTMB failed to converge (sparse categories / quasi-separation likely).\n")
  } else {
    print(summary(m_re_core))
  }

  # (4) Conditional FE logit (country FE absorbed) — robustness
  cat("\n=== Conditional FE logit (absorbing country + year) — CORE ===\n")
  m_condfe <- try(
    fixest::feglm(
      as.formula(paste(
        "EXIT_SUSTAINED ~", paste(core, collapse=" + "),
        "+ ns(t_at_risk,3) |", COUNTRY, "+", YEAR)),
      data = haz, family = "logit", cluster = COUNTRY
    ),
    silent = TRUE
  )
  if (inherits(m_condfe, "try-error")) {
    cat("Conditional FE logit failed (possibly all-country separation in some years).\n")
  } else {
    print(summary(m_condfe))
  }

  # (5) AMEs (from plain GLM without year FE; AMEs are about X, not time dummies)
  cat("\n=== Average Marginal Effects — CORE GLM ===\n")
  ame <- margins::margins(m_glm_core, data = haz)  # <-- add data=
  print(summary(ame))

  # (6) Predictive diagnostics
  phat <- predict(m_glm_core, type = "response")
  y    <- haz$EXIT_SUSTAINED
  dm   <- diag_metrics(y, phat)
  cat("\n=== Diagnostics (GLM w/ duration) ===\n")
  print(dm)

  # store models
  MODELS[[paste0("K", K, "_poolFE")]]  <- m_pool_core
  MODELS[[paste0("K", K, "_rare")]]    <- m_rare_core
  MODELS[[paste0("K", K, "_glm")]]     <- m_glm_core
  MODELS[[paste0("K", K, "_frailty")]] <- m_re_core
  MODELS[[paste0("K", K, "_condFE")]]  <- m_condfe

  cat("\n================= End K =", K, "=================\n")
}

cat("\n[Done] Coefficient collection available in `COLLECT`; models in `MODELS`.\n")

# Example: tidy coef table across K for a few key variables
if (exists("COLLECT_three_year")) {
  key_terms <- c("(Intercept)", CORE_NAMES[CORE_NAMES %in% COLLECT_three_year$term], "ns(t_at_risk, 3)1", "ns(t_at_risk, 3)2", "ns(t_at_risk, 3)3")
  print(
    COLLECT_three_year %>%
      filter(term %in% key_terms) %>%
      mutate(z = beta / se, p = 2*pnorm(abs(z), lower.tail = FALSE)) %>%
      arrange(term, K)
  )
}

# ------------------------------------------------------------------------------
# LATEX Table ------------------------------------------------------------------
# ------------------------------------------------------------------------------

stopifnot(exists("COLLECT_three_year"))

suppressPackageStartupMessages({
  library(dplyr); library(tidyr); library(kableExtra); library(stringr)
})

# 1) Terms to include: drop intercept, spline basis, and year FE artifacts
omit_patterns <- c("^\\(Intercept\\)$", "splines::ns\\(", "^ns\\(", "^factor\\(", "^i\\(")
omit_re <- paste(omit_patterns, collapse="|")

terms_all <- COLLECT_three_year %>%
  mutate(term = as.character(term)) %>%
  filter(!str_detect(term, omit_re))

# 2) Pretty labels (override where you want; others auto-format)
var_labs_override <- c(
  ODA_RECEIVED_PER_CAPITA_L1 = "ODA per capita (t-3)",
  GDP_GROWTH_L1              = "Real GDP per capita growth (t-3)",
  GDP_PER_CAPITA_L1          = "GDP per capita (t-3)",
  GDP_DEFLATOR_L1            = "GDP deflator (t-3)",
  POLITICAL_REGIME_L1        = "Political regime (t-3)",
  ELECTORAL_DEMOCRACY_SCORE_L1 = "Electoral democracy (t-3)",
  LIBERAL_DEMOCRACY_SCORE_L1   = "Liberal democracy (t-3)",
  TERRITORIAL_FRAGMENTATION_L1 = "Territorial fragmentation (t-3)",
  INSTITUTIONAL_DEMOCRACY_SOCRE_L1 = "Institutional democracy (t-3)",
  INSTITUTIONAL_AUTOCRACY_SCORE_L1 = "Institutional autocracy (t-3)",
  REGIME_DURABILITY_YEARS_L1      = "Regime durability, years (t-3)",
  POLITICAL_COMPETITION_SCORE_L1  = "Political competition (t-3)",
  PARTIAL_DEMOCRACY_WITH_FACTIONALISM_L1 = "Partial democracy w/ factionalism (t-3)",
  CONFLICT_INTENSITY_YEAR_L1      = "Conflict intensity (acute, t-3)",
  CONFLICT_CUMULATIVE_INTENSITY_ACROSS_YEARS_L1 = "Cumulative conflict intensity (t-3)",
  N_WAR_FRONTS_L1                 = "N war fronts (t-3)",
  MAX_CONFLICT_INTENSITY_L1       = "Max conflict intensity (t-3)",
  AVG_CONFLICT_INTENSITY_L1       = "Conflict intensity (chronic avg., t-3)",
  N_TOTAL_TROOPS_L1               = "Total troops (t-2)"
)

auto_label <- function(x) {
  x %>%
    str_replace("_L1$", " (t-1)") %>%
    str_replace_all("_", " ") %>%
    str_to_sentence()
}

# Vectorized labeler: named lookup with [v], fill NAs with auto labels
make_label <- function(v) {
  v <- as.character(v)
  lbl <- unname(var_labs_override[v])      # returns named vector with NAs for misses
  miss <- is.na(lbl)
  lbl[miss] <- auto_label(v[miss])
  lbl
}

# 3) Format OR [LCL, UCL], bold if CI excludes 1
fmt_or_ci <- function(b, s) {
  OR  <- exp(b); LCL <- exp(b - 1.96*s); UCL <- exp(b + 1.96*s)
  cell <- sprintf("%.2f [%.2f, %.2f]", OR, LCL, UCL)
  if (!is.na(LCL) && !is.na(UCL) && (UCL < 1 || LCL > 1)) cell <- paste0("\\textbf{", cell, "}")
  cell
}

# 4) Build long table (K x term)
tab_long <- terms_all %>%
  mutate(
    term_label = make_label(term),
    cell = mapply(fmt_or_ci, beta, se)
  ) %>%
  select(term_label, K, cell)

# Ensure K columns exist in order 3/5/7 and blanks where missing
all_rows <- tibble(term_label = sort(unique(tab_long$term_label)))
all_cols <- tibble(K = c(3L, 5L, 7L))
tab_full <- tidyr::complete(tab_long, term_label = all_rows$term_label, K = all_cols$K, fill = list(cell = ""))

# 5) Optional buckets for section headers
bucket_of <- function(lbl) {
  lbl <- as.character(lbl)
  out <- rep("Other", length(lbl))
  out[stringr::str_detect(lbl, stringr::regex("conflict|war|troop", ignore_case = TRUE))] <- "Conflict & security"
  out[stringr::str_detect(lbl, stringr::regex("gdp|price|oda|per capita|growth|deflator", ignore_case = TRUE))] <- "Economy"
  out[stringr::str_detect(lbl, stringr::regex("democ|autoc|polity|regime|competition|recruitment|fragment", ignore_case = TRUE))] <- "Institutions & politics"
  factor(out, levels = c("Economy","Institutions & politics","Conflict & security","Other"))
}

tab_wide <- tab_full %>%
  dplyr::mutate(
    K = factor(K, levels = c(3,5,7), labels = c("K = 3","K = 5","K = 7")),
    bucket = bucket_of(term_label)
  ) %>%
  tidyr::pivot_wider(names_from = K, values_from = cell) %>%
  dplyr::arrange(bucket, term_label)

# 6) Render LaTeX with grouped sections (robust start/end calc)
kb <- kable(
  tab_wide %>% select(term_label, `K = 3`, `K = 5`, `K = 7`),
  format = "latex",
  booktabs = TRUE,
  linesep = "",
  caption = "Sustained Exit from Fragility: Odds Ratios with 95\\% Clustered CIs (ALL predictors, rare-events logit)",
  col.names = c("Predictors", "K = 3", "K = 5", "K = 7"),
  escape = FALSE,
  align = c("l","c","c","c"),
  label = "tab:all_or"
) %>%
  kable_styling(latex_options = c("hold_position","striped","scale_down"),
                font_size = 8) %>%                                 # smaller font + auto-resize
  column_spec(1, width = "4.5cm") %>%                              # wrap predictor names
  column_spec(2, width = "3.2cm") %>%                              # narrow K columns
  column_spec(3, width = "3.2cm") %>%
  column_spec(4, width = "3.2cm") %>%
  add_header_above(c(" " = 1, "Sustain horizon (years)" = 3))

# Add group headers without rle()
bucket_levels <- c("Economy","Institutions & politics","Conflict & security","Other")
# make sure ordering is locked
tab_wide$bucket <- factor(tab_wide$bucket, levels = bucket_levels)

idx_by_bucket <- split(seq_len(nrow(tab_wide)), tab_wide$bucket)

for (lev in bucket_levels) {
  idx <- idx_by_bucket[[lev]]
  if (!is.null(idx) && length(idx)) {
    kb <- pack_rows(kb, lev, start = min(idx), end = max(idx), indent = FALSE)
  }
}

kb <- footnote(
  kb,
  general = "Entries are odds ratios with 95% clustered confidence intervals. Models are bias-reduced (brglm2, ASmixed). Covariates standardized before lagging. Spline of time-at-risk included; estimates correspond to the rare-events specification collected in COLLECT (no year FE). For the year-FE version, rebuild with COLLECTYFE and rerun. Outcome: first sustained exit (Non-Fragile) at t persisting for K years; sample is the risk set plus first exit year. We exclude Executive Recruitment and Polity Score from the core specification because their coefficients are not well identified: Executive Recruitment exhibits complete or quasi-separation in the risk set, yielding boundary estimates and uninformative confidence intervals, while Polity Score is highly collinear with our V-Dem democracy measures. Both variables are reported in Appendix Table A.x for completeness.",
  threeparttable = TRUE, escape = FALSE
)

cat(kb)

```

## Four-Year Lags

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# ---------------- Settings ----------------
COUNTRY <- "COUNTRY_NAME"
YEAR    <- "YEAR"
STATUS  <- "VDEM_STATUS_IDEAL"  # "Fragile","Transitioning","Non Fragile"

COVERAGE_MIN       <- 0.70
K_GRID             <- c(3L, 5L, 7L)
DROP_TOP_LEVERAGE  <- 0.02

PREDICTORS <- c(
  "ODA_RECEIVED_PER_CAPITA",
  "GDP_GROWTH",
  "GDP_PER_CAPITA",
  "GDP_DEFLATOR",
  "POLITICAL_REGIME",
  "ELECTORAL_DEMOCRACY_SCORE",
  "LIBERAL_DEMOCRACY_SCORE",
  "TERRITORIAL_FRAGMENTATION",
  "INSTITUTIONAL_DEMOCRACY_SOCRE",     # keep literal if that's in your data
  "INSTITUTIONAL_AUTOCRACY_SCORE",
  "REGIME_DURABILITY_YEARS",
  "POLITICAL_COMPETITION_SCORE",
  "PARTIAL_DEMOCRACY_WITH_FACTIONALISM",
  "CONFLICT_INTENSITY_YEAR",
  "CONFLICT_CUMULATIVE_INTENSITY_ACROSS_YEARS",
  "N_WAR_FRONTS",
  "MAX_CONFLICT_INTENSITY",
  "AVG_CONFLICT_INTENSITY",
  "N_TOTAL_TROOPS"
)

CORE_NAMES <- c(
  "ODA_RECEIVED_PER_CAPITA_L1",
  "GDP_GROWTH_L1",
  "GDP_PER_CAPITA_L1",
  "GDP_DEFLATOR_L1",
  "ELECTORAL_DEMOCRACY_SCORE_L1",
  "LIBERAL_DEMOCRACY_SCORE_L1",
  "TERRITORIAL_FRAGMENTATION_L1",
  "INSTITUTIONAL_DEMOCRACY_SOCRE_L1",
  "INSTITUTIONAL_AUTOCRACY_SCORE_L1",
  "REGIME_DURABILITY_YEARS_L1",
  "POLITICAL_COMPETITION_SCORE_L1",
  "CONFLICT_INTENSITY_YEAR_L1",
  "CONFLICT_CUMULATIVE_INTENSITY_ACROSS_YEARS_L1",
  "N_WAR_FRONTS_L1",
  "MAX_CONFLICT_INTENSITY_L1",
  "AVG_CONFLICT_INTENSITY_L1",
  "N_TOTAL_TROOPS_L1"
)

# ---------------- Helpers ----------------
as_exit_dummy <- function(vec, k = 5L) {
  n <- length(vec); exit <- integer(n); nf <- (vec == "Non Fragile")
  for (t in seq_len(n)) if (nf[t] && t + k - 1 <= n && all(nf[t:(t + k - 1)])) { exit[t] <- 1L; break }
  exit
}

build_hazard_raw <- function(df, predictors, k_sustain) {
  df <- df %>% arrange(.data[[COUNTRY]], .data[[YEAR]])
  df <- df %>%
    group_by(.data[[COUNTRY]]) %>%
    mutate(EXIT_SUSTAINED = as_exit_dummy(.data[[STATUS]], k = k_sustain)) %>%
    ungroup() %>%
    group_by(.data[[COUNTRY]]) %>%
    mutate(
      ever_exited = any(EXIT_SUSTAINED == 1L),
      first_exit_year = dplyr::if_else(
        ever_exited, min(.data[[YEAR]][EXIT_SUSTAINED == 1L], na.rm = TRUE), NA_integer_
      ),
      at_risk = dplyr::if_else(
        .data[[STATUS]] %in% c("Fragile","Transitioning") &
          (is.na(first_exit_year) | .data[[YEAR]] < first_exit_year),
        1L, 0L
      )
    ) %>% ungroup()

  haz <- df %>% filter(at_risk == 1L | EXIT_SUSTAINED == 1L)

  predictors <- intersect(predictors, names(haz))
  if (!length(predictors)) stop("None of the listed predictors are in the data.")

  haz <- haz %>%
    group_by(.data[[COUNTRY]]) %>%
    mutate(across(all_of(predictors), ~ dplyr::lag(.x, 4L), .names = "{.col}_L1")) %>%
    ungroup()

  # clean factor lags
  refactor_if_exists <- function(df, var) {
    if (var %in% names(df)) df[[var]] <- fct_drop(factor(df[[var]], exclude = NULL))
    df
  }
  haz <- refactor_if_exists(haz, "POLITICAL_REGIME_L1")
  haz <- refactor_if_exists(haz, "PARTIAL_DEMOCRACY_WITH_FACTIONALISM_L1")
  haz
}

prune_by_coverage <- function(haz_raw, predictors, cov_min) {
  lag_cols <- paste0(intersect(predictors, names(haz_raw)), "_L1")
  lag_cols <- intersect(lag_cols, names(haz_raw))
  if (!length(lag_cols)) stop("After lagging, no candidate predictors remained.")

  coverage <- sapply(lag_cols, function(v) mean(!is.na(haz_raw[[v]])))
  keep_lag <- names(coverage)[coverage >= cov_min]
  if (!length(keep_lag)) stop(paste0("No lagged predictors meet coverage >= ", cov_min))

  # strict complete-case on kept lags
  haz <- haz_raw %>% filter(!if_any(all_of(keep_lag), is.na))

  # drop zero-variance among kept lags
  nzv <- vapply(haz[keep_lag], function(x) { ux <- unique(x[!is.na(x)]); length(ux) > 1 }, logical(1))
  list(haz = haz, keep_lag = keep_lag[nzv], coverage = sort(coverage, decreasing = TRUE))
}

fit_clustered <- function(model) {
  vc <- vcovCL(model, cluster = model$data[[COUNTRY]])
  coeftest(model, vcov = vc)
}

influence_drop <- function(glm_model, drop_frac = 0.02) {
  h <- hatvalues(glm_model)
  cutoff <- stats::quantile(h, 1 - drop_frac, na.rm = TRUE)
  which(h >= cutoff)
}

diag_metrics <- function(y, p) {
  out <- list(
    brier = mean((y - p)^2)
  )
  if (have_pROC)  out$auc_roc <- as.numeric(pROC::roc(y, p, quiet = TRUE)$auc)
  if (have_PRROC) out$auc_pr  <- PRROC::pr.curve(scores.class0 = p[y == 1],
                                                 scores.class1 = p[y == 0])$auc.integral
  out
}

# ---------------- Load & prep ----------------
stopifnot(exists("standardized_data_final"))

df <- standardized_data_final %>%
  mutate(
    !!YEAR   := suppressWarnings(as.integer(.data[[YEAR]])),
    !!STATUS := as.character(.data[[STATUS]])
  )

# sanity: warn if any named predictors missing (pre-lag)
missing_preds <- setdiff(PREDICTORS, names(df))
if (length(missing_preds)) warning("Predictors missing in data (pre-lag): ",
                                   paste(missing_preds, collapse=", "))

# factorize regime-like vars pre-lag
fac_candidates <- c("POLITICAL_REGIME","PARTIAL_DEMOCRACY_WITH_FACTIONALISM")
for (v in fac_candidates) if (v %in% names(df)) df[[v]] <- factor(df[[v]], exclude = NULL)

# standardize numeric predictors pre-lag (global z-scores)
num_vars <- setdiff(PREDICTORS, fac_candidates)
num_vars <- intersect(num_vars, names(df))
for (v in num_vars) if (is.numeric(df[[v]])) df[[v]] <- as.numeric(scale(df[[v]]))

cat("\n[Audit] Rows in original df:", nrow(df), "\n")

# ---------------- Master loop ----------------
COLLECT_four_year <- NULL
MODELS  <- list()

for (K in K_GRID) {
  cat("\n================= K =", K, "=================\n")

  haz_raw <- build_hazard_raw(df, predictors = PREDICTORS, k_sustain = K)
  cat("[Audit] Risk-set + event rows (pre-lag-filter):", nrow(haz_raw), "\n")

  # coverage audit
  lag_cols_all <- paste0(intersect(PREDICTORS, names(haz_raw)), "_L1")
  lag_cols_all <- intersect(lag_cols_all, names(haz_raw))
  lag_cov <- sapply(lag_cols_all, function(v) mean(!is.na(haz_raw[[v]])))
  cat("[Audit] Lagged non-missing (top 10):\n")
  print(sort(round(lag_cov, 3), decreasing = TRUE)[1:min(10, length(lag_cov))])

  pruned <- prune_by_coverage(haz_raw, predictors = PREDICTORS, cov_min = COVERAGE_MIN)
  haz      <- pruned$haz
  keep_lag <- pruned$keep_lag
  cat("[Pruning] Kept lagged predictors (", length(keep_lag), ")\n", sep = "")
  cat(paste(keep_lag, collapse = ", "), "\n")
  cat("[Pruning] Rows after pruning:", nrow(haz), "\n")

  # --------- Add duration dependence (spell clock) ----------
  haz <- haz %>%
    arrange(.data[[COUNTRY]], .data[[YEAR]]) %>%
    group_by(.data[[COUNTRY]]) %>%
    mutate(new_spell = (lag(!!sym(STATUS), default = first(.data[[STATUS]])) == "Non Fragile") &
                       .data[[STATUS]] %in% c("Fragile","Transitioning"),
           spell_id  = cumsum(replace_na(new_spell, FALSE))) %>%
    group_by(.data[[COUNTRY]], spell_id) %>%
    mutate(t_at_risk = row_number() - 1L) %>%
    ungroup()

  # --------- End-of-panel trim to avoid immortal-time bias ----------
  max_year <- max(haz[[YEAR]], na.rm = TRUE)
  haz <- haz %>% filter(.data[[YEAR]] <= (max_year - (K - 1L)))
  cat("[Trim] Dropped end-of-panel years to avoid immortal-time bias. Remaining rows:", nrow(haz), "\n")

  # --------- CORE selection ----------
  core <- intersect(CORE_NAMES, keep_lag)
  if ("GDP_DEFLATOR_L1" %in% keep_lag) core <- union(core, "GDP_DEFLATOR_L1")
  if (length(core) < 3L) core <- unique(c(core, head(setdiff(keep_lag, core), 5L)))
  cat("[Core] Variables used:", paste(core, collapse = ", "), "\n")

  # --------- Models ----------
  # (1) Pooled logit w/ Year FE + ns(t_at_risk,3), clustered by country
  f_core_yearfe <- as.formula(paste(
  "EXIT_SUSTAINED ~", paste(core, collapse = " + "),
  "+ splines::ns(t_at_risk, 3) + i(", YEAR, ")", sep = ""
  ))
  m_pool_core <- fixest::feglm(f_core_yearfe, data = haz, family = "logit", cluster = COUNTRY)
  cat("\n=== Pooled logit + Year FE + duration spline (clustered) — CORE ===\n")
  print(summary(m_pool_core))

  # (1b) Influence refit (drop top leverage) — GLM without year FE (transparent influence check)
  f_core <- as.formula(paste(
  "EXIT_SUSTAINED ~", paste(core, collapse = " + "),
  "+ splines::ns(t_at_risk, 3)"
  ))
  m_glm_core <- glm(f_core, data = haz, family = binomial("logit"))
  idx_hi <- influence_drop(m_glm_core, drop_frac = DROP_TOP_LEVERAGE)
  if (length(idx_hi)) {
    haz_rb <- haz[-idx_hi, , drop = FALSE]
    m_pool_core_rb <- fixest::feglm(f_core_yearfe, data = haz_rb, family = "logit", cluster = COUNTRY)
    cat("\n--- Refit after dropping top", round(100*DROP_TOP_LEVERAGE,1), "% leverage (n drop =",
        length(idx_hi), ") ---\n")
    print(summary(m_pool_core_rb))
  }

  # (2) Rare-events (bias-reduced) + clustered SEs — CORE
  m_rare_core <- glm(f_core, data = haz, family = binomial("logit"),
                     method = "brglmFit", control = brglmControl(type = "AS_mixed"))
  cat("\n=== Rare-events (bias-reduced) — CORE ===\n")
  print(summary(m_rare_core))
  cat("\n--- Rare-events + clustered SEs (country) ---\n")
  print(fit_clustered(m_rare_core))

  # collect coef + clustered SE for rare-events model
  co  <- coef(m_rare_core)
  V   <- vcovCL(m_rare_core, cluster = m_rare_core$data[[COUNTRY]])
  se  <- sqrt(diag(V))
  tmp <- data.frame(K = K, term = names(co), beta = unname(co), se = unname(se), row.names = NULL)
  COLLECT_four_year <- bind_rows(COLLECT_four_year, tmp)

  # (3) Random-intercept (country frailty) — CORE
  cat("\n=== Random-intercept (country) — CORE ===\n")
  m_re_core <- try(
    glmmTMB(f_core, data = haz, family = binomial(),
            control = glmmTMBControl(
              optimizer = optim, optArgs = list(method = "BFGS"),
              optCtrl = list(maxit = 1e4)
            )),
    silent = TRUE
  )
  if (inherits(m_re_core, "try-error")) {
    cat("glmmTMB failed to converge (sparse categories / quasi-separation likely).\n")
  } else {
    print(summary(m_re_core))
  }

  # (4) Conditional FE logit (country FE absorbed) — robustness
  cat("\n=== Conditional FE logit (absorbing country + year) — CORE ===\n")
  m_condfe <- try(
    fixest::feglm(
      as.formula(paste(
        "EXIT_SUSTAINED ~", paste(core, collapse=" + "),
        "+ ns(t_at_risk,3) |", COUNTRY, "+", YEAR)),
      data = haz, family = "logit", cluster = COUNTRY
    ),
    silent = TRUE
  )
  if (inherits(m_condfe, "try-error")) {
    cat("Conditional FE logit failed (possibly all-country separation in some years).\n")
  } else {
    print(summary(m_condfe))
  }

  # (5) AMEs (from plain GLM without year FE; AMEs are about X, not time dummies)
  cat("\n=== Average Marginal Effects — CORE GLM ===\n")
  ame <- margins::margins(m_glm_core, data = haz)  # <-- add data=
  print(summary(ame))

  # (6) Predictive diagnostics
  phat <- predict(m_glm_core, type = "response")
  y    <- haz$EXIT_SUSTAINED
  dm   <- diag_metrics(y, phat)
  cat("\n=== Diagnostics (GLM w/ duration) ===\n")
  print(dm)

  # store models
  MODELS[[paste0("K", K, "_poolFE")]]  <- m_pool_core
  MODELS[[paste0("K", K, "_rare")]]    <- m_rare_core
  MODELS[[paste0("K", K, "_glm")]]     <- m_glm_core
  MODELS[[paste0("K", K, "_frailty")]] <- m_re_core
  MODELS[[paste0("K", K, "_condFE")]]  <- m_condfe

  cat("\n================= End K =", K, "=================\n")
}

cat("\n[Done] Coefficient collection available in `COLLECT`; models in `MODELS`.\n")

# Example: tidy coef table across K for a few key variables
if (exists("COLLECT_four_year")) {
  key_terms <- c("(Intercept)", CORE_NAMES[CORE_NAMES %in% COLLECT_four_year$term], "ns(t_at_risk, 3)1", "ns(t_at_risk, 3)2", "ns(t_at_risk, 3)3")
  print(
    COLLECT_four_year %>%
      filter(term %in% key_terms) %>%
      mutate(z = beta / se, p = 2*pnorm(abs(z), lower.tail = FALSE)) %>%
      arrange(term, K)
  )
}

# ------------------------------------------------------------------------------
# LATEX Table ------------------------------------------------------------------
# ------------------------------------------------------------------------------

stopifnot(exists("COLLECT_four_year"))

suppressPackageStartupMessages({
  library(dplyr); library(tidyr); library(kableExtra); library(stringr)
})

# 1) Terms to include: drop intercept, spline basis, and year FE artifacts
omit_patterns <- c("^\\(Intercept\\)$", "splines::ns\\(", "^ns\\(", "^factor\\(", "^i\\(")
omit_re <- paste(omit_patterns, collapse="|")

terms_all <- COLLECT_four_year %>%
  mutate(term = as.character(term)) %>%
  filter(!str_detect(term, omit_re))

# 2) Pretty labels (override where you want; others auto-format)
var_labs_override <- c(
  ODA_RECEIVED_PER_CAPITA_L1 = "ODA per capita (t-4)",
  GDP_GROWTH_L1              = "Real GDP per capita growth (t-4)",
  GDP_PER_CAPITA_L1          = "GDP per capita (t-4)",
  GDP_DEFLATOR_L1            = "GDP deflator (t-4)",
  POLITICAL_REGIME_L1        = "Political regime (t-4)",
  ELECTORAL_DEMOCRACY_SCORE_L1 = "Electoral democracy (t-4)",
  LIBERAL_DEMOCRACY_SCORE_L1   = "Liberal democracy (t-4)",
  TERRITORIAL_FRAGMENTATION_L1 = "Territorial fragmentation (t-4)",
  INSTITUTIONAL_DEMOCRACY_SOCRE_L1 = "Institutional democracy (t-4)",
  INSTITUTIONAL_AUTOCRACY_SCORE_L1 = "Institutional autocracy (t-4)",
  REGIME_DURABILITY_YEARS_L1      = "Regime durability, years (t-4)",
  POLITICAL_COMPETITION_SCORE_L1  = "Political competition (t-4)",
  PARTIAL_DEMOCRACY_WITH_FACTIONALISM_L1 = "Partial democracy w/ factionalism (t-4)",
  CONFLICT_INTENSITY_YEAR_L1      = "Conflict intensity (acute, t-4)",
  CONFLICT_CUMULATIVE_INTENSITY_ACROSS_YEARS_L1 = "Cumulative conflict intensity (t-4)",
  N_WAR_FRONTS_L1                 = "N war fronts (t-4)",
  MAX_CONFLICT_INTENSITY_L1       = "Max conflict intensity (t-4)",
  AVG_CONFLICT_INTENSITY_L1       = "Conflict intensity (chronic avg., t-4)",
  N_TOTAL_TROOPS_L1               = "Total troops (t-4)"
)

auto_label <- function(x) {
  x %>%
    str_replace("_L1$", " (t-1)") %>%
    str_replace_all("_", " ") %>%
    str_to_sentence()
}

# Vectorized labeler: named lookup with [v], fill NAs with auto labels
make_label <- function(v) {
  v <- as.character(v)
  lbl <- unname(var_labs_override[v])      # returns named vector with NAs for misses
  miss <- is.na(lbl)
  lbl[miss] <- auto_label(v[miss])
  lbl
}

# 3) Format OR [LCL, UCL], bold if CI excludes 1
fmt_or_ci <- function(b, s) {
  OR  <- exp(b); LCL <- exp(b - 1.96*s); UCL <- exp(b + 1.96*s)
  cell <- sprintf("%.2f [%.2f, %.2f]", OR, LCL, UCL)
  if (!is.na(LCL) && !is.na(UCL) && (UCL < 1 || LCL > 1)) cell <- paste0("\\textbf{", cell, "}")
  cell
}

# 4) Build long table (K x term)
tab_long <- terms_all %>%
  mutate(
    term_label = make_label(term),
    cell = mapply(fmt_or_ci, beta, se)
  ) %>%
  select(term_label, K, cell)

# Ensure K columns exist in order 3/5/7 and blanks where missing
all_rows <- tibble(term_label = sort(unique(tab_long$term_label)))
all_cols <- tibble(K = c(3L, 5L, 7L))
tab_full <- tidyr::complete(tab_long, term_label = all_rows$term_label, K = all_cols$K, fill = list(cell = ""))

# 5) Optional buckets for section headers
bucket_of <- function(lbl) {
  lbl <- as.character(lbl)
  out <- rep("Other", length(lbl))
  out[stringr::str_detect(lbl, stringr::regex("conflict|war|troop", ignore_case = TRUE))] <- "Conflict & security"
  out[stringr::str_detect(lbl, stringr::regex("gdp|price|oda|per capita|growth|deflator", ignore_case = TRUE))] <- "Economy"
  out[stringr::str_detect(lbl, stringr::regex("democ|autoc|polity|regime|competition|recruitment|fragment", ignore_case = TRUE))] <- "Institutions & politics"
  factor(out, levels = c("Economy","Institutions & politics","Conflict & security","Other"))
}

tab_wide <- tab_full %>%
  dplyr::mutate(
    K = factor(K, levels = c(3,5,7), labels = c("K = 3","K = 5","K = 7")),
    bucket = bucket_of(term_label)
  ) %>%
  tidyr::pivot_wider(names_from = K, values_from = cell) %>%
  dplyr::arrange(bucket, term_label)

# 6) Render LaTeX with grouped sections (robust start/end calc)
kb <- kable(
  tab_wide %>% select(term_label, `K = 3`, `K = 5`, `K = 7`),
  format = "latex",
  booktabs = TRUE,
  linesep = "",
  caption = "Sustained Exit from Fragility: Odds Ratios with 95\\% Clustered CIs (ALL predictors, rare-events logit)",
  col.names = c("Predictors", "K = 3", "K = 5", "K = 7"),
  escape = FALSE,
  align = c("l","c","c","c"),
  label = "tab:all_or"
) %>%
  kable_styling(latex_options = c("hold_position","striped","scale_down"),
                font_size = 8) %>%                                 # smaller font + auto-resize
  column_spec(1, width = "4.5cm") %>%                              # wrap predictor names
  column_spec(2, width = "3.2cm") %>%                              # narrow K columns
  column_spec(3, width = "3.2cm") %>%
  column_spec(4, width = "3.2cm") %>%
  add_header_above(c(" " = 1, "Sustain horizon (years)" = 3))

# Add group headers without rle()
bucket_levels <- c("Economy","Institutions & politics","Conflict & security","Other")
# make sure ordering is locked
tab_wide$bucket <- factor(tab_wide$bucket, levels = bucket_levels)

idx_by_bucket <- split(seq_len(nrow(tab_wide)), tab_wide$bucket)

for (lev in bucket_levels) {
  idx <- idx_by_bucket[[lev]]
  if (!is.null(idx) && length(idx)) {
    kb <- pack_rows(kb, lev, start = min(idx), end = max(idx), indent = FALSE)
  }
}

kb <- footnote(
  kb,
  general = "Entries are odds ratios with 95% clustered confidence intervals. Models are bias-reduced (brglm2, ASmixed). Covariates standardized before lagging. Spline of time-at-risk included; estimates correspond to the rare-events specification collected in COLLECT (no year FE). For the year-FE version, rebuild with COLLECTYFE and rerun. Outcome: first sustained exit (Non-Fragile) at t persisting for K years; sample is the risk set plus first exit year. We exclude Executive Recruitment and Polity Score from the core specification because their coefficients are not well identified: Executive Recruitment exhibits complete or quasi-separation in the risk set, yielding boundary estimates and uninformative confidence intervals, while Polity Score is highly collinear with our V-Dem democracy measures. Both variables are reported in Appendix Table A.x for completeness.",
  threeparttable = TRUE, escape = FALSE
)

cat(kb)

```

## Five-Year Lags

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# ---------------- Settings ----------------
COUNTRY <- "COUNTRY_NAME"
YEAR    <- "YEAR"
STATUS  <- "VDEM_STATUS_IDEAL"  # "Fragile","Transitioning","Non Fragile"

COVERAGE_MIN       <- 0.70
K_GRID             <- c(3L, 5L, 7L)
DROP_TOP_LEVERAGE  <- 0.02

PREDICTORS <- c(
  "ODA_RECEIVED_PER_CAPITA",
  "GDP_GROWTH",
  "GDP_PER_CAPITA",
  "GDP_DEFLATOR",
  "POLITICAL_REGIME",
  "ELECTORAL_DEMOCRACY_SCORE",
  "LIBERAL_DEMOCRACY_SCORE",
  "TERRITORIAL_FRAGMENTATION",
  "INSTITUTIONAL_DEMOCRACY_SOCRE",     # keep literal if that's in your data
  "INSTITUTIONAL_AUTOCRACY_SCORE",
  "REGIME_DURABILITY_YEARS",
  "POLITICAL_COMPETITION_SCORE",
  "PARTIAL_DEMOCRACY_WITH_FACTIONALISM",
  "CONFLICT_INTENSITY_YEAR",
  "CONFLICT_CUMULATIVE_INTENSITY_ACROSS_YEARS",
  "N_WAR_FRONTS",
  "MAX_CONFLICT_INTENSITY",
  "AVG_CONFLICT_INTENSITY",
  "N_TOTAL_TROOPS"
)

CORE_NAMES <- c(
  "ODA_RECEIVED_PER_CAPITA_L1",
  "GDP_GROWTH_L1",
  "GDP_PER_CAPITA_L1",
  "GDP_DEFLATOR_L1",
  "ELECTORAL_DEMOCRACY_SCORE_L1",
  "LIBERAL_DEMOCRACY_SCORE_L1",
  "TERRITORIAL_FRAGMENTATION_L1",
  "INSTITUTIONAL_DEMOCRACY_SOCRE_L1",
  "INSTITUTIONAL_AUTOCRACY_SCORE_L1",
  "REGIME_DURABILITY_YEARS_L1",
  "POLITICAL_COMPETITION_SCORE_L1",
  "CONFLICT_INTENSITY_YEAR_L1",
  "CONFLICT_CUMULATIVE_INTENSITY_ACROSS_YEARS_L1",
  "N_WAR_FRONTS_L1",
  "MAX_CONFLICT_INTENSITY_L1",
  "AVG_CONFLICT_INTENSITY_L1",
  "N_TOTAL_TROOPS_L1"
)

# ---------------- Helpers ----------------
as_exit_dummy <- function(vec, k = 5L) {
  n <- length(vec); exit <- integer(n); nf <- (vec == "Non Fragile")
  for (t in seq_len(n)) if (nf[t] && t + k - 1 <= n && all(nf[t:(t + k - 1)])) { exit[t] <- 1L; break }
  exit
}

build_hazard_raw <- function(df, predictors, k_sustain) {
  df <- df %>% arrange(.data[[COUNTRY]], .data[[YEAR]])
  df <- df %>%
    group_by(.data[[COUNTRY]]) %>%
    mutate(EXIT_SUSTAINED = as_exit_dummy(.data[[STATUS]], k = k_sustain)) %>%
    ungroup() %>%
    group_by(.data[[COUNTRY]]) %>%
    mutate(
      ever_exited = any(EXIT_SUSTAINED == 1L),
      first_exit_year = dplyr::if_else(
        ever_exited, min(.data[[YEAR]][EXIT_SUSTAINED == 1L], na.rm = TRUE), NA_integer_
      ),
      at_risk = dplyr::if_else(
        .data[[STATUS]] %in% c("Fragile","Transitioning") &
          (is.na(first_exit_year) | .data[[YEAR]] < first_exit_year),
        1L, 0L
      )
    ) %>% ungroup()

  haz <- df %>% filter(at_risk == 1L | EXIT_SUSTAINED == 1L)

  predictors <- intersect(predictors, names(haz))
  if (!length(predictors)) stop("None of the listed predictors are in the data.")

  haz <- haz %>%
    group_by(.data[[COUNTRY]]) %>%
    mutate(across(all_of(predictors), ~ dplyr::lag(.x, 5L), .names = "{.col}_L1")) %>%
    ungroup()

  # clean factor lags
  refactor_if_exists <- function(df, var) {
    if (var %in% names(df)) df[[var]] <- fct_drop(factor(df[[var]], exclude = NULL))
    df
  }
  haz <- refactor_if_exists(haz, "POLITICAL_REGIME_L1")
  haz <- refactor_if_exists(haz, "PARTIAL_DEMOCRACY_WITH_FACTIONALISM_L1")
  haz
}

prune_by_coverage <- function(haz_raw, predictors, cov_min) {
  lag_cols <- paste0(intersect(predictors, names(haz_raw)), "_L1")
  lag_cols <- intersect(lag_cols, names(haz_raw))
  if (!length(lag_cols)) stop("After lagging, no candidate predictors remained.")

  coverage <- sapply(lag_cols, function(v) mean(!is.na(haz_raw[[v]])))
  keep_lag <- names(coverage)[coverage >= cov_min]
  if (!length(keep_lag)) stop(paste0("No lagged predictors meet coverage >= ", cov_min))

  # strict complete-case on kept lags
  haz <- haz_raw %>% filter(!if_any(all_of(keep_lag), is.na))

  # drop zero-variance among kept lags
  nzv <- vapply(haz[keep_lag], function(x) { ux <- unique(x[!is.na(x)]); length(ux) > 1 }, logical(1))
  list(haz = haz, keep_lag = keep_lag[nzv], coverage = sort(coverage, decreasing = TRUE))
}

fit_clustered <- function(model) {
  vc <- vcovCL(model, cluster = model$data[[COUNTRY]])
  coeftest(model, vcov = vc)
}

influence_drop <- function(glm_model, drop_frac = 0.02) {
  h <- hatvalues(glm_model)
  cutoff <- stats::quantile(h, 1 - drop_frac, na.rm = TRUE)
  which(h >= cutoff)
}

diag_metrics <- function(y, p) {
  out <- list(
    brier = mean((y - p)^2)
  )
  if (have_pROC)  out$auc_roc <- as.numeric(pROC::roc(y, p, quiet = TRUE)$auc)
  if (have_PRROC) out$auc_pr  <- PRROC::pr.curve(scores.class0 = p[y == 1],
                                                 scores.class1 = p[y == 0])$auc.integral
  out
}

# ---------------- Load & prep ----------------
stopifnot(exists("standardized_data_final"))

df <- standardized_data_final %>%
  mutate(
    !!YEAR   := suppressWarnings(as.integer(.data[[YEAR]])),
    !!STATUS := as.character(.data[[STATUS]])
  )

# sanity: warn if any named predictors missing (pre-lag)
missing_preds <- setdiff(PREDICTORS, names(df))
if (length(missing_preds)) warning("Predictors missing in data (pre-lag): ",
                                   paste(missing_preds, collapse=", "))

# factorize regime-like vars pre-lag
fac_candidates <- c("POLITICAL_REGIME","PARTIAL_DEMOCRACY_WITH_FACTIONALISM")
for (v in fac_candidates) if (v %in% names(df)) df[[v]] <- factor(df[[v]], exclude = NULL)

# standardize numeric predictors pre-lag (global z-scores)
num_vars <- setdiff(PREDICTORS, fac_candidates)
num_vars <- intersect(num_vars, names(df))
for (v in num_vars) if (is.numeric(df[[v]])) df[[v]] <- as.numeric(scale(df[[v]]))

cat("\n[Audit] Rows in original df:", nrow(df), "\n")

# ---------------- Master loop ----------------
COLLECT_five_year <- NULL
MODELS  <- list()

for (K in K_GRID) {
  cat("\n================= K =", K, "=================\n")

  haz_raw <- build_hazard_raw(df, predictors = PREDICTORS, k_sustain = K)
  cat("[Audit] Risk-set + event rows (pre-lag-filter):", nrow(haz_raw), "\n")

  # coverage audit
  lag_cols_all <- paste0(intersect(PREDICTORS, names(haz_raw)), "_L1")
  lag_cols_all <- intersect(lag_cols_all, names(haz_raw))
  lag_cov <- sapply(lag_cols_all, function(v) mean(!is.na(haz_raw[[v]])))
  cat("[Audit] Lagged non-missing (top 10):\n")
  print(sort(round(lag_cov, 3), decreasing = TRUE)[1:min(10, length(lag_cov))])

  pruned <- prune_by_coverage(haz_raw, predictors = PREDICTORS, cov_min = COVERAGE_MIN)
  haz      <- pruned$haz
  keep_lag <- pruned$keep_lag
  cat("[Pruning] Kept lagged predictors (", length(keep_lag), ")\n", sep = "")
  cat(paste(keep_lag, collapse = ", "), "\n")
  cat("[Pruning] Rows after pruning:", nrow(haz), "\n")

  # --------- Add duration dependence (spell clock) ----------
  haz <- haz %>%
    arrange(.data[[COUNTRY]], .data[[YEAR]]) %>%
    group_by(.data[[COUNTRY]]) %>%
    mutate(new_spell = (lag(!!sym(STATUS), default = first(.data[[STATUS]])) == "Non Fragile") &
                       .data[[STATUS]] %in% c("Fragile","Transitioning"),
           spell_id  = cumsum(replace_na(new_spell, FALSE))) %>%
    group_by(.data[[COUNTRY]], spell_id) %>%
    mutate(t_at_risk = row_number() - 1L) %>%
    ungroup()

  # --------- End-of-panel trim to avoid immortal-time bias ----------
  max_year <- max(haz[[YEAR]], na.rm = TRUE)
  haz <- haz %>% filter(.data[[YEAR]] <= (max_year - (K - 1L)))
  cat("[Trim] Dropped end-of-panel years to avoid immortal-time bias. Remaining rows:", nrow(haz), "\n")

  # --------- CORE selection ----------
  core <- intersect(CORE_NAMES, keep_lag)
  if ("GDP_DEFLATOR_L1" %in% keep_lag) core <- union(core, "GDP_DEFLATOR_L1")
  if (length(core) < 3L) core <- unique(c(core, head(setdiff(keep_lag, core), 5L)))
  cat("[Core] Variables used:", paste(core, collapse = ", "), "\n")

  # --------- Models ----------
  # (1) Pooled logit w/ Year FE + ns(t_at_risk,3), clustered by country
  f_core_yearfe <- as.formula(paste(
  "EXIT_SUSTAINED ~", paste(core, collapse = " + "),
  "+ splines::ns(t_at_risk, 3) + i(", YEAR, ")", sep = ""
  ))
  m_pool_core <- fixest::feglm(f_core_yearfe, data = haz, family = "logit", cluster = COUNTRY)
  cat("\n=== Pooled logit + Year FE + duration spline (clustered) — CORE ===\n")
  print(summary(m_pool_core))

  # (1b) Influence refit (drop top leverage) — GLM without year FE (transparent influence check)
  f_core <- as.formula(paste(
  "EXIT_SUSTAINED ~", paste(core, collapse = " + "),
  "+ splines::ns(t_at_risk, 3)"
  ))
  m_glm_core <- glm(f_core, data = haz, family = binomial("logit"))
  idx_hi <- influence_drop(m_glm_core, drop_frac = DROP_TOP_LEVERAGE)
  if (length(idx_hi)) {
    haz_rb <- haz[-idx_hi, , drop = FALSE]
    m_pool_core_rb <- fixest::feglm(f_core_yearfe, data = haz_rb, family = "logit", cluster = COUNTRY)
    cat("\n--- Refit after dropping top", round(100*DROP_TOP_LEVERAGE,1), "% leverage (n drop =",
        length(idx_hi), ") ---\n")
    print(summary(m_pool_core_rb))
  }

  # (2) Rare-events (bias-reduced) + clustered SEs — CORE
  m_rare_core <- glm(f_core, data = haz, family = binomial("logit"),
                     method = "brglmFit", control = brglmControl(type = "AS_mixed"))
  cat("\n=== Rare-events (bias-reduced) — CORE ===\n")
  print(summary(m_rare_core))
  cat("\n--- Rare-events + clustered SEs (country) ---\n")
  print(fit_clustered(m_rare_core))

  # collect coef + clustered SE for rare-events model
  co  <- coef(m_rare_core)
  V   <- vcovCL(m_rare_core, cluster = m_rare_core$data[[COUNTRY]])
  se  <- sqrt(diag(V))
  tmp <- data.frame(K = K, term = names(co), beta = unname(co), se = unname(se), row.names = NULL)
  COLLECT_five_year <- bind_rows(COLLECT_five_year, tmp)

  # (3) Random-intercept (country frailty) — CORE
  cat("\n=== Random-intercept (country) — CORE ===\n")
  m_re_core <- try(
    glmmTMB(f_core, data = haz, family = binomial(),
            control = glmmTMBControl(
              optimizer = optim, optArgs = list(method = "BFGS"),
              optCtrl = list(maxit = 1e4)
            )),
    silent = TRUE
  )
  if (inherits(m_re_core, "try-error")) {
    cat("glmmTMB failed to converge (sparse categories / quasi-separation likely).\n")
  } else {
    print(summary(m_re_core))
  }

  # (4) Conditional FE logit (country FE absorbed) — robustness
  cat("\n=== Conditional FE logit (absorbing country + year) — CORE ===\n")
  m_condfe <- try(
    fixest::feglm(
      as.formula(paste(
        "EXIT_SUSTAINED ~", paste(core, collapse=" + "),
        "+ ns(t_at_risk,3) |", COUNTRY, "+", YEAR)),
      data = haz, family = "logit", cluster = COUNTRY
    ),
    silent = TRUE
  )
  if (inherits(m_condfe, "try-error")) {
    cat("Conditional FE logit failed (possibly all-country separation in some years).\n")
  } else {
    print(summary(m_condfe))
  }

  # (5) AMEs (from plain GLM without year FE; AMEs are about X, not time dummies)
  cat("\n=== Average Marginal Effects — CORE GLM ===\n")
  ame <- margins::margins(m_glm_core, data = haz)  # <-- add data=
  print(summary(ame))

  # (6) Predictive diagnostics
  phat <- predict(m_glm_core, type = "response")
  y    <- haz$EXIT_SUSTAINED
  dm   <- diag_metrics(y, phat)
  cat("\n=== Diagnostics (GLM w/ duration) ===\n")
  print(dm)

  # store models
  MODELS[[paste0("K", K, "_poolFE")]]  <- m_pool_core
  MODELS[[paste0("K", K, "_rare")]]    <- m_rare_core
  MODELS[[paste0("K", K, "_glm")]]     <- m_glm_core
  MODELS[[paste0("K", K, "_frailty")]] <- m_re_core
  MODELS[[paste0("K", K, "_condFE")]]  <- m_condfe

  cat("\n================= End K =", K, "=================\n")
}

cat("\n[Done] Coefficient collection available in `COLLECT`; models in `MODELS`.\n")

# Example: tidy coef table across K for a few key variables
if (exists("COLLECT_five_year")) {
  key_terms <- c("(Intercept)", CORE_NAMES[CORE_NAMES %in% COLLECT_five_year$term], "ns(t_at_risk, 3)1", "ns(t_at_risk, 3)2", "ns(t_at_risk, 3)3")
  print(
    COLLECT_five_year %>%
      filter(term %in% key_terms) %>%
      mutate(z = beta / se, p = 2*pnorm(abs(z), lower.tail = FALSE)) %>%
      arrange(term, K)
  )
}

# ------------------------------------------------------------------------------
# LATEX Table ------------------------------------------------------------------
# ------------------------------------------------------------------------------

stopifnot(exists("COLLECT_five_year"))

suppressPackageStartupMessages({
  library(dplyr); library(tidyr); library(kableExtra); library(stringr)
})

# 1) Terms to include: drop intercept, spline basis, and year FE artifacts
omit_patterns <- c("^\\(Intercept\\)$", "splines::ns\\(", "^ns\\(", "^factor\\(", "^i\\(")
omit_re <- paste(omit_patterns, collapse="|")

terms_all <- COLLECT_five_year %>%
  mutate(term = as.character(term)) %>%
  filter(!str_detect(term, omit_re))

# 2) Pretty labels (override where you want; others auto-format)
var_labs_override <- c(
  ODA_RECEIVED_PER_CAPITA_L1 = "ODA per capita (t-4)",
  GDP_GROWTH_L1              = "Real GDP per capita growth (t-4)",
  GDP_PER_CAPITA_L1          = "GDP per capita (t-4)",
  GDP_DEFLATOR_L1            = "GDP deflator (t-4)",
  POLITICAL_REGIME_L1        = "Political regime (t-4)",
  ELECTORAL_DEMOCRACY_SCORE_L1 = "Electoral democracy (t-4)",
  LIBERAL_DEMOCRACY_SCORE_L1   = "Liberal democracy (t-4)",
  TERRITORIAL_FRAGMENTATION_L1 = "Territorial fragmentation (t-4)",
  INSTITUTIONAL_DEMOCRACY_SOCRE_L1 = "Institutional democracy (t-4)",
  INSTITUTIONAL_AUTOCRACY_SCORE_L1 = "Institutional autocracy (t-4)",
  REGIME_DURABILITY_YEARS_L1      = "Regime durability, years (t-4)",
  POLITICAL_COMPETITION_SCORE_L1  = "Political competition (t-4)",
  PARTIAL_DEMOCRACY_WITH_FACTIONALISM_L1 = "Partial democracy w/ factionalism (t-4)",
  CONFLICT_INTENSITY_YEAR_L1      = "Conflict intensity (acute, t-4)",
  CONFLICT_CUMULATIVE_INTENSITY_ACROSS_YEARS_L1 = "Cumulative conflict intensity (t-4)",
  N_WAR_FRONTS_L1                 = "N war fronts (t-4)",
  MAX_CONFLICT_INTENSITY_L1       = "Max conflict intensity (t-4)",
  AVG_CONFLICT_INTENSITY_L1       = "Conflict intensity (chronic avg., t-4)",
  N_TOTAL_TROOPS_L1               = "Total troops (t-4)"
)

auto_label <- function(x) {
  x %>%
    str_replace("_L1$", " (t-1)") %>%
    str_replace_all("_", " ") %>%
    str_to_sentence()
}

# Vectorized labeler: named lookup with [v], fill NAs with auto labels
make_label <- function(v) {
  v <- as.character(v)
  lbl <- unname(var_labs_override[v])      # returns named vector with NAs for misses
  miss <- is.na(lbl)
  lbl[miss] <- auto_label(v[miss])
  lbl
}

# 3) Format OR [LCL, UCL], bold if CI excludes 1
fmt_or_ci <- function(b, s) {
  OR  <- exp(b); LCL <- exp(b - 1.96*s); UCL <- exp(b + 1.96*s)
  cell <- sprintf("%.2f [%.2f, %.2f]", OR, LCL, UCL)
  if (!is.na(LCL) && !is.na(UCL) && (UCL < 1 || LCL > 1)) cell <- paste0("\\textbf{", cell, "}")
  cell
}

# 4) Build long table (K x term)
tab_long <- terms_all %>%
  mutate(
    term_label = make_label(term),
    cell = mapply(fmt_or_ci, beta, se)
  ) %>%
  select(term_label, K, cell)

# Ensure K columns exist in order 3/5/7 and blanks where missing
all_rows <- tibble(term_label = sort(unique(tab_long$term_label)))
all_cols <- tibble(K = c(3L, 5L, 7L))
tab_full <- tidyr::complete(tab_long, term_label = all_rows$term_label, K = all_cols$K, fill = list(cell = ""))

# 5) Optional buckets for section headers
bucket_of <- function(lbl) {
  lbl <- as.character(lbl)
  out <- rep("Other", length(lbl))
  out[stringr::str_detect(lbl, stringr::regex("conflict|war|troop", ignore_case = TRUE))] <- "Conflict & security"
  out[stringr::str_detect(lbl, stringr::regex("gdp|price|oda|per capita|growth|deflator", ignore_case = TRUE))] <- "Economy"
  out[stringr::str_detect(lbl, stringr::regex("democ|autoc|polity|regime|competition|recruitment|fragment", ignore_case = TRUE))] <- "Institutions & politics"
  factor(out, levels = c("Economy","Institutions & politics","Conflict & security","Other"))
}

tab_wide <- tab_full %>%
  dplyr::mutate(
    K = factor(K, levels = c(3,5,7), labels = c("K = 3","K = 5","K = 7")),
    bucket = bucket_of(term_label)
  ) %>%
  tidyr::pivot_wider(names_from = K, values_from = cell) %>%
  dplyr::arrange(bucket, term_label)

# 6) Render LaTeX with grouped sections (robust start/end calc)
kb <- kable(
  tab_wide %>% select(term_label, `K = 3`, `K = 5`, `K = 7`),
  format = "latex",
  booktabs = TRUE,
  linesep = "",
  caption = "Sustained Exit from Fragility: Odds Ratios with 95\\% Clustered CIs (ALL predictors, rare-events logit)",
  col.names = c("Predictors", "K = 3", "K = 5", "K = 7"),
  escape = FALSE,
  align = c("l","c","c","c"),
  label = "tab:all_or"
) %>%
  kable_styling(latex_options = c("hold_position","striped","scale_down"),
                font_size = 8) %>%                                 # smaller font + auto-resize
  column_spec(1, width = "4.5cm") %>%                              # wrap predictor names
  column_spec(2, width = "3.2cm") %>%                              # narrow K columns
  column_spec(3, width = "3.2cm") %>%
  column_spec(4, width = "3.2cm") %>%
  add_header_above(c(" " = 1, "Sustain horizon (years)" = 3))

# Add group headers without rle()
bucket_levels <- c("Economy","Institutions & politics","Conflict & security","Other")
# make sure ordering is locked
tab_wide$bucket <- factor(tab_wide$bucket, levels = bucket_levels)

idx_by_bucket <- split(seq_len(nrow(tab_wide)), tab_wide$bucket)

for (lev in bucket_levels) {
  idx <- idx_by_bucket[[lev]]
  if (!is.null(idx) && length(idx)) {
    kb <- pack_rows(kb, lev, start = min(idx), end = max(idx), indent = FALSE)
  }
}

kb <- footnote(
  kb,
  general = "Entries are odds ratios with 95% clustered confidence intervals. Models are bias-reduced (brglm2, ASmixed). Covariates standardized before lagging. Spline of time-at-risk included; estimates correspond to the rare-events specification collected in COLLECT (no year FE). For the year-FE version, rebuild with COLLECTYFE and rerun. Outcome: first sustained exit (Non-Fragile) at t persisting for K years; sample is the risk set plus first exit year. We exclude Executive Recruitment and Polity Score from the core specification because their coefficients are not well identified: Executive Recruitment exhibits complete or quasi-separation in the risk set, yielding boundary estimates and uninformative confidence intervals, while Polity Score is highly collinear with our V-Dem democracy measures. Both variables are reported in Appendix Table A.x for completeness.",
  threeparttable = TRUE, escape = FALSE
)

cat(kb)

```
